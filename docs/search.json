[
  {
    "objectID": "slides/01-introduction.html#goals-of-the-course",
    "href": "slides/01-introduction.html#goals-of-the-course",
    "title": "01-Introduction",
    "section": "Goals of the course",
    "text": "Goals of the course\n\n\nIntroduction to “Data Science”\nStatistical concepts and selected methods\n\nStatistical parameters\nDistributions and probability\nStatistical tests\nModel selection\n\nPractical experience\n\nData strutures\nBasics of the R language\nApplications with real and simulated data sets\n\n\n\\(\\Rightarrow\\) Practical understanding and “statistical feeling”,\n\\(\\rightarrow\\) More important than facts learned by heart."
  },
  {
    "objectID": "slides/01-introduction.html#topics",
    "href": "slides/01-introduction.html#topics",
    "title": "01-Introduction",
    "section": "Topics",
    "text": "Topics\n\n\nBasic Concepts of Statistics\nAn Introduction to R\nStatistical Parameters and Distributions\nLinear Models\nAnalysis of Variance\nNonlinear Regression\nTime Series Analysis\n(Multivariate Statistics)"
  },
  {
    "objectID": "slides/01-introduction.html#material",
    "href": "slides/01-introduction.html#material",
    "title": "01-Introduction",
    "section": "Material",
    "text": "Material\n\n\nSlides\nExercises\nCourse script for self study “Data Analysis with R – Selected Topics and Examples”\nNote: Slides, script and exercises are regularly updated, depending on the progress of the course.\nExam: Written exam at the end of the semester. \\(\\rightarrow\\) Attend the labs!\n\n\n\nQuestions?"
  },
  {
    "objectID": "slides/01-introduction.html#an-introductory-example",
    "href": "slides/01-introduction.html#an-introductory-example",
    "title": "01-Introduction",
    "section": "An introductory example",
    "text": "An introductory example\nDaily average discharge of River Elbe, pegel Dresden, river km 55.6\ndate,       discharge\n1806-01-01,  472\n1806-01-02, 1050\n1806-01-03, 1310\n1806-01-04, 1020\n1806-01-05,  767\n1806-01-06,  616\n...\n2020-10-11,  216\n2020-10-12,  204\n2020-10-13,  217\n2020-10-14,  288\n2020-10-15,  440\n2020-10-16,  601\n2020-10-17,  570\n2020-10-18,  516\n2020-10-19,  450\n2020-10-20,  422\n2020-10-21,  396\n2020-10-22,  372\n2020-10-23,  356\n2020-10-24,  357\n2020-10-25,  332\n2020-10-26,  303\n2020-10-27,  302\n2020-10-28,  316\n2020-10-29,  321\n2020-10-30,  331\n2020-10-31,  353\n2020-11-01,  395\n\\(>\\) 70,000 measurements. How can we analyse this and what does it mean?\nData Source: Bundesanstalt für Gewässerkunde"
  },
  {
    "objectID": "slides/01-introduction.html#plot-the-last-20-years",
    "href": "slides/01-introduction.html#plot-the-last-20-years",
    "title": "01-Introduction",
    "section": "Plot the last 20 years",
    "text": "Plot the last 20 years\n\nDischarge of the Elbe River, gauge station Dresden, data source BfG"
  },
  {
    "objectID": "slides/01-introduction.html#what-do-these-data-tell-us",
    "href": "slides/01-introduction.html#what-do-these-data-tell-us",
    "title": "01-Introduction",
    "section": "What do these data tell us?",
    "text": "What do these data tell us?\n\n\nWhat is the average discharge? → mean values\nHow much variation is in the data? → variance\nHow likely are droughts or floods? → distribution\nHow precise are our forecasts? → confidence intervals\nWhich factors influence discharge? → correlations"
  },
  {
    "objectID": "slides/01-introduction.html#how-to-start",
    "href": "slides/01-introduction.html#how-to-start",
    "title": "01-Introduction",
    "section": "How to start",
    "text": "How to start\n\n\nMean value: 224\nMedian value: 224\nStandard deviation: 253\nRange: 2, 4500\n\nWhich of these parameters are most appropriate?"
  },
  {
    "objectID": "slides/01-introduction.html#graphics",
    "href": "slides/01-introduction.html#graphics",
    "title": "01-Introduction",
    "section": "Graphics",
    "text": "Graphics"
  },
  {
    "objectID": "slides/01-introduction.html#boxplots",
    "href": "slides/01-introduction.html#boxplots",
    "title": "01-Introduction",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nNote the log scale of y!\nIn the right version, whiskers extend to the most extreme data point which is no more than 1.5 times the interquartile range from the box."
  },
  {
    "objectID": "slides/01-introduction.html#three-ways-to-work-with-statistics",
    "href": "slides/01-introduction.html#three-ways-to-work-with-statistics",
    "title": "01-Introduction",
    "section": "Three ways to work with statistics",
    "text": "Three ways to work with statistics\nDescriptive statistics and graphics\n\nplots, like in the examples\nmean values, standard deviations, …\ninterpret raw data\n\nHypothesis testing\n\ndistinguish effects from random fluctuations\nmake results more convincing\n\nStatistical modelling\n\nmeasure size of effects (e.g. climate trends)\nbuild models that aggregate dependencies\nmachine learning"
  },
  {
    "objectID": "slides/01-introduction.html#statistical-hypothesis-testing",
    "href": "slides/01-introduction.html#statistical-hypothesis-testing",
    "title": "01-Introduction",
    "section": "Statistical hypothesis testing",
    "text": "Statistical hypothesis testing\n\nHow likely is it, that our hypothesis is true?\n\n\nTurn scientific into statistical hypothesis\nEstimate probability (p value) of a given hypothesis Examples\nIs a medical treatment successful or not? → \\(\\chi^2\\)-test\nDoes a specific food diet increase yield of a fish farm? → t-test\nWhich factors (e.g. food, temperature, pH) of a combined treatment influence growth of aquatic animals? → ANOVA\n(How) does observed algal biomass depend on phosphorus?"
  },
  {
    "objectID": "slides/01-introduction.html#statistical-modeling",
    "href": "slides/01-introduction.html#statistical-modeling",
    "title": "01-Introduction",
    "section": "Statistical modeling",
    "text": "Statistical modeling\nFit a statistical model to the data\n\nSelect proper modelling strategy\nDesign statistical models\nMeasure effect size\nSelect the optimum model between different model candidates\n\nExamples\n\nFit a distribution to annual discharge data to estimate the 100 year flood.\nFit an ANOVA model to experimental data to see which factors influence the result most.\nFit a multiple linear model to climate data to see how much climate trends differ between geographical location."
  },
  {
    "objectID": "slides/01-introduction.html#example-compare-two-mean-values",
    "href": "slides/01-introduction.html#example-compare-two-mean-values",
    "title": "01-Introduction",
    "section": "Example: Compare two mean values",
    "text": "Example: Compare two mean values\n\n\nA given data set (Dobson, 1983) contains the birth weight (in g) of 12 boys and 12 girls.\nHas the weight difference something to do with the gender of the babies or is it a purely random fluctuation?"
  },
  {
    "objectID": "slides/01-introduction.html#example-correlation-and-regression",
    "href": "slides/01-introduction.html#example-correlation-and-regression",
    "title": "01-Introduction",
    "section": "Example: Correlation and regression",
    "text": "Example: Correlation and regression\n\n\n\n\n\n\nDependence of chlorophyll concentration in lakes on phosphorus, a regional data set from Koschel and Scheffler (1985) (left) and from Vollenweider and Kerekes (1980) (right).\nWhich of the two figures has greater predictive power? Why?\n\n\n\nThe parameter \\(r\\) is the Pearson correlation coefficient."
  },
  {
    "objectID": "slides/01-introduction.html#required-software",
    "href": "slides/01-introduction.html#required-software",
    "title": "01-Introduction",
    "section": "Required software",
    "text": "Required software\n\nA spreadsheet program, Excel or LibreOffice https://www.libreoffice.org/\nThe R system for data analysis and graphics https://www.r-project.org\nRStudio for making R more user-friendly https://posit.co/download/rstudio-desktop/"
  },
  {
    "objectID": "slides/01-introduction.html#why-r",
    "href": "slides/01-introduction.html#why-r",
    "title": "01-Introduction",
    "section": "Why R?",
    "text": "Why R?\n\n\nStatisticians call it “lingua franca” in computational statistics.\n\nExtremely powerful\nNo other system has so much statistics\nUsed in statistical research\n\nFree (OpenSource)\n\nFree to use\nFree to modify\nFree to contribute\n\nLess complicated than its first appearance:\n\nYes, it needs command line programming\nbut: already a single line can do much\nhuge number of books and online scripts\n\n\n\n\n\n\nIn contrast to other systems Copy & Paste is allowed! – just cite it."
  },
  {
    "objectID": "slides/01-introduction.html#books",
    "href": "slides/01-introduction.html#books",
    "title": "01-Introduction",
    "section": "Books",
    "text": "Books\nStatistics\n\nWell-readable introductions\n\nDalgaard, P., 2008: Introductory Statistics with R. Springer, New York, 2nd edition. (fulltext of the 1st edition freely available)\nVerzani, J. (2019). Using R for introductory statistics. CRC press. Fulltext available via DOI https://doi.org/10.1111/anzs.12146\n\nA very well understandable introduction into many fields: of statistics, especially regression and time series analysis:\n\nKleiber, C. and Zeileis, A., 2008: Applied Econometrics with R, Springer Verlag, New York. https://link.springer.com/book/10.1007/978-0-387-77318-6\n\n\nR Programming\n\nAn introduction to data science using the modern “tidyverse” approach:\n\nWickham, H., Çetinkaya-Rundel, M and Grolemund, G, 2023: R for Data Science. https://r4ds.hadley.nz/\n\n\n\n\n\n\n\n\nAnd lots of material available freely on the internet …"
  },
  {
    "objectID": "slides/05-classtests.html#statistical-test",
    "href": "slides/05-classtests.html#statistical-test",
    "title": "05-Classical Tests",
    "section": "Statistical test",
    "text": "Statistical test\n\nA statistical hypothesis test is a method of statistical inference.\n\nCommonly, two samples are compared, or a sample is compared against properties from an idealized model.\nA hypothesis \\(H_a\\) for the statistical relationship between the two data sets, is compared to an idealized null hypothesis H0 that proposes no relationship between two data sets.\nThe comparison is considered statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probability – the significance level.\n\nadapted from: https://en.wikipedia.org/wiki/Statistical hypothesis testing"
  },
  {
    "objectID": "slides/05-classtests.html#effect-size-and-significance",
    "href": "slides/05-classtests.html#effect-size-and-significance",
    "title": "05-Classical Tests",
    "section": "Effect size and significance",
    "text": "Effect size and significance\n\nIn case of relative mean differences, the relative effect size is:\n\n\\[\n  \\delta = \\frac{\\bar{\\mu}_1-\\bar{\\mu}_2}{\\sigma}=\\frac{\\Delta}{\\sigma}\n\\]\n\nwith:\n\nmean values of two populations \\(\\mu_1, \\mu_2\\)\neffect size \\(\\Delta\\)\nrelative effect size \\(\\delta\\) (also called Cohen’s d)\nsignificance means that an observed effect is unlikely the result of pure random variation."
  },
  {
    "objectID": "slides/05-classtests.html#null-hypothesis-and-alternative-hypothesis",
    "href": "slides/05-classtests.html#null-hypothesis-and-alternative-hypothesis",
    "title": "05-Classical Tests",
    "section": "Null hypothesis and alternative hypothesis",
    "text": "Null hypothesis and alternative hypothesis\n\n\\(H_0\\) null hypothesis: two populations are not different with respect to a certain property.\n\nAssumption: observed effect occured purely at random, true effect is zero.\n\n\\(H_a\\) alternative hypothesis (experimental hypothesis): existence of a certain effect.\n\nAn alternative hypothesis is never completely true or “proven”.\nAcceptance of \\(H_A\\) means only than \\(H_0\\) is unlikely.\n\n“Not significant” means either no effect or sample size too small!\n\nNote: Different meaning of significance (\\(H_0\\) unlikely) and relevance (effect large enough to play a role in practice)."
  },
  {
    "objectID": "slides/05-classtests.html#the-p-value",
    "href": "slides/05-classtests.html#the-p-value",
    "title": "05-Classical Tests",
    "section": "The p-value",
    "text": "The p-value\n\nThe interpretation of the p-value was often confused in the past, even in statistics textbooks, so it is good to refer to a clear definition:\n\n\nThe p-value is defined as the probability of obtaining a result equal to or ‘more extreme’ than what was actually observed, when the null hypothesis is true.\n\n\n\n\nhttps://en.wikipedia.org/wiki/P-value:\nHubbard (2004) Alphabet Soup: Blurring the Distinctions Between p’s and a’s in Psychological Research, Theory Psychology 14(3), 295-327. DOI: 10.1177/0959354304043638"
  },
  {
    "objectID": "slides/05-classtests.html#alpha-and-beta-errors",
    "href": "slides/05-classtests.html#alpha-and-beta-errors",
    "title": "05-Classical Tests",
    "section": "Alpha and beta errors",
    "text": "Alpha and beta errors\n\n\n\n\n\n\n\n\n\nReality\nDecision of the test\ncorrect?\nprobability\n\n\n\n\n\\(H_0\\) = true\nsignificant\nno\n\\(\\alpha\\)-error\n\n\n\\(H_0\\) = false\nnot significant\nno\n\\(\\beta\\)-error\n\n\n\\(H_0\\) = true\nnot significant\nyes\n\\(1-\\alpha\\)\n\n\n\\(H_0\\) = false\nsignificant\nyes\n\\(1-\\beta\\) (power)\n\n\n\n\n\n\n\n\n\n1.\\(H_0\\) falsely rejected (error of the first kind or \\(\\alpha\\)-error)\n\nwe claim an effect, that does not exist, e.g. a drug with no effect\n\n2.\\(H_0\\) falsely retained (error of the second kind or \\(\\beta\\)-error)\n\ntypical case in small studies, where effect was not enough to detect existing effects\n\nUse in practice\n\ncommon convention in environmental sciences: \\(\\alpha=0.05\\), must be set beforehand\n\\(\\beta=f(\\alpha, \\text{effectsize}, \\text{sample size}, \\text{kind of test})\\), should be \\(\\le 0.2\\)"
  },
  {
    "objectID": "slides/05-classtests.html#significance-and-relevance",
    "href": "slides/05-classtests.html#significance-and-relevance",
    "title": "05-Classical Tests",
    "section": "Significance and relevance",
    "text": "Significance and relevance\n\nSignificance is not the only important. It is often more important to focus on effect size and relevance.\nWhen evaluating significant effects, it is always necessary to look also at the effect size. While significance means that the null hypothesis \\(H_0\\) is unlikely in a statistical sense, relevance means that the effect size is large enough to play a role in practice. This means that whether an effect can be relevant or not depends on its effect size and the field of application.\nLet’s for example consider a vaccination. If a vaccine had a significant effect in a clinical test, but protected only 10 out of 1000 people, one would not consider this effect as relevant and not produce this vaccine.\nOn the other hand, even small effects can be relevant. So if a toxic substance would have an effect on 1 out of 1000 people to produce cancer, we would consider this as relevant. To detect this as a significant effect would need an epidemiological study with a large number of people. But as it is highly relevant, it is worth the effort."
  },
  {
    "objectID": "slides/05-classtests.html#take-home-messages",
    "href": "slides/05-classtests.html#take-home-messages",
    "title": "05-Classical Tests",
    "section": "Take home messages",
    "text": "Take home messages\n\n\nA p-value measures the probability that a purely random effect would be equally or more extreme than an observed effect.\n“Significant” means that an observed result is unlikely to have occurred by chance alone.\n“Not significant” means either “no effect” or “sample size too small”.\nDon’t focus on p-values alone. Never forget to report also sample size, effect size and relevance of your results.\nWith large data sets, the p-value loses importance. It is then easy to find significant effects, but it is often very small and not relevant in practice.\nThe p-value is an important tool in classical statistics, but its abuse can lead to mis-interpretation."
  },
  {
    "objectID": "slides/05-classtests.html#one-sample-t-test",
    "href": "slides/05-classtests.html#one-sample-t-test",
    "title": "05-Classical Tests",
    "section": "One sample t-Test",
    "text": "One sample t-Test\n\n\ntests if a sample is from a population with given mean value \\(\\mu\\)\nbased on checking if the population mean \\(\\mu\\) is in the confidence interval of \\(\\bar{x}\\)\n\n\nLet’s assume a sample of size with \\(n=10, \\bar{x}=5.5, s=1\\) and \\(\\mu=5\\).\nEstimate the 95% confidence interval of \\(\\bar{x}\\):\n\n\\[\nCI = \\bar{x} \\pm t_{1-\\alpha/2, n-1} \\cdot s_{\\bar{x}}\n\\] with \\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\qquad \\text{(standard error)}\n\\]\nDifferent ways of calculation shown at the next slides"
  },
  {
    "objectID": "slides/05-classtests.html#remember-standard-deviation-and-standard-error",
    "href": "slides/05-classtests.html#remember-standard-deviation-and-standard-error",
    "title": "05-Classical Tests",
    "section": "Remember: standard deviation and standard error",
    "text": "Remember: standard deviation and standard error\n\n Visualization of a one-sample t-test. Left: original distribution of the data measured by standard deviation, right: distribution of mean values, measured by its standard error. \n\\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\qquad \\text{(standard error)}\n\\]\n\nstandard error < standard deviation\nmeasures precision of the mean value\nCLT!\n\nThe test is based on the distribution of the means, not distribution of original data."
  },
  {
    "objectID": "slides/05-classtests.html#method-1-is-mu-in-the-confidence-interval",
    "href": "slides/05-classtests.html#method-1-is-mu-in-the-confidence-interval",
    "title": "05-Classical Tests",
    "section": "Method 1: Is \\(\\mu\\) in the confidence interval?",
    "text": "Method 1: Is \\(\\mu\\) in the confidence interval?\n\n\nSample: \\(n=10, \\bar{x}=5.5, s=1\\) and \\(\\mu=5\\)\nLet \\(\\alpha = 0.05\\), we get a two-sided 95% confidence interval with:\n\n\\[\\bar{x} \\pm t_{0,975, n-1} \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\n5.5 + c(-1, 1) * qt(0.975, 10-1) * 1/sqrt(10)\n\n[1] 4.784643 6.215357\n\n\n\n\n\nCheck if \\(\\mu=5.0\\) is in this interval?\nYes, it is inside \\(\\Rightarrow\\) difference not significant."
  },
  {
    "objectID": "slides/05-classtests.html#method-2-comparison-with-a-tabulated-t-value",
    "href": "slides/05-classtests.html#method-2-comparison-with-a-tabulated-t-value",
    "title": "05-Classical Tests",
    "section": "Method 2: Comparison with a tabulated t-value",
    "text": "Method 2: Comparison with a tabulated t-value\n\nRearrange the equation of the confidence interval, to calculate an observed \\(t_{obs}\\)\n\n\\[\nt_{obs} = |\\bar{x}-\\mu | \\cdot \\frac{1}{s_{\\bar{x}}} = \\frac{|\\bar{x}-\\mu |}{s} \\cdot \\sqrt{n} = \\frac{|5.5 -5.0|}{1.0} \\cdot \\sqrt{10}\n\\]\nWe can calculate this in R:\n\nt <- abs(5.5 - 5.0) / 1.0 * sqrt(10)\nt\n\n[1] 1.581139\n\n\n\nCompare \\(t_{obs}\\) with a tabulated value\n\n\n“Old style”: find critical t-value in a table for given \\(\\alpha\\) and degrees of freedom (\\(n-1\\))\nFor \\(\\alpha=0.05\\) and two-sided, this is: \\(t_{1-\\alpha/2, n-1} = 2.26\\).\n\nComparison: \\(1.58 < 2.26\\) \\(\\Rightarrow\\) no significant difference between \\(\\bar{x}\\) and \\(\\mu\\)."
  },
  {
    "objectID": "slides/05-classtests.html#method-3-calculation-of-the-p-value-from-t_obs",
    "href": "slides/05-classtests.html#method-3-calculation-of-the-p-value-from-t_obs",
    "title": "05-Classical Tests",
    "section": "Method 3: Calculation of the p-value from \\(t_{obs}\\)",
    "text": "Method 3: Calculation of the p-value from \\(t_{obs}\\)\n\n\nuse computerized probability function (pt) instead of table lookup\n\\(t = t_{obs}\\) and the degrees of freedom (\\(n-1\\)):\n\n\n\n2 * (1 - pt(t, df = 10 - 1)) # 2 * (1 - p) is re-arranged from 1-alpha/2\n\n[1] 0.1483047\n\n\nThis p-value = 0.1483047 is greater than \\(0.05\\) so we consider the difference as not significant.\n\nFAQ: less than or greater than?\n\n\n\n\n\n\n\n\n\n\np-value\n\\(\\text{p-value} < \\alpha\\)\nnull hypothesis unlikely\nsignificant\n\n\ntest statistic\n\\(t_{obs} > t_{1-\\alpha/2, n-1}\\)\neffect exceeds confint.\nsignificant"
  },
  {
    "objectID": "slides/05-classtests.html#method-4-built-in-t-test-function-in-r",
    "href": "slides/05-classtests.html#method-4-built-in-t-test-function-in-r",
    "title": "05-Classical Tests",
    "section": "Method 4: Built-in t-test function in R",
    "text": "Method 4: Built-in t-test function in R\n\nThe same can be done much easier with the computer in R.\nLet’s assume we have a sample with \\(\\bar{x}=5, s=1\\):\n\n## define sample\nx <- c(5.5, 3.5, 5.4, 5.3, 6, 7.2, 5.4, 6.3, 4.5, 5.9)\n\n## perform one-sample t-test\nt.test(x, mu=5)\n\n\n    One Sample t-test\n\ndata:  x\nt = 1.5811, df = 9, p-value = 0.1483\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 4.784643 6.215357\nsample estimates:\nmean of x \n      5.5 \n\n\nThe test returns the observed t-value, the 95% confidence interval and the p-value.\nAn important difference is, that this method needs the original data, while the other methods need only mean, standard deviation and sample size."
  },
  {
    "objectID": "slides/05-classtests.html#two-sample-t-test",
    "href": "slides/05-classtests.html#two-sample-t-test",
    "title": "05-Classical Tests",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nThe two-sample t-test compares two independent samples:\n\nx1 <- c(5.3, 6.0, 7.1, 6.4, 5.7, 4.9, 5.0, 4.6, 5.7, 4.0, 4.5, 6.5)\nx2 <- c(5.8, 7.1, 5.8, 7.0, 6.7, 7.7, 9.2, 6.0, 7.2, 7.8, 7.8, 5.7)\nt.test(x1, x2)\n\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = -3.7185, df = 21.611, p-value = 0.001224\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.3504462 -0.6662205\nsample estimates:\nmean of x mean of y \n 5.475000  6.983333 \n\n\n\n\n\\(\\rightarrow\\) both samples differ significantly (\\(p < 0.05\\))\nNote: R has not performed the “ordinary” t-test but the Welch test (= heteroscedastic t-test)\nwhere variances of both samples don’t need to be identical."
  },
  {
    "objectID": "slides/05-classtests.html#hypothesis-and-formula-of-the-two-sample-t-test",
    "href": "slides/05-classtests.html#hypothesis-and-formula-of-the-two-sample-t-test",
    "title": "05-Classical Tests",
    "section": "Hypothesis and formula of the two-sample t-test",
    "text": "Hypothesis and formula of the two-sample t-test\n\n\n\\(H_0\\) \\(\\mu_1 = \\mu_2\\)\n\\(H_a\\) the two means are different\ntest criterion\n\\[\nt_{obs} =\\frac{|\\bar{x}_1-\\bar{x}_2|}{s_{tot}} \\cdot \\sqrt{\\frac{n_1 n_2}{n_1+n_2}}\n\\]\n\n\n\n\n\n\n\n\npooled standard deviation\n\\[\ns_{tot} = \\sqrt{{({n}_1 - 1)\\cdot s_1^2 + ({n}_2 - 1)\\cdot s_2^2\n\\over ({n}_1 + {n}_2 - 2)}}\n\\]\nassumptions: independence, equal variances, approximate normal distribution"
  },
  {
    "objectID": "slides/05-classtests.html#the-welch-test",
    "href": "slides/05-classtests.html#the-welch-test",
    "title": "05-Classical Tests",
    "section": "The Welch test",
    "text": "The Welch test\n\nKnown as t-test for samples with unequal variance, works also for equal variance!\n\nTest criterion:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{s^2_{\\bar{x}_1} + s^2_{\\bar{x}_2}}}\n\\]\nStandard error of each sample:\n\\[\ns_{\\bar{x}_i} = \\frac{s_i}{\\sqrt{n_i}}\n\\] Corrected degrees of freedom:\n\\[\n\\text{df} = \\frac{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}{\\frac{s^4_1}{n^2_1(n_1-1)} + \\frac{s^4_2}{n^2_2(n_2-1)}}\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#equality-of-variance-f-test",
    "href": "slides/05-classtests.html#equality-of-variance-f-test",
    "title": "05-Classical Tests",
    "section": "Equality of variance: F-test",
    "text": "Equality of variance: F-test\n\n\\(H_0\\): \\(\\sigma_1^2 = \\sigma_2^2\\)\n\\(H_a\\): variances unequal\nTest criterion:\n\\[F = \\frac{s_1^2}{s_2^2} \\]\n\nlarger of the two variances in the enumerator \\((s^2_1 > s^2_2)\\)\nseparate degrees of freedom (\\(n-1\\))\n\n\n\n\n\n\n\n\n\nExample:\n\n\\(s_1=1\\), \\(s_2 =2\\), \\(n_1=5, n_2=10, F=\\frac{2^2}{1^2}=4\\)\ndeg. of freedom: \\(9 \\atop 4\\)\n\n\\(\\Rightarrow\\) \\(F_{9, 4, \\alpha=0.975} = 8.9 > 4 \\quad\\rightarrow\\) not significant"
  },
  {
    "objectID": "slides/05-classtests.html#homogeneity-of-variances-with-2-samples",
    "href": "slides/05-classtests.html#homogeneity-of-variances-with-2-samples",
    "title": "05-Classical Tests",
    "section": "Homogeneity of variances with > 2 samples",
    "text": "Homogeneity of variances with > 2 samples\n\n\n\n\n\n\n\n\nBartlett’s test:\n\nbartlett.test(list(x1, x2, x3))\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  list(x1, x2, x3)\nBartlett's K-squared = 7.7136, df = 2, p-value = 0.02114\n\n\n Fligner-Killeen test (recommended):\n\nfligner.test(list(x1, x2, x3))\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  list(x1, x2, x3)\nFligner-Killeen:med chi-squared = 2.2486, df = 2, p-value = 0.3249\n\n\n\n\n\n\n\n\n\n\n\ntests are often used to check assumptions of the ANOVA"
  },
  {
    "objectID": "slides/05-classtests.html#recommendation-for-two-sample-t-tests",
    "href": "slides/05-classtests.html#recommendation-for-two-sample-t-tests",
    "title": "05-Classical Tests",
    "section": "Recommendation for two sample t-tests",
    "text": "Recommendation for two sample t-tests\n\nTraditional procedure:\n\nTest for equal variances using the F-test: var.test(x, y)\nIf variances are equal: t.test(x, y, var.equal=TRUE)\notherwise, use t.test(x, y) (= Welch test)\nCheck if both samples follow a normal distribution.\n\n\nMore modern recommendation:\n\nDon’t use pre-tests!\nUse always the Welch test: t.test(x, y)\nCheck approximate normal distribution by using box-plots. Not necessary if \\(n\\) is large.\n\nsee Zimmerman (2004) or Wikipedia."
  },
  {
    "objectID": "slides/05-classtests.html#paired-t-test",
    "href": "slides/05-classtests.html#paired-t-test",
    "title": "05-Classical Tests",
    "section": "Paired t-Test",
    "text": "Paired t-Test\n\nsometimes also called “t-test of dependent samples”\n\nthe term “dependent” can be misleading, better “pairwise”\nvalues within samples must still be independent\n\nexamples: left arm / right arm; before / after\nis essentially a one-sample t-test of pairwise differences against \\(\\mu=0\\)\n\nadvantage: eliminates “covariate”\n\n\n\nx1 <- c(2, 3, 4, 5, 6)\nx2 <- c(3, 4, 7, 6, 8)\nt.test(x1, x2, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  x1 and x2\nt = -1.372, df = 8, p-value = 0.2073\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.28924  1.08924\nsample estimates:\nmean of x mean of y \n      4.0       5.6 \n\n\np=0.20, not significant\n\n\n\nx1 <- c(2, 3, 4, 5, 6)\nx2 <- c(3, 4, 7, 6, 8)\nt.test(x1, x2, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  x1 and x2\nt = -4, df = 4, p-value = 0.01613\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.710578 -0.489422\nsample estimates:\nmean difference \n           -1.6 \n\n\np=0.016, significant\n\nIt can be seen that the paired t-test has a greater discriminatory power in this case."
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney-and-wilcoxon-test",
    "href": "slides/05-classtests.html#mann-whitney-and-wilcoxon-test",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney and Wilcoxon-test",
    "text": "Mann-Whitney and Wilcoxon-test\n\n\nNon-parametric tests:\n\nNo assumptions about shape and parameters of distribution, but\ndistributions should be similar, otherwise test may be misleading.\n\nMann-Whitney U-test and Wilcoxon-test are very similar.\nIn a strict sense, “Wilcoxon” is the version for paired data\n\n Basic principle: Count of so-called “inversions” of ranks, where samples overlap\n\nSample A: 1, 3, 4, 5, 7\nSample B: 6, 8, 9, 10, 11\nBoth samples ordered together: 1, 3, 4, 5, 6, 7, 8, 9, 10, 11\nInversions: \\(\\rightarrow\\) \\(U = 1\\)"
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney-test-procedure-in-practice",
    "href": "slides/05-classtests.html#mann-whitney-test-procedure-in-practice",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney test procedure in practice",
    "text": "Mann-Whitney test procedure in practice\n\n\nAssign ranks \\(R_A\\) and \\(R_B\\) to both samples \\(A\\), and \\(B\\) with sample size \\(m\\) and \\(n\\).\nCalculate number of inversions \\(U\\):\n\n\\[\\begin{align*}\n     U_A &= m \\cdot n + \\frac{m (m + 1)}{2} - \\sum_{i=1}^m R_A \\\\\n     U_B &= m \\cdot n + \\frac{n (n + 1)}{2} - \\sum_{i=1}^n R_B \\\\\n     U   &= \\min(U_A, U_B)\n\\end{align*}\\]\n\nCritical values of \\(U\\) can be found in common statistics text books.\nNot necessary in R, p-value directly printed.\nNote: Use special version wilcox.exact with correction if sample has ties."
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney---wilcoxon-test-in-r",
    "href": "slides/05-classtests.html#mann-whitney---wilcoxon-test-in-r",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney - Wilcoxon-test in R",
    "text": "Mann-Whitney - Wilcoxon-test in R\n\n\nA <- c(1, 3, 4, 5, 7)\nB <- c(6, 8, 9, 10, 11)\n\nwilcox.test(A, B) # use optional argument `paired = TRUE` for paired data.\n\n\n    Wilcoxon rank sum exact test\n\ndata:  A and B\nW = 1, p-value = 0.01587\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nMann-Whitney - Wilcoxon-test with tie correction\n\napplied if the rank differences contain doubled values\n\n\nA <- c(1, 3, 4, 5, 7)\nB <- c(6, 8, 9, 10, 11)\n\n\nlibrary(\"exactRankTests\")\nwilcox.exact(A, B, paired=TRUE)\n\n\n    Exact Wilcoxon signed rank test\n\ndata:  A and B\nV = 0, p-value = 0.0625\nalternative hypothesis: true mu is not equal to 0"
  },
  {
    "objectID": "slides/05-classtests.html#permutation-methods",
    "href": "slides/05-classtests.html#permutation-methods",
    "title": "05-Classical Tests",
    "section": "Permutation methods",
    "text": "Permutation methods\n\nBasic principle: Estimation of a test statistic \\(\\xi_{obs}\\) from sample,\nResampling: Simulate many \\(\\xi_{i, sim}\\) from randomly permuted data set (\\(n = 999\\) or more)\nWhere does \\(\\xi_{est}\\) appear within the ordered series of simulated values \\(\\xi_{i, sim}\\)?\n\n\nLet \\(\\xi_{obs}\\) be \\(4.5\\) in our example, then \\(\\Rightarrow\\) \\(p= 0.97\\)."
  },
  {
    "objectID": "slides/05-classtests.html#testing-for-distributions",
    "href": "slides/05-classtests.html#testing-for-distributions",
    "title": "05-Classical Tests",
    "section": "Testing for distributions",
    "text": "Testing for distributions\nNominal variables\n\n\\(\\chi^2\\)-test\nFisher’s exact test\n\nOrdinal variables\n\nCramér-von-Mises-Test\n\\(\\rightarrow\\) more powerful than \\(\\chi^2\\) or KS-test\n\nMetric scales\n\nKolmogorov-Smirnov-Test (KS-test)\nShapiro-Wilks-Test (for normal distribution)"
  },
  {
    "objectID": "slides/05-classtests.html#contingency-tables-for-nominal-variables",
    "href": "slides/05-classtests.html#contingency-tables-for-nominal-variables",
    "title": "05-Classical Tests",
    "section": "Contingency tables for nominal variables",
    "text": "Contingency tables for nominal variables\n\nused for nominal (i.e. categorical or qualitative) data\nexamples: eye and hair color, medical treatment and the number of cured/not cured\nimportant: use absolute mesurements (true numbers!), not percentages or other calculated data (e.g. not something like biomass per area)\n\nData example\n\nDaphnia (water flee)-clones and their preferred depth in a lake\nfood algae in the deep water zone, poor of oxygen\nonly adapted clones with high haemoglobin can dive into deep water\n\n\n\n\nClone\nUpper layer\nDeep layer\n\n\n\n\nA\n50\n87\n\n\nB\n37\n78\n\n\nC\n72\n45"
  },
  {
    "objectID": "slides/05-classtests.html#calculation-of-the-chi2-test",
    "href": "slides/05-classtests.html#calculation-of-the-chi2-test",
    "title": "05-Classical Tests",
    "section": "Calculation of the \\(\\chi^2\\)-test",
    "text": "Calculation of the \\(\\chi^2\\)-test\n\nObserved frequencies \\(O_{ij}\\) and sums of rows and columns:\n\n\n\n\n\nClone A\nClone B\nClone C\nSum \\(s_i\\)\n\n\n\n\nEpilimnion\n50\n37\n72\n159\n\n\nHypolimnion\n87\n78\n45\n210\n\n\nSum {\\(s_j\\)}\n137\n115\n117\n\\(n=369\\)\n\n\n\n\n\n\n\n\n\n\n\nExpected frequencies \\(E_{ij}\\) under the \\(H_0\\): \\({\\mathbf E = s_i \\otimes s_j} / n\\)\n\n\n\n\n\nClone A\nClone B\nClone C\n\n\n\n\nEpilimnion\n59.0\n49.6\n50.4\n\n\nHypolimnion\n78.0\n65.4\n66.6\n\n\n\n\n\n\n\n\n\n\nCalculate \\(\\hat{\\chi}^2 = \\sum_{i, j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\) with \\((n_{row} - 1) \\cdot (n_{col} - 1)\\) degrees of freedom.\n\n\nCompare with critical \\(\\chi^2\\) from table.\nNote: The results are only reliable if all observed frequencies are \\(\\geq 5\\).\nFor smaller samples, use Fisher’s exact test"
  },
  {
    "objectID": "slides/05-classtests.html#the-chi2-test-in-r",
    "href": "slides/05-classtests.html#the-chi2-test-in-r",
    "title": "05-Classical Tests",
    "section": "The \\(\\chi^2\\)-test in R",
    "text": "The \\(\\chi^2\\)-test in R\n\nOrganize data in a matrix with 3 rows (for the clones) and 2 columns (for the depths):\n\n\nx <- matrix(c(50, 37, 72, 87, 78, 45), ncol=2)\nx\n\n     [,1] [,2]\n[1,]   50   87\n[2,]   37   78\n[3,]   72   45\n\n\n\n\n\nchisq.test(x)\n\n\n    Pearson's Chi-squared test\n\ndata:  x\nX-squared = 24.255, df = 2, p-value = 5.408e-06"
  },
  {
    "objectID": "slides/05-classtests.html#fishers-exact-test",
    "href": "slides/05-classtests.html#fishers-exact-test",
    "title": "05-Classical Tests",
    "section": "Fisher’s exact test",
    "text": "Fisher’s exact test\n\n\n\nx <- matrix(c(50, 37, 72, 87, 78, 45), ncol=2)\nx\n\n     [,1] [,2]\n[1,]   50   87\n[2,]   37   78\n[3,]   72   45\n\n\n\nfisher.test(x)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  x\np-value = 5.807e-06\nalternative hypothesis: two.sided\n\n\n\n\n\\(\\rightarrow\\) significant correlation between the clones and their location.\n\ndependency between clones and vertical distribution in the lake"
  },
  {
    "objectID": "slides/05-classtests.html#favorite-numbers-of-hse-students",
    "href": "slides/05-classtests.html#favorite-numbers-of-hse-students",
    "title": "05-Classical Tests",
    "section": "Favorite numbers of HSE students",
    "text": "Favorite numbers of HSE students\n\n\nNumbers from 1..9, \\(n=34\\)\n\\(H_0\\): equal probability of all numbers \\(1/9\\) (discrete uniform distribution)\n\\(H_A\\): some numbers are favored \\(\\rightarrow\\) departure from discrete uniform"
  },
  {
    "objectID": "slides/05-classtests.html#chisquare-test",
    "href": "slides/05-classtests.html#chisquare-test",
    "title": "05-Classical Tests",
    "section": "Chisquare test",
    "text": "Chisquare test\n\n\n\nobsfreq <- c(1, 1, 6, 2, 2, 5, 8, 6, 3)\nchisq.test(obsfreq)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obsfreq\nX-squared = 13.647, df = 8, p-value = 0.09144\n\nchisq.test(obsfreq, simulate.p.value=TRUE, B=1000)\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 1000 replicates)\n\ndata:  obsfreq\nX-squared = 13.647, df = NA, p-value = 0.0969\n\n\n\n\n\none-sample \\(\\chi^2\\)-test. It tests for equality of frequency in all classes.\nThe simulation-based version of the test (with 1000 replicates) is slightly more precise than the standard \\(\\chi^2\\)-test, but both ar not significant."
  },
  {
    "objectID": "slides/05-classtests.html#cramér-von-mises-test",
    "href": "slides/05-classtests.html#cramér-von-mises-test",
    "title": "05-Classical Tests",
    "section": "Cramér-von-Mises-Test",
    "text": "Cramér-von-Mises-Test\n\n\\[\nT = n \\omega^2 = \\frac{1}{12n} + \\sum_{i=1}^n \\left[ \\frac{2i-1}{2n}-F(x_i) \\right]^2\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#cramér-von-mises-test-in-r",
    "href": "slides/05-classtests.html#cramér-von-mises-test-in-r",
    "title": "05-Classical Tests",
    "section": "Cramér-von-Mises-Test in R",
    "text": "Cramér-von-Mises-Test in R\n\nlibrary(dgof)\nobsfreq <- c(1, 1, 6, 2, 2, 5, 8, 6, 3)\n\n## CvM-test needs individual values, not class frequencies\nx <- rep(1:length(obsfreq), obsfreq)\nx\n\n [1] 1 2 3 3 3 3 3 3 4 4 5 5 6 6 6 6 6 7 7 7 7 7 7 7 7 8 8 8 8 8 8 9 9 9\n\n\n\n\n## create a cumulative function with equal probability of all cases\ncdf <- stepfun(1:9, cumsum(c(0, rep(1/9, 9))))\ncdf <- ecdf(1:9)\n\n## perform the test\ncvm.test(x, cdf)\n\n\n    Cramer-von Mises - W2\n\ndata:  x\nW2 = 0.51658, p-value = 0.03665\nalternative hypothesis: Two.sided\n\n\n\nThe Cramér-von-Mises-test works with the original, unbinned values\nUse of cumulative function respects order of classes \\(\\rightarrow\\) more powerful, than \\(\\chi^2\\)-test."
  },
  {
    "objectID": "slides/05-classtests.html#test-distribution-type-of-metric-variables",
    "href": "slides/05-classtests.html#test-distribution-type-of-metric-variables",
    "title": "05-Classical Tests",
    "section": "Test distribution type of metric variables",
    "text": "Test distribution type of metric variables\n\n\n\nhistogram, boxplot, quantile-quantile plot\nShapiro Wilk W-test\nBox-Cox method\nsee section Distributions"
  },
  {
    "objectID": "slides/05-classtests.html#correlation",
    "href": "slides/05-classtests.html#correlation",
    "title": "05-Classical Tests",
    "section": "Correlation",
    "text": "Correlation\n\nFrequencies of nominal variables\n\n\\(\\chi^2\\)-test\nFisher’s exact test\n\n⇒ dependence between plant society and soil type\n(see before)\nOrdinal variables\n\nSpearman-Correlation\n\n\\(\\rightarrow\\) rank numbers\nMetric scales\n\nPearson-correlation\nSpearman-correlation"
  },
  {
    "objectID": "slides/05-classtests.html#variance-and-covariance",
    "href": "slides/05-classtests.html#variance-and-covariance",
    "title": "05-Classical Tests",
    "section": "Variance and Covariance",
    "text": "Variance and Covariance\n\n\nVariance\n\nmeasures variation of a single variable\n\n\\[\n  s^2_x = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\n\\]\nCovariance\n\nmeasures how two variables change together\n\n\\[\n  q_{x,y} = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n\\]\nCorrelation: scaled to \\((-1, +1)\\)\n\\[\n  r_{x,y} = \\frac{q_{x,y}}{s_x \\cdot s_y}\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#correlation-coefficient-after-pearson",
    "href": "slides/05-classtests.html#correlation-coefficient-after-pearson",
    "title": "05-Classical Tests",
    "section": "Correlation coefficient after Pearson",
    "text": "Correlation coefficient after Pearson\n\n\n\nthe usual correlation coefficient that we all know\ntests for linear dependence\n\n\\[\nr_p=\\frac{\\sum{(x_i-\\bar{x})  (y_i-\\bar{y})}}\n       {\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n\\]\nOr:\n\\[\nr_p=\\frac {\\sum xy - \\sum y \\sum y / n}\n        {\\sqrt{(\\sum x^2-(\\sum x)^2/n)(\\sum y^2-(\\sum y)^2/n)}}\n\\] \nRange of values: \\(-1 \\le r_p \\le +1\\)\n\n\n\n\\(0\\)\nno interdependence\n\n\n\\(+1 \\,\\text{or}\\,-1\\)\nstrictly positive resp. negative dependence\n\n\n\\(0 < |r_p| < 1\\)\npositive resp. negative dependence"
  },
  {
    "objectID": "slides/05-classtests.html#which-size-of-correlation-indicates-dependency",
    "href": "slides/05-classtests.html#which-size-of-correlation-indicates-dependency",
    "title": "05-Classical Tests",
    "section": "Which size of correlation indicates dependency?",
    "text": "Which size of correlation indicates dependency?\n\n\n\n\n\n\n\\(r=0.4, \\quad p=0.0039\\)\n\n\n\n\n\n\n\n\\(r=0.85, \\quad p=0.07\\)"
  },
  {
    "objectID": "slides/05-classtests.html#significant-correlation",
    "href": "slides/05-classtests.html#significant-correlation",
    "title": "05-Classical Tests",
    "section": "Significant correlation?",
    "text": "Significant correlation?\n\\[\n\\hat{t}_{\\alpha/2;n-2} =\\frac{|r_p|\\sqrt{n-2}}{\\sqrt{1-r^2_p}}\n\\]\n\\(t=0.829 \\cdot \\sqrt{1000-2}/\\sqrt{1-0.829^2}=46.86, df=998\\)\n Quick test: critical values for \\(r_p\\)\n\n\n\n\\(n\\)\nd.f.\n\\(t\\)\n\\(r_{crit}\\)\n\n\n3\n1\n12.706\n0.997\n\n\n5\n3\n3.182\n0.878\n\n\n10\n8\n2.306\n0.633\n\n\n20\n18\n2.101\n0.445\n\n\n50\n48\n2.011\n0.280\n\n\n100\n98\n1.984\n0.197\n\n\n1000\n998\n1.962\n0.062"
  },
  {
    "objectID": "slides/05-classtests.html#rank-correlation-according-to-spearman",
    "href": "slides/05-classtests.html#rank-correlation-according-to-spearman",
    "title": "05-Classical Tests",
    "section": "Rank-correlation according to Spearman",
    "text": "Rank-correlation according to Spearman\n\n\nmeasures monotonous (and not necessarily linear) dependence\nestimation from rank differences:\n\n\\[\nr_s=1-\\frac{6 \\sum d^2_i}{n(n^2-1)}\n\\]\n\nor, alternatively: Pearson-correlation of ranked data (necessary in case of ties).\nTest: for \\(n < 10\\) \\(\\rightarrow\\) table of critical values\n\nfor \\(10 \\leq n\\) \\(\\rightarrow\\) \\(t\\)-distribution\n\\[\n   \\hat{t}_{1-\\frac{\\alpha}{2};n-2}\n      =\\frac{|r_s|}{\\sqrt{1-r^2_S}} \\sqrt{n-2}\n\\]\n\n\nComputer statistics packages use a special algorithm (algorithm AS 89 according to Best and Roberts, 1975)."
  },
  {
    "objectID": "slides/05-classtests.html#example",
    "href": "slides/05-classtests.html#example",
    "title": "05-Classical Tests",
    "section": "Example",
    "text": "Example\n\n\n\n\n\\(x\\)\n\\(y\\)\n\\(R_x\\)\n\\(R_y\\)\n\\(d\\)\n\\(d^2\\)\n\n\n\n\n1\n2.7\n1\n1\n0\n0\n\n\n2\n7.4\n2\n2\n0\n0\n\n\n3\n20.1\n3\n3\n0\n0\n\n\n4\n500.0\n4\n5\n-1\n1\n\n\n5\n148.4\n5\n4\n+1\n1\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\\[\nr_s=1-\\frac{6 \\cdot 2}{5\\cdot (25-1)}=1-\\frac{12}{120}=0.9\n\\]\nFor comparison: \\(r_p=0.58\\)"
  },
  {
    "objectID": "slides/05-classtests.html#application-of-spearmans-r_s",
    "href": "slides/05-classtests.html#application-of-spearmans-r_s",
    "title": "05-Classical Tests",
    "section": "Application of Spearman’s-\\(r_s\\)",
    "text": "Application of Spearman’s-\\(r_s\\)\n\nAdvantages\n\ndistribution free (does not require normal distribution),\ndetects any dependence,\nnot much affected by outliers.\n\nDisadvantages:\n\ncertain information loss due to ranking,\nno information about type of dependency,\nno direct relationship to coefficient of determination.\n\nConclusion: \\(r_s\\) is nevertheless highly recommended!"
  },
  {
    "objectID": "slides/05-classtests.html#correlation-coefficients-in-r",
    "href": "slides/05-classtests.html#correlation-coefficients-in-r",
    "title": "05-Classical Tests",
    "section": "Correlation coefficients in R",
    "text": "Correlation coefficients in R\n\nPearson’s product-moment correlation coefficient\nSpearman’s rank correlation coefficient\n\n\nx <- c(1, 2, 3, 5, 7,  9)\ny <- c(3, 2, 5, 6, 8, 11)\ncor.test(x, y, method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 7.969, df = 4, p-value = 0.001344\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7439930 0.9968284\nsample estimates:\n      cor \n0.9699203 \n\n\nIf linearity or normality of residuals is doubtful, use a rank correlation\n\ncor.test(x, y, method=\"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  x and y\nS = 2, p-value = 0.01667\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9428571"
  },
  {
    "objectID": "slides/05-classtests.html#problematic-cases",
    "href": "slides/05-classtests.html#problematic-cases",
    "title": "05-Classical Tests",
    "section": "Problematic cases",
    "text": "Problematic cases"
  },
  {
    "objectID": "slides/05-classtests.html#outlook-more-than-two-independent-variables",
    "href": "slides/05-classtests.html#outlook-more-than-two-independent-variables",
    "title": "05-Classical Tests",
    "section": "Outlook: More than two independent variables",
    "text": "Outlook: More than two independent variables\n\nMultiple correlation\n\nExample: Chl-a=\\(f(x_1, x_2, x_3, \\dots)\\), where \\(x_i\\) = biomass of the \\(i\\)th phytoplankton species.\nmultiple correlation coefficient\npartial correlation coefficient\nattractive method \\(\\leftrightarrow\\) but difficult in practice:\n\n“independent” variables may correlate with each other (multi-collinearity) \\(\\Rightarrow\\) bias of the multiple \\(r\\).\nnon-linearities are even more difficult to handle than in the two-sample case.\n\n\nRecommendation:\n\nUse multivariate methods (NMDS, PCA, …) for a first overview,\napply multiple regression with care and use process knowledge."
  },
  {
    "objectID": "slides/05-classtests.html#determining-the-power-of-statistical-tests",
    "href": "slides/05-classtests.html#determining-the-power-of-statistical-tests",
    "title": "05-Classical Tests",
    "section": "Determining the power of statistical tests",
    "text": "Determining the power of statistical tests\n\nHow many replicates will I need?\n\nDepends on:\n\nthe relative effect size \\(\\frac{\\mathrm{effect}}{\\mathrm{standard ~ deviation}}\\)\n\n\\[\\delta=\\frac{(\\bar{x}_1-\\bar{x}_2)}{s}\\]\n\nthe sample size \\(n\\)\nand the pre-defined significance level \\(\\alpha\\)\nand the applied method\n\nThe smaller \\(\\alpha\\), \\(n\\) and \\(\\delta\\), the bigger the type II (\\(\\beta\\)) error.\nThis is the probability to overlook effects despite of their existence."
  },
  {
    "objectID": "slides/05-classtests.html#power-analysis-1",
    "href": "slides/05-classtests.html#power-analysis-1",
    "title": "05-Classical Tests",
    "section": "Power analysis",
    "text": "Power analysis\n\nFormula for minimum sample size in the one-sample case:\n\\[\nn = \\bigg(\\frac{z_\\alpha + z_{1-\\beta}}{\\delta}\\bigg)^2\n\\]\n\n\\(z\\): the quantiles (qnorm) of the standard normal distribution for \\(\\alpha\\) and for \\(1-\\beta\\)\n\\(\\delta=\\Delta / s\\): relative effect size.\n\nExample\nTwo-tailed test with \\(\\alpha=0.025\\) and \\(\\beta=0.2\\)\n\\(\\rightarrow\\) \\(z_\\alpha = 1.96\\), \\(z_\\beta=0.84\\), then:\n\\[\nn= (1.96 \\pm 0.84)^2 \\cdot 1/\\delta^2 \\approx 8 /\\delta^2\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#power-of-the-t-test",
    "href": "slides/05-classtests.html#power-of-the-t-test",
    "title": "05-Classical Tests",
    "section": "Power of the t-test",
    "text": "Power of the t-test\nThe power of a t-test, or the minimum sample size, can be calculated with: power.t.test():\n\npower.t.test(n=5, delta=0.5, sig.level=0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.1038399\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\rightarrow\\) power = 0.10\n\nFor \\(n=5\\) an existing effect of \\(0.5\\sigma\\) is only detected in 1 out of 10 cases.\nFor a power of 80% at \\(n=5\\) we need an effect size of at least \\(2\\sigma\\):\n\n\npower.t.test(n=5, power=0.8, sig.level=0.05)\n\nFor a weak effect of \\(0.5\\sigma\\) we need a sample size of \\(n\\ge64\\) in each group:\n\npower.t.test(delta=0.5,power=0.8,sig.level=0.05)\n\n\\(\\Rightarrow\\) we ned either a large sample size or a strong effect."
  },
  {
    "objectID": "slides/05-classtests.html#simulated-power-of-a-t-test",
    "href": "slides/05-classtests.html#simulated-power-of-a-t-test",
    "title": "05-Classical Tests",
    "section": "Simulated power of a t-test",
    "text": "Simulated power of a t-test\n\n# population parameters\nn      <- 10\nxmean1 <- 50\nxmean2 <- 55\nxsd1   <- xsd2 <- 10\nalpha  <- 0.05\n\n# number of test runs in the simulation\nnn <- 1000\na <- b <- 0\nfor (i in 1:nn) {\n  # creating random numbers\n  x1 <- rnorm(n, xmean1, xsd1)\n  x2 <- rnorm(n, xmean2, xsd2)\n  # results of the t-test\n  p <- t.test(x1,x2,var.equal = TRUE)$p.value \n  if (p < alpha) {\n     a <- a+1\n   } else {\n     b <- b+1\n  }\n}\nprint(paste(\"a=\", a, \", b=\", b, \", a/n=\", a/nn, \", b/n=\", b/nn))"
  },
  {
    "objectID": "slides/05-classtests.html#references",
    "href": "slides/05-classtests.html#references",
    "title": "05-Classical Tests",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nHubbard, R. (2004). Alphabet soup: Blurring the distinctions betweenp’s anda’s in psychological research. Theory & Psychology, 14(3), 295–327. https://doi.org/10.1177/0959354304043638\n\n\nZimmerman, D. W. (2004). A note on preliminary tests of equality of variances. British Journal of Mathematical and Statistical Psychology, 57(1), 173–181. https://doi.org/10.1348/000711004849222"
  }
]