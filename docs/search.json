[
  {
    "objectID": "tutorials/s3-multivar-lakes.html",
    "href": "tutorials/s3-multivar-lakes.html",
    "title": "Multivariate Lake Data Example",
    "section": "",
    "text": "The following example demonstrates basic multivariate principles by means of a teaching example. A detailed description of theory and applications is found in excellent books of Legendre & Legendre (1998) and Borcard et al. (2018). Practical help is found in the tutorials of the vegan package (Oksanen et al., 2020)."
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#data-set-and-terms-of-use",
    "href": "tutorials/s3-multivar-lakes.html#data-set-and-terms-of-use",
    "title": "Multivariate Lake Data Example",
    "section": "2 Data set and terms of use",
    "text": "2 Data set and terms of use\nThe lake data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe document and codes provided here can be shared according to CC BY 4.0."
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#load-the-data",
    "href": "tutorials/s3-multivar-lakes.html#load-the-data",
    "title": "Multivariate Lake Data Example",
    "section": "3 Load the data",
    "text": "3 Load the data\nHere we load the data set and add English column names and abbreviated lake identifiers as row names to the table, that are useful for the multivariate plotting functions.\n\nlibrary(\"readxl\") # read Excel files directly\nlibrary(\"vegan\")  # multivariate statistics in ecology\nlakes <- as.data.frame(\n  read_excel(\"../data/uba/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) <- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nrownames(lakes) <- paste0(1:nrow(lakes), substr(lakes$name, 1, 4))\n\nText columns, e.g Federal State names and lake type are removed and rows with missing data excluded. If population is not used, the analysis can be repeated with more lakes.\n\nvalid_columns <- c(\"drainage\", \"population\", \"altitude\", \"z_mean\",\n                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                   \"shore_devel\", \"drain_ratio\")\n\n#valid_columns <- c(\"drainage\", \"altitude\", \"z_mean\",\n#                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n#                   \"shore_devel\",\"drain_ratio\")\ndat <- lakes[valid_columns]\ndat <- na.omit(dat)"
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#data-inspection",
    "href": "tutorials/s3-multivar-lakes.html#data-inspection",
    "title": "Multivariate Lake Data Example",
    "section": "4 Data inspection",
    "text": "4 Data inspection\nIt is alwas a good idea to plot the data first, as time series or boxplots for example, dependingon the type of data. Here we use boxplots, that we scale (z-transform) to a mean zero and standard deviation one to have comparable values.\nAs we can see a number of high extreme values, we apply also a square root transformation, that is less extreme than log transform and not sensitive against zero values, but because altitude contains a negative value (below sea level) we replace this with zero. As it is a small value, it does not influence our analysis, but we should always be very careful to document such workarounds.\n\npar(mfrow = c(1, 1))\npar(mar = c(7, 4, 2, 1) + .1)\nboxplot(scale(dat), las = 2)\n\n\n\ndat$altitude <- ifelse(dat$altitude < 0, 0, dat$altitude)\nboxplot(scale(sqrt(dat)), las=2)"
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#multivariate-analysis",
    "href": "tutorials/s3-multivar-lakes.html#multivariate-analysis",
    "title": "Multivariate Lake Data Example",
    "section": "5 Multivariate Analysis",
    "text": "5 Multivariate Analysis\n\n5.1 Principal Components: PCA\n\npc <- prcomp(scale(dat))\nsummary(pc)\n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.305 1.4737 1.1459 1.0686 0.84953 0.50024 0.24164\nProportion of Variance 0.483 0.1974 0.1194 0.1038 0.06561 0.02275 0.00531\nCumulative Proportion  0.483 0.6805 0.7998 0.9036 0.96925 0.99200 0.99731\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.12590 0.08400 0.07563 0.03077\nProportion of Variance 0.00144 0.00064 0.00052 0.00009\nCumulative Proportion  0.99875 0.99939 0.99991 1.00000\n\nplot(pc)\n\n\n\nbiplot(pc)\n\n\n\n\nAs the PCA with the untransformed data looks somewhat asymmetric, we repeat it with square transformed data. In addition, also the 3rd PC is plotted.\n\ndat2 <- sqrt(dat)\npc2 <- prcomp(scale(dat2))\nsummary(pc2)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.1886 1.5906 1.2499 1.0634 0.79782 0.44854 0.28572\nProportion of Variance 0.4354 0.2300 0.1420 0.1028 0.05786 0.01829 0.00742\nCumulative Proportion  0.4354 0.6654 0.8075 0.9103 0.96812 0.98641 0.99383\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.17665 0.13833 0.12041 0.05528\nProportion of Variance 0.00284 0.00174 0.00132 0.00028\nCumulative Proportion  0.99666 0.99840 0.99972 1.00000\n\npar(mfrow=c(1,2))\npar(mar=c(5, 4, 4, 2) + 0.1)\nbiplot(pc2, cex=0.6)\nbiplot(pc2, cex=0.6, choices=c(3, 2))\n\n\n\n\nA PCA is also possible with the rda function of the vegan package. The syntax of the plot functions is somewhat different. Instead of biplot as above, we can directly use plot. Details are found in the vegan documentation.\n\npar(mfrow=c(1,1))\npc3 <- rda(dat2, scale = TRUE)\npc3\n\nCall: rda(X = dat2, scale = TRUE)\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\nInertia is correlations \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n4.790 2.530 1.562 1.131 0.637 0.201 0.082 0.031 0.019 0.014 0.003 \n\n#summary(pc3)\nplot(pc3)\n\n\n\n\n\n\n5.2 Nonmetric Multidimensional Scaling: NMDS\nLt’s now perform an NMDS for the data set. Function metaMDS runs a series of NMDS fits with different start values to avoid local minima. It has also some automatic transformations built in and works usually with the Bray-Curtis dissimilarity, that is used for plants and animal species abundance data. As we work with physical data here, we set the distance measure to “euclidean”.\n\nmd <- metaMDS(dat2, scale = TRUE, distance = \"euclid\")\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1181117 \nRun 1 stress 0.1207022 \nRun 2 stress 0.1188526 \nRun 3 stress 0.1188534 \nRun 4 stress 0.1230331 \nRun 5 stress 0.1188257 \nRun 6 stress 0.1208973 \nRun 7 stress 0.1207018 \nRun 8 stress 0.1181117 \n... New best solution\n... Procrustes: rmse 1.728789e-05  max resid 5.144074e-05 \n... Similar to previous best\nRun 9 stress 0.1181116 \n... New best solution\n... Procrustes: rmse 4.843601e-05  max resid 0.0001439558 \n... Similar to previous best\nRun 10 stress 0.1188267 \nRun 11 stress 0.1181116 \n... Procrustes: rmse 9.483475e-05  max resid 0.0002783637 \n... Similar to previous best\nRun 12 stress 0.1188538 \nRun 13 stress 0.1181116 \n... Procrustes: rmse 0.000104247  max resid 0.0003014734 \n... Similar to previous best\nRun 14 stress 0.1181117 \n... Procrustes: rmse 8.306504e-05  max resid 0.0002417354 \n... Similar to previous best\nRun 15 stress 0.1181117 \n... Procrustes: rmse 4.608876e-05  max resid 0.0001336367 \n... Similar to previous best\nRun 16 stress 0.1188572 \nRun 17 stress 0.1188527 \nRun 18 stress 0.2076954 \nRun 19 stress 0.1181117 \n... Procrustes: rmse 6.38454e-05  max resid 0.0001867477 \n... Similar to previous best\nRun 20 stress 0.1208971 \n*** Best solution repeated 6 times\n\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\n\n\n\n\n5.3 Cluster analysis\nHere we apply a hierarchical cluster analysis with square root transformed data and two different agglomeration schemes, “complete linkage” and “Ward’s method”.\n\npar(mfrow=c(2,1))\nhc <- hclust(dist(scale(dat2)), method=\"complete\") # the default\nplot(hc)\n\nhc2 <- hclust(dist(scale(dat2)), method=\"ward.D2\")\nplot(hc2)\n\n\n\n\nWe can also use the clusters to indicate groups in the NMDS plot. Function rect.hclust indicates a given number of clusters in the dendrogram, then we cut the tree with cutree and use the groups grp as color codes. R has 8 standard colors. If we need more, we can define an own palette.\n\nplot(hc, hang = -1)\nrect.hclust(hc, 5)\n\n\n\ngrp <- cutree(hc, 5)\n# grp                  # can be used to show the groups\nplot(md, type = \"n\")\ntext(md$points, row.names(dat2), col = grp)\n\n\n\n\nInstead of hierarchical clustering, we can also use a non-hierarchical method, e.g. k-means clustering. This is an iterative method, and avoids the problem that cluster assignment depends on the order of clustering and the agglomeration method.\nDepending on the question, it may be a disadvantage, that the number of clusters needs to be specified beforehand (e.g. from hierarchical clustering) and that we do not get a tree diagramm."
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#task",
    "href": "tutorials/s3-multivar-lakes.html#task",
    "title": "Multivariate Lake Data Example",
    "section": "6 Task",
    "text": "6 Task\n\nTry to understand the analysis,\ndiscuss the results,\nask questions.\nThe idea is to work on this report together and to make it more complete."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html",
    "href": "tutorials/s1-introductory-r-session.html",
    "title": "An Introductory R Session",
    "section": "",
    "text": "This tutorial is available in HTML format for screen reading and PDF for printing."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#program-start-and-help-system",
    "href": "tutorials/s1-introductory-r-session.html#program-start-and-help-system",
    "title": "An Introductory R Session",
    "section": "2.1 Program start and help system",
    "text": "2.1 Program start and help system\nThe easiest way to learn R is the creative understanding and modification of given examples, the usage of R for solving practical problems and the diagnosis of the frequently occurring problems and error messages. Don’t worry: error messages are a normal phenomenon in scientific computing and not an indication of a dysfunction of the computer or the human brain. The opposite is true, a certain amount of stress hormones helps to acquire permanent learning effects. Then, after a certain level of experience reading the official R-Documentation “An Introduction to R” (Venables et al., 2021). or any good R-book is strongly recommended.\nThe first sections of this “crash course” are intended to give an overview over some of the most important elements of R and an insight into a typical work flow, that may be useful for the first statistical analyses and as a starting point for self-education.\nWe begin our first session by starting RStudio, a platform independent interface that makes working with R easier. RStudio divides the screen into 3 (resp. 4) windows (called panes), where some of them have additional tabs to switch between different views.\n\nFigure 1: R Studio with 4 panes. Use File – New R Script to open the the source code pane (shown top left). Then enter some code and don’t forget to explore the help files.\nIn a fresh RStudio session, one “Pane” should be the main help page of R. It is a good idea to browse a little bit around to get an impression about the amount and the typical style of the available help topics. The most important sections are “An Introduction to R”, “Search Engine & Keywords”, “Packages”, the “Frequently Asked Questions” and possibly “R Data Import/Export”.\nWe start now to explore the R-System itself."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#r-as-a-pocket-calculator",
    "href": "tutorials/s1-introductory-r-session.html#r-as-a-pocket-calculator",
    "title": "An Introductory R Session",
    "section": "2.2 R as a pocket calculator",
    "text": "2.2 R as a pocket calculator\nEntering an arithmetic expression like this:\n\n2 + 4\n\nshows that R can be used as a pocket calculator, that immediately outputs the result:\n\n\n[1] 6\n\n\nInstead of printing the result to the screen, it is also possible to save the result into a named variable using the assignment operator “<-”.\n\na <- 2 + 4\n\nIt seems that nothing happens, but the result is now saved in the variable a that can be recalled at any time by entering the variable name alone:\n\na\n\nVariable names in R start always with a character (or for special purposes a dot), followed by further characters, numerals, dots or underscores, where a distinction is made between small and capital letters, i.e. the variables value, Value and VALUE can contain different data. A few character combinations are reserved words and cannot be used as variables:\nbreak, for, function, if, in, next, repeat, while and “...” (three dots).\nOther identifiers like plot can be re-defined, but this should be done with care to avoid unwanted confusion and side effects."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#vectors",
    "href": "tutorials/s1-introductory-r-session.html#vectors",
    "title": "An Introductory R Session",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nYou may have noticed, that the output of the example above had a leading [1], which means that the line begins with the first element of a. This brings us to a very important feature of R that variables can contain more than single values: vectors, matrices, lists, data frames (tables) and so on.\nThe most basic data type is the vector, that can be filled with data using the c (combine) function:\n\nvalues <- c(2, 3, 5, 7, 8.3, 10)\nvalues\n\n[1]  2.0  3.0  5.0  7.0  8.3 10.0\n\n\nTo create a sequence of values, one can use the : (colon):\n\nx <- 1:10\nx\n\nor, even more flexibly the seq function:\n\nx <- seq(2, 4, 0.25)\nx\n\nSequences of repeated equal values can be obtained with rep:\n\nx <- rep(2, 4)\nx"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercise",
    "href": "tutorials/s1-introductory-r-session.html#exercise",
    "title": "An Introductory R Session",
    "section": "2.4 Exercise",
    "text": "2.4 Exercise\nThere are many ways to use these functions, try for example:\n\nseq(0, 10)\nseq(0, 10, by = 2)\nseq(0, pi, length = 12)\nrep(c(0, 1, 2, 4, 9), times = 5)\nrep(c(0, 1, 2, 4, 9), each = 2)\nrep(c(0, 1, 2, 4, 9), each = 2, times = 5)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#access-to-vector-elements",
    "href": "tutorials/s1-introductory-r-session.html#access-to-vector-elements",
    "title": "An Introductory R Session",
    "section": "2.5 Access to vector elements",
    "text": "2.5 Access to vector elements\nInstead of accessing vectors as a whole, it is also possible to extract single elements, where the index of the requested data is itself a vector:\n\nvalues[5]\nvalues[2:4]\nvalues[c(1, 3, 5)]\n\nSometimes, elements of a vector may have individual names, which makes it easy to access them:\n\nnamed <- c(a = 1, b = 2.3, c = 4.5)\nnamed\nnamed[\"a\"]\n\nIn R (and in contrast to other languages like C/C++) vector indices start with 1. Negative indices are also possible, but they have the special purpose to delete one or several elements:\n\nvalues[-3]\n\nIt is also possible to extend a given vector by preceding or appending values with the combine function (c):\n\nc(1, 1, values, 0, 0)\n\nThe length of a vector can be determined with:\n\nlength(values)\n\nand it is also possible to have empty vectors, i.e. vectors that exist, but do not contain any values. Here the keyword NULL means “nothing” in contrast to “0” (zero) that has length 1:\n\nvalues <- NULL\nvalues\nlength(values)\n\nSuch empty vectors are sometimes used as “containers” for appending data step by step:\n\nvalues <- NULL\nvalues\nlength(values)\nvalues <- c(values, 1)\nvalues\nvalues <- c(values, 1.34)\nvalues\n\nIf a data element should be removed completely, this can be done using the remove function:\nrm(values)\nvalues\nError: Object \"values\" not found\nThe complete workspace can be deleted from the menu of R or RStudio (Session – Clear workspace) or from the command line with rm (remove):\n\nrm(list = ls(all = TRUE))\n\nThe R session can be closed by using the menu as usual or by entering:\n\nq()\n\nSometimes and depending of the configuration, R asks whether the “R workspace” should be saved to the disk. This may be useful for continuing work at a later time, but has the risk to clutter the workspace and to get irreproducible results at a later session, so it is recommended to say “No” for now, except if you exactly know why.\nLater we will learn how to save only the data (and commands) that are needed."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#r-as-function-plotter",
    "href": "tutorials/s1-introductory-r-session.html#r-as-function-plotter",
    "title": "An Introductory R Session",
    "section": "3.1 R as function plotter",
    "text": "3.1 R as function plotter\nNow, we will see how to use R as a function plotter by drawing sine or cosine functions within an interval between 0 to 10. First, we create two vectors with x and y. To obtain a smooth curve, it is reasonable to choose a small step size. As a rule of thumb I always recommend to use about 100…400 small steps as a good compromise between smoothness and memory requirements, so let’s set the step size to 0.1:\n\nx <- seq(0, 10, 0.1)\ny <- sin(x)\nplot(x, y)\n\n\n\n\nInstead of plotting points, we can also draw continuous lines. This is indicated by supplying an optional argument type = \"l\".\nNote: the symbol used here for type is the small letter “L” for “line” and not the – in printing very similar – numeral “1” (one)!\nWe see also, that optional arguments like type can be given as “keyword = value” pair. This has the advantage that the order of arguments does not matter, because arguments are referenced by their name:\n\nplot(x, y, type = \"l\")\n\nNow we want to add a cosine function with another color. This can be done with one of the function lines or points, for adding lines or points to an existing figure:\n\ny1 <- cos(x)\nlines(x, y1, col = \"red\")\n\nWith the help of text it is also possible to add arbitrary text, by specifying first the x and y coordinates and then the text:\n\nx1 <- 1:10\ntext(x1, sin(x1), x1, col = \"green\")\n\nMany options exist to modify the behavior of most graphics functions so the following specifies user-defined coordinate limits (xlim, ylim), axis labels and a heading (xlab, ylab, main).\n\nplot(x, y, xlim = c(-10, 10), ylim = c(-2, 2),\n    xlab = \"x-Values\", ylab = \"y-Values\", main = \"Example Graphics\")\n\nCode formatting and line breaks\nThe above example shows a rather long command that may not fit on a single line. In such cases, R displays a + (plus sign) to indicate that a command must be continued, e.g. because a closing parenthesis or a closing quote is still missing. Such a + at the beginning of a line is an automatic “prompt” similar to the ordinary > prompt and must never be typed in manually. If, however, the + continuation prompt occurs by accident, press “ESC” to cancel this mode.\nIn contrast to the long line continuation prompt, it is also possible to write several commands on one line, separated by a semi-colon “;”. This is unseful in some cases, but as a general rule it is much better to use the script editor and then to:\n\nwrite each command to a separate line\navoid long lines with more than about 80 characters\nuse proper indentation, e.g. 2 characters per indentation level\nuse spacing to improve readability of the code, e.g. before and after the assignment operator <-.\n\nFinally, a number symbol (or hash) # means that a complete line or the part of the line that follows # is a comment and should be ignored by R."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#additional-plotting-options",
    "href": "tutorials/s1-introductory-r-session.html#additional-plotting-options",
    "title": "An Introductory R Session",
    "section": "3.2 Additional plotting options",
    "text": "3.2 Additional plotting options\nIn order to explore the wealth of graphical functions, you may now have a more extensive look into the online help, especially regarding ?plot or ?plot.default, and you should experiment a little bit with different plotting parameters, like lty, pch, lwd, type, log etc. R contains uncountable possibilities to get full control over the style and content of your graphics, e.g. with user-specified axes (axis), legends (legend) or user-defined lines and areas (abline, rect, polygon). The general style of figures like (font size, margins, line width) can be influenced with the par function."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#high-level-plotting-functions",
    "href": "tutorials/s1-introductory-r-session.html#high-level-plotting-functions",
    "title": "An Introductory R Session",
    "section": "3.3 High level plotting functions",
    "text": "3.3 High level plotting functions\nIn addition, R and its packages contain numerous “high level”-graphics functions for specific purposes. To demonstrate a few, we first generate a data set with normally distributed random numbers (mean = 0, standard deviation sd = 1), then we plot them and create a histogram. Here, the function par(mfrow = c(2, 2)) divides the plotting area into 2 rows and 2 columns to show 4 separate figures:\n\npar(mfrow = c(2, 2))\nx <- rnorm(100)\nplot(x)\nhist(x)\n\nNow, we add a so-called normal probability plot and a second histogram with relative frequencies together with the bell-shaped density curve of the standard normal distribution. The optional argument probability = TRUE makes sure that the histogram has the same scaling as the density function, so that both can be overlayed:\n\nqqnorm(x)\nqqline(x, col = \"red\")\nhist(x, probability = TRUE)\nxx <- seq(-3, 3, 0.1)\nlines(xx, dnorm(xx, 0, 1), col = \"red\")\n\n\n\n\n\n\n\nHere it may also be a good chance to do a little bit summary statistics like: z.B. mean(x), var(x), sd(x), range(x), summary(x), min(x), max(x), …\nOr we may consider to test if the generated random numbers x are approximately normal distributed using the Shapiro-Wilks-W-Test:\n\nx <- rnorm(100)\nshapiro.test(x)\n\nA p-value bigger than 0.05 tells us that the test has no objections against normal distribution of the data. The concrete results may differ, because x contains random numbers, so it makes sense to repeat this several times. It can be also useful compare these normally distributed random numbers generated with rnorm with uniformly distributed random numbers generated with runif:\n\npar(mfrow=c(2,2))\ny <- runif(100)\nplot(y)\nhist(y)\nqqnorm(y)\nqqline(y, col=\"red\")\nmean(y)\nvar(y)\nmin(y)\nmax(y)\nhist(y, probability=TRUE)\nyy <- seq(min(y), max(y), length = 50)\nlines(yy, dnorm(yy, mean(y), sd(y)), col = \"red\")\nshapiro.test(y)\n\nAt the end, we compare the pattern of both data sets with box-and-whisker plots:\n\npar(mfrow=c(1, 1))\nboxplot(x, y)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercises",
    "href": "tutorials/s1-introductory-r-session.html#exercises",
    "title": "An Introductory R Session",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nRepeat this example with new random numbers and vary sample size (n), mean value (mean) and standard deviation (sd) for random numbers created with rnorm, and use different min and max for runif. Consult the help pages for an explanation of the functions and its arguments, and create boxplots with different data sets."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#numeric-and-character-vectors",
    "href": "tutorials/s1-introductory-r-session.html#numeric-and-character-vectors",
    "title": "An Introductory R Session",
    "section": "4.1 Numeric and character vectors",
    "text": "4.1 Numeric and character vectors\nAll data objects have the two built-in attributes mode (data type) and length (number of data in the object).\nModes can be “numeric” for calculations or “character” for text elements.\n\nx <- c(1, 3, 4, 5)       # numeric\na <- c(\"hello\", \"world\") # character\n\nThe following is also a character variable, because the numbers are given in quotes. It is then not possible to do calculations:\n\nx <- c(\"1\", \"3\", \"4\", \"5\")  # character\nsum(x)\n\nError in sum(x) : invalid 'type' (character) of argument\n\nHere it is necessary to convert the character to numeric first:\n\ny <- as.numeric(x)\nsum(y)\n\n[1] 9.040591"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#factors",
    "href": "tutorials/s1-introductory-r-session.html#factors",
    "title": "An Introductory R Session",
    "section": "4.2 Factors",
    "text": "4.2 Factors\nA special kind of mode is factor. This is, statistically speaking, a nominal variable that appears like characters, e.g. “control”, “treatment A”, “treatment B” …, but its levels are internally encoded as integer.\nHere a typical example with three factor levels. In a first step,let’s create a character variable:\n\ntext <- rep(c(\"control\", \"treatment A\", \"treatment B\"), each=5)\ntext\n\n [1] \"control\"     \"control\"     \"control\"     \"control\"     \"control\"    \n [6] \"treatment A\" \"treatment A\" \"treatment A\" \"treatment A\" \"treatment A\"\n[11] \"treatment B\" \"treatment B\" \"treatment B\" \"treatment B\" \"treatment B\"\n\n\nand then convert it to a factor:\n\nf <- factor(text)\nf\n\n [1] control     control     control     control     control     treatment A\n [7] treatment A treatment A treatment A treatment A treatment B treatment B\n[13] treatment B treatment B treatment B\nLevels: control treatment A treatment B\n\n\nWe se that the character variable is printed with quotes and the factor without quotes, but with an additional information about the Levels. The reason for this is, that the factor is internally encoded as integer values with assigned levels as a translation table:\n\nlevels(f)\n\n[1] \"control\"     \"treatment A\" \"treatment B\"\n\n\nTo show encoding, we can convert the factor into an integer\n\nas.integer(f)\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3\n\n\nThe encoding is done in alphabetical order by default. It can be changed by using an additional levels argument:\n\nf2 <- factor(text, levels=c(\"treatment A\", \"treatment B\", \"control\"))\nf2\n\n [1] control     control     control     control     control     treatment A\n [7] treatment A treatment A treatment A treatment A treatment B treatment B\n[13] treatment B treatment B treatment B\nLevels: treatment A treatment B control\n\nas.numeric(f2)\n\n [1] 3 3 3 3 3 1 1 1 1 1 2 2 2 2 2\n\n\nFactors are useful for statistical analyses like ANOVA and statistical tests, and also as categories for plotting:\n\nx <- c(7.44, 6.45, 6.04, 5.58, 4.5, 8.13, 5.54, 7.34, 8.91, 5.16, 8.7, 7.74, 6.8, 6.49, 6.2)\nplot(x ~ f)\n\n\n\n\nWe see that a boxplot is created and no x-y-plot, because the explanation variable is a factor.\nNumeric variables (especially ordinal) can also be converted to factors. This is useful, if we want to make clear, that numbers are to be treated as name without order.\nHowever, conversion of such factors back into numeric variables types should be done with care, because a character “123” may be encoded with another value (e.g. 1) and not 123, see the following demonstration of a correct and wrong factor conversions:\n\nx <- c(2, 4, 6, 5, 8)\nf <- as.factor(x)\nas.numeric(f)               # wrong !!!\nas.numeric(as.character(f)) # correct\nas.numeric(levels(f))[f]    # even better\n\nSuch a factor coding is not specific to R and appears also in other statistics packages. Then they are sometimes called “dummy variables”."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#matrices-and-arrays",
    "href": "tutorials/s1-introductory-r-session.html#matrices-and-arrays",
    "title": "An Introductory R Session",
    "section": "4.3 Matrices and arrays",
    "text": "4.3 Matrices and arrays\nA matrix is a two-dimensional data structure that can be used for matrix algebra. To create a matrix, we can first create a one-dimensional vector and then reformat it as two-dimensional matrix with nrow rows and ncolcolumns:\n\nx <- 1:20\nx\n\ny <- matrix(x, nrow = 5, ncol = 4)\ny\n\nWe see that the matrix is filled rowwise. We can also convert it back to a vector:\n\nas.vector(y) # flattens the matrix to a vector\n\nAn array extends the matrix concept to more than two dimensions:\n\nx <- array(1:24, dim=c(3, 4, 2))\n\nVectors, matrices and arrays have an important limitation: they can only contain one data type (mode), either numeric or character. So, if a single element is of type character, the whole matrix will be of mode character and appears in quotes:\n\nx <- c(1, 2, 5, 2, \"a\")\nmode(x)\nx"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#lists",
    "href": "tutorials/s1-introductory-r-session.html#lists",
    "title": "An Introductory R Session",
    "section": "4.4 Lists",
    "text": "4.4 Lists\nThe most flexible data type of R is the list. It can contain arbitrary data of different modes. Lists can be nested to form a tree-like structure:\n\nl <- list(x = 1:3, y = c(1,2,3,4), a = \"hello\", L = list(x = 1, y = 2))\n\nLists are extremely powerful and flexible and may be discussed later. The impatient may have a look at the tutorial of w3schools.com."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#data-frames",
    "href": "tutorials/s1-introductory-r-session.html#data-frames",
    "title": "An Introductory R Session",
    "section": "4.5 Data frames",
    "text": "4.5 Data frames\nThe typical data structure for data analysis in R is the so-called data.frame. It can contain both, columns with numeric data and columns of mode character. Some packages use ando extended versions of data frames, a so called tibbles.\nA data frame can be constructed from scratch directly in the R code or read from a file or the internet. As an example, students were asked in different years for their favorite number from one to 9. The results can be put in a data frame like follows:\n\nfavnum <- data.frame(\n  favorite = 1:9,\n  obs2019  = c(1, 1, 6, 2, 2,  5,  8, 6, 3),\n  obs2020  = c(1, 2, 8, 1, 2,  2, 20, 2, 4),\n  obs2021  = c(2, 6, 8, 1, 6,  4, 13, 2, 4),\n  obs2022  = c(2, 3, 7, 8, 2, 10, 12, 6, 1)\n)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#direct-input",
    "href": "tutorials/s1-introductory-r-session.html#direct-input",
    "title": "An Introductory R Session",
    "section": "5.1 Direct input",
    "text": "5.1 Direct input\nWe used this method already when creating vectors with the c (combine)-Function:\n\nx <- c(1, 2, 5, 7, 3, 4, 5, 8)\nx\n\nIn the same way it is possible to create other data types like data frames:\n\ndat <- data.frame(f = c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"),\n                  x = c(1,   4,   3,   3,   5,   7)\n       )\ndat\n\nor matrices:\n\nA <- matrix(c(1:9), nrow=3)\nA\n\nWe see that a matrix is not much different from a vector, formatted into rows and columns."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#read-data-from-a-text-file",
    "href": "tutorials/s1-introductory-r-session.html#read-data-from-a-text-file",
    "title": "An Introductory R Session",
    "section": "5.2 Read data from a text file",
    "text": "5.2 Read data from a text file\nR has very flexible functions to read data from text files. Let’s for example use a table that contains some data from a lake area in north-eastern Germany (Table 1).\nTable 1: Morphometrical and chemical properties of selected lakes (S=Stechlinsee, NN=Nehmitzsee Nord, NS=Nehmitzsee Süd, BL=Breiter Luzin, SL = Schmaler Luzin, DA = Dagowsee, HS = Feldberger Haussee; z=mean depth (m), t=theoretical retention time (a), P=phosphorus concentration (\\(\\mathrm{\\mu g L^{-1}}\\)), N=nitrogen concentration (\\(\\mathrm{mg L{^-1}}\\)), Chl=chlorophyll concentration (\\(\\mathrm{\\mu g L^{-1}}\\)), PP=annual primary production (\\(\\mathrm{g C m^{-2} a^{-1}}\\)), SD = secchi depth (m)). The data are an adapted and simplified “toy version” taken from Casper (1985) and Koschel & Scheffler (1985).\n\n\n\n\n\nLake\nz\nt\nP\nN\nChl\nPP\nSD\n\n\n\n\nS\n23.7\n40\n2.5\n0.20\n0.7\n95\n8.4\n\n\nNN\n5.9\n10\n2.0\n0.20\n1.1\n140\n7.4\n\n\nNS\n7.1\n10\n2.5\n0.10\n0.9\n145\n6.5\n\n\nBL\n25.2\n17\n50.0\n0.10\n6.1\n210\n3.8\n\n\nSL\n7.8\n2\n30.0\n0.10\n4.7\n200\n3.7\n\n\nDA\n5.0\n4\n100.0\n0.50\n14.9\n250\n1.9\n\n\nHS\n6.3\n4\n1150.0\n0.75\n17.5\n420\n1.6\n\n\n\n\n\nThe data can be downloaded from https://github.com/tpetzoldt/datasets/tree/main/data\n\n5.2.1 Set working directory\nR needs to know where to find the data on your computer. One way is to provide the full path to the data set, e.g. if it is c:/users/<username>/documents, then\n\nlakes <- read.csv(\"c:/users/julia/documents/lakes.csv\")\n\nThis can be cumbersome and error-prone, so the preferred method is to set the working directory of R to the data directory. This can be done in RStudio like follows:\n\nLocate the folder with the data in the “Files” pane\nSelect “More”\nSelect “Set As Working Directory”\n\n Figure2: Setting the working directory in RStudio\nAfter this, data can be retrieved directly from the working directory :\n\nlakes <- read.csv(\"lakes.csv\", header=TRUE)\n\nWe may also consider to create a sub-folder data of the working direktory and put the data in. Then we could use for example:\n\nlakes <- read.csv(\"../data/lakes.csv\", header=TRUE)\n\nNote also that we use always the ordinary slash “/” and not the backslash “\\”, even on Windows.\nIn some countries that have the comma and not the dot as a decimal separator and then for example a semicolon as column separator, additional arguments dec = \",\", sep=\";\" may be required. The details are found on the read.table help page."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#read-data-from-the-internet",
    "href": "tutorials/s1-introductory-r-session.html#read-data-from-the-internet",
    "title": "An Introductory R Session",
    "section": "5.3 Read data from the internet",
    "text": "5.3 Read data from the internet\nIf the data are available on an internet server, it can be read directly from there:\n\nlakes <- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/lakes.csv\")\n\nNow, as the data are saved in the data frame lakes it is possible to access them as usual:\n\nlakes\nsummary(lakes)\nboxplot(lakes[-1])\n\nHere summary shows a quick overview and boxplot creates a boxplot for all columns except the first, that contains no numbers.\nNow, we are ready to inspect the content of this new variable lakes. If we use RStudio a View can be invoked by clicking to lakes in the environment window."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#import-dataset-in-rstudio",
    "href": "tutorials/s1-introductory-r-session.html#import-dataset-in-rstudio",
    "title": "An Introductory R Session",
    "section": "5.4 “Import Dataset” in RStudio",
    "text": "5.4 “Import Dataset” in RStudio\nRStudio contains a handy feature that makes importing of data more convenient. Essentially, this “Import Dataset” wizard helps us to construct the correct read.table, read.csv or read_delim function interactively. It is possible to try different options until a satisfying result is obtained. Current versions of RStudio contain several different ways to import data. Here we demonstrate the “Import Dataset From Text (readr)” assistant:\n\nFrom the menu select: File – Import DataSet – From CSV.\nSelect the requested file and select suitable options like the name of the variable the data are to be assigned to, the delimiter character (comma or Tab) and whether the first row of the file contains variable names.\n\n\nImport Dataset From Text (readr) assistant of RStudio.\nHint: In the exmple above, the name of the data frame is identical to the file name, i.e. “lakes”. If we want to name it differently (e.g.: dat), we must not forget to change this setting.\nNote also that the Code Preview contains the commands that the wizard created. If we copy these commands to the script pane, you can re-read the data several times without going back to the menu system:\n\nlibrary(readr)\nlakes <- read_csv(\"D:/DATA/lakes.csv\")\nView(lakes)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#display-the-content-of-the-data-frame",
    "href": "tutorials/s1-introductory-r-session.html#display-the-content-of-the-data-frame",
    "title": "An Introductory R Session",
    "section": "6.1 Display the content of the data frame",
    "text": "6.1 Display the content of the data frame\nThe easiest way is to just enter the name of the data frame to the R console, e.g.:\n\nlakes\n\nor to click to the name of the data frame in the “Environment” explorer of RStudio. This executes then View(lakes), so that the data are shown.\nFor large tables it is often not very useful to display the full content with View, so it may be better to use the function str (structure) that gives a compact overview over type, size and content of a variable:\n\nstr(mydata)\n\nThe str function is universal and also suitable for complicated object types like lists. Of course, there are many more possibilities for inspecting the content of a variable:\n\nnames(lakes)\nmode(lakes)\nlength(lakes)\n\nand sometimes even:\n\nplot(lakes)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#access-single-columns-with",
    "href": "tutorials/s1-introductory-r-session.html#access-single-columns-with",
    "title": "An Introductory R Session",
    "section": "6.2 Access single columns with $",
    "text": "6.2 Access single columns with $\nSingle columns of a data frame can be accessed by using indices (with []) similar to a vector or a matrix or by using the column name and the $) operator:\n\nmean(lakes[,2])\nmean(lakes$z)\nmean(lakes[,\"z\"])\nmean(lakes[[\"z\"]])\nplot(lakes$z, lakes$t)\n\nwhere z is the mean depth of the lakes and t the so-called mean residence time.\nWe should also nitice the subtle difference of the output of the [] and the [[]]- version. The difference is as follows: single brackets return a data frame with one column, but double square brackets return the content of the column as a vector without the caption.\nWarning: In some older books, the $-style is sometimes abbreviated using the attach and detach-functions. This “prehistoric relict” is strongly discouraged, as it can lead to data inconsistency and strange errors. If you find it somewhere where it is still used, then it is a good idea to use detach repeatedly until an error message confirms us that there is nothing else that can be detached. Finally: never use attach/detach in a package.\nInstead, it is much better to use another function width, that opens the data frame only temporarily:\n\nwith(lakes, plot(z, t))"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#subsets-and-logical-indices",
    "href": "tutorials/s1-introductory-r-session.html#subsets-and-logical-indices",
    "title": "An Introductory R Session",
    "section": "6.3 Subsets and logical indices",
    "text": "6.3 Subsets and logical indices\nIn the following, we use another data set that we read directly from the internet:\n\nfruits <- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/clementines.csv\")\n\nIt contains weight, width and height measurements of two brands of Clementine fruits. After loading the data, we look at it with View(fruits) or the Environment explorer of Rstudio\nA very powerful feature of R is the use of logical vectors as “indices”, with similar results like data base queries. As an example, we can show the weight of all fruits of brand “A” with\n\nfruits[fruits$brand == \"A\", c(\"brand\", \"weight\")]\n\n   brand weight\n6      A     81\n7      A    113\n8      A     94\n9      A     86\n10     A    108\n11     A     91\n12     A     94\n13     A     86\n14     A     91\n15     A     88\n\n\nHere the first indext in the square brackets indicates the subset of rows that we want and the second argument the columns. If we want to see all columns, we leave the argument after the comma empty:\n\nfruits[fruits$brand == \"A\", ]\n\n   id no brand weight width height\n6   6  7     A     81    53     54\n7   7  8     A    113    61     60\n8   8  9     A     94    62     49\n9   9  3     A     86    58     53\n10 10  6     A    108    64     50\n11 11  1     A     91    59     51\n12 12 10     A     94    59     51\n13 13  4     A     86    55     55\n14 14  2     A     91    61     51\n15 15  5     A     88    54     55\n\n\nA logical comparison requires always a double “==”. Logical operations like & (and) and | (or) are also possible. Note that “and” has always precedence before “or”, except this is changed with parenthesis.\nA subset of a data frame can also be extracted with the subset function:\n\nbrand_B <- subset(fruits, brand == \"B\")\nbrand_B\n\nLike in the example before, the condition argument allows also logical expressions with & (and) and | (or).\nWe can also access single elements in matrix-like manner.\nThe element from the 2nd row and the 4th column can be selected with:\n\nfruits[2, 4]\n\nthe complete 5th row with:\n\nfruits[5, ]\n\nand rows 5:10 of the 4th column (weight) with:\n\nfruits[5:10, 4]\n\nAdditional methods for working with matrices, data frames and lists can be found in R textbooks or in the official R documentation."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#pipelines-and-summaries",
    "href": "tutorials/s1-introductory-r-session.html#pipelines-and-summaries",
    "title": "An Introductory R Session",
    "section": "7.1 Pipelines and summaries",
    "text": "7.1 Pipelines and summaries\nThe last examples are intended to demonstrate how powerful a single line can be in R. How to analyse data sets evolved over the history of R, so there is for example a function aggregate to compute statistics (e.g. mean values) depending on given criteria.\nThis works well and is still used, but the modern methods are more compact and easier to understand. Here let’s introduce a modern concept first, that is called “pipelining”. The idea is, that the result of a function is directly pipelined to another function, so instead of writing:\n\nbrand_B <- subset(fruits, brand == \"B\")\ncolumns_B <- brand_B[c(\"weight\", \"width\", \"height\")]\nmeans_B <- colMeans(columns_B)\nmeans_B\n\nwe can directly write:\n\nlibrary(\"dplyr\")\nfruits |> filter(brand == \"B\") |> select(weight, width, height) |> colMeans()\n\nHere we load an add-on package dplyr first, that contains a lot of helpful functions for data management, for example filter that selects rows and select that selects columns. The pipeline operator |> pipes then the data set fruitsto the filter- function and subsequently to the next. The “native pipeline operator” |> was introduced with R 4.1. As an alternative we can also use the %>% pipeline operator, that is loaded by the dplyr package. Its function would be identical in this case.\nTwo other extremely useful dplyr functions are group_by and summary, that allow to calculate arbitrary summary statistics in dependence of grouping variables. If we use the brand for grouping, we can summarize all groups simultaneously:\n\nfruits |> \n  group_by(brand) |>\n  summarise(mean(weight), mean(width), mean(height))\n\nThe summarize line above can still be made better, and there plenty of other stunning possibilities, so we will come back to this later."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#output-of-results",
    "href": "tutorials/s1-introductory-r-session.html#output-of-results",
    "title": "An Introductory R Session",
    "section": "7.2 Output of Results",
    "text": "7.2 Output of Results\nThe most simple method to save outputs from R is to copy it directly from the R console to any other program (e.g. LibreOffice, Microsoft Word or Powerpoint) via the Clipboard. This is convenient, but cannot be automated. Therefore, it is better to use a programmatic approach.\nLet’s use the example before and store the results in a new data frame results:\n\nresults <-\n  fruits |> \n  group_by(brand) |>\n  summarise(mean(weight), mean(width), mean(height))\n\nData frames can be saved as text files with write.table, write.csv or write_csv. Here we use write_csv (with underscore, not dot) from package readr:\n\nlibrary(\"readr\")\nwrite_csv(results, file=\"output-data.csv\")\n\nIn addition to these basic functions R has a wealth of possibilities to save output and data for later use in reports and presentations. All of them are of course documented in the online help, e.g. print, print.table, cat for text files, and pdf, png for figures. The add-on packages xtable contains functions for creating LaTeX or HTML-tables while full HTML output is supported by the R2HTML or knitr packages."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#plots-with-ggplot2",
    "href": "tutorials/s1-introductory-r-session.html#plots-with-ggplot2",
    "title": "An Introductory R Session",
    "section": "7.3 Plots with ggplot2",
    "text": "7.3 Plots with ggplot2\nIn addition to the plot functions we used so far, other plot packages exist, for example lattice or ggplot2. Here a few small examples with the very popular **ggplot2* package:\n\nlibrary(\"ggplot2\")\nfruits |>\n  ggplot(aes(brand, weight)) + geom_boxplot()\n\n\n\n\nOr a scatterplot to compare weight and width of the fruits\n\nfruits |> ggplot(aes(weight, width)) + geom_point(aes(color=brand))\n\n\n\n\nwhere the brand is indicated as color. Another option could be:\n\nfruits |> ggplot(aes(weight, width)) + \n  geom_point() + \n  geom_smooth(method=\"lm\") + \n  facet_grid(~brand)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercises-1",
    "href": "tutorials/s1-introductory-r-session.html#exercises-1",
    "title": "An Introductory R Session",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\nR contains lots of data sets for exploring its graphical and statistical functions and that can be activated by using the data function, e.g. data(iris) or data(cars). A description of the data set can be found as usual in the help files, e.g. ?iris, ?cars.\nUse one of these data sets and try\n\nways to access columns, to select rows and to create subsets\nways for summary statistics and visualization with R’s base plot functions and optionally with ggplot."
  },
  {
    "objectID": "slides/10-multivariate-1.html#data-sets-and-terms-of-use",
    "href": "slides/10-multivariate-1.html#data-sets-and-terms-of-use",
    "title": "10-Multivariate methods I",
    "section": "Data sets and terms of use",
    "text": "Data sets and terms of use\n\n\nThe “UBA-lakes” data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe “gauernitz” data set contains simplified teaching versions from research data, of the study from Winkelmann et al. (2011)\nThe document itself, the codes and the ebedded images are own work and can be shared according to CC BY 4.0."
  },
  {
    "objectID": "slides/10-multivariate-1.html#an-introductory-example",
    "href": "slides/10-multivariate-1.html#an-introductory-example",
    "title": "10-Multivariate methods I",
    "section": "An introductory example",
    "text": "An introductory example"
  },
  {
    "objectID": "slides/10-multivariate-1.html#correlation-between-all-variables",
    "href": "slides/10-multivariate-1.html#correlation-between-all-variables",
    "title": "10-Multivariate methods I",
    "section": "Correlation between all variables?",
    "text": "Correlation between all variables?\n\nlibrary(\"readxl\") # read Excel files directly\nlakes <- as.data.frame(\n  read_excel(\"../data/uba/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) <- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nstr(lakes)\n\n'data.frame':   56 obs. of  14 variables:\n $ name        : chr  \"Ammersee\" \"Arendsee\" \"Bleilochtalsperre\" \"Bodensee\" ...\n $ state       : chr  \"BY\" \"ST\" \"TH\" \"BW\" ...\n $ drainage    : num  993 29.8 1239.9 11477 28.2 ...\n $ population  : num  114900 3200 180000 1400000 28 ...\n $ altitude    : num  532.9 22.8 404 395.4 13.1 ...\n $ z_mean      : num  37.6 28.6 23.3 85 2.4 ...\n $ z_max       : num  81.1 48.7 55 254 4.8 58.3 32.5 73.4 1.7 18.8 ...\n $ t_ret       : num  2.7 50 0.526 4.2 1.9 16.3 NA 1.26 0.1 2.3 ...\n $ volume      : num  1.75 0.147 0.215 48.522 0.009 ...\n $ area        : num  46.6 5.14 9.2 571.5 3.9 ...\n $ shore_length: num  43 9 NA 273 10.4 13.7 17.5 64 7.5 10.1 ...\n $ shore_devel : num  1.78 1.12 NA 2.26 1.48 2.09 1.64 2.02 2.22 1.6 ...\n $ drain_ratio : num  20.3 5.8 NA 21.9 7.2 ...\n $ wfd_type    : chr  \"Typ 4\" \"Typ 13\" \"Typ 5\" \"Typ 4\" ...\n\n\n\nnames(lakes) replaces the original German column names by abbreviated English abbreviations"
  },
  {
    "objectID": "slides/10-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables",
    "href": "slides/10-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables",
    "title": "10-Multivariate methods I",
    "section": "Create pairwise scatter plots for all variables?",
    "text": "Create pairwise scatter plots for all variables?\n\nplot(lakes)"
  },
  {
    "objectID": "slides/10-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables-1",
    "href": "slides/10-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables-1",
    "title": "10-Multivariate methods I",
    "section": "Create pairwise scatter plots for all variables?",
    "text": "Create pairwise scatter plots for all variables?\n\n\nnot a good idea, 14 variables would produce 182 (or 91) plots\ncan lead to “statistical fishing”\nwe need methods to extract the main information with a small number of plots"
  },
  {
    "objectID": "slides/10-multivariate-1.html#the-multivariate-approach",
    "href": "slides/10-multivariate-1.html#the-multivariate-approach",
    "title": "10-Multivariate methods I",
    "section": "The multivariate approach",
    "text": "The multivariate approach\n\nWork with the complete original variables directly\n\n\\(\\rightarrow\\) Multivariate statistics\n\ndependent variables + explanation variables optional\nanalyze distance and location in multidimensional space\nfind way to vizualize relationship in lower dimensions\n\n\nFor comparison\n\nunivariate: 1 variable\nbivariate: 1 dependent, 1 independent variable\nmultiple: 1 dependent, >1 independent variables"
  },
  {
    "objectID": "slides/10-multivariate-1.html#two-approches-of-multivariate-statistics",
    "href": "slides/10-multivariate-1.html#two-approches-of-multivariate-statistics",
    "title": "10-Multivariate methods I",
    "section": "Two approches of multivariate statistics",
    "text": "Two approches of multivariate statistics\n\n\n\n\nOrdination\n\n\n\n\n\n\ndimension reduction\nrelate observations and variables\n\n\n\nCluster analysis\n\n\n\n\n\n\ndistance and similarity\nidentification of groups"
  },
  {
    "objectID": "slides/10-multivariate-1.html#basic-concepts",
    "href": "slides/10-multivariate-1.html#basic-concepts",
    "title": "10-Multivariate methods I",
    "section": "Basic concepts",
    "text": "Basic concepts\n\nSimilarity and correlation\n\ndistance and similarity: \\(\\rightarrow\\) How different or similar are the observations?\ncorrelation and covariance: \\(\\rightarrow\\) Are variables interdependent?\ndimension reduction: \\(\\rightarrow\\) Try to show essential parts of information on a lower number of dimensions.\ncluster analysis: \\(\\rightarrow\\) Show which observations are closely together.\nordination: \\(\\rightarrow\\) Plot data at lower dimensions, similar observations closely together."
  },
  {
    "objectID": "slides/10-multivariate-1.html#a-data-set-from-german-lakes",
    "href": "slides/10-multivariate-1.html#a-data-set-from-german-lakes",
    "title": "10-Multivariate methods I",
    "section": "A data set from German lakes",
    "text": "A data set from German lakes"
  },
  {
    "objectID": "slides/10-multivariate-1.html#example-a-simplified-subset-from-the-uba-lake-data",
    "href": "slides/10-multivariate-1.html#example-a-simplified-subset-from-the-uba-lake-data",
    "title": "10-Multivariate methods I",
    "section": "Example: A simplified subset from the UBA lake data",
    "text": "Example: A simplified subset from the UBA lake data\n\n\n\n\n\n\n\n\n \n  \n      \n    name \n    shortname \n    z_mean \n    z_max \n    t_ret \n    volume \n    area \n    p_tot \n    n_no3 \n    chl \n    wfd_type \n  \n \n\n  \n    Ammer \n    Ammersee \n    Ammer \n    37.60 \n    81.1 \n    2.70 \n    1.75000 \n    46.600 \n    7.3 \n    1.09 \n    2.80 \n    Typ 4 \n  \n  \n    Arend \n    Arendsee \n    Arend \n    28.60 \n    48.7 \n    50.00 \n    0.14700 \n    5.140 \n    375.0 \n    0.05 \n    22.30 \n    Typ 13 \n  \n  \n    Boden \n    Bodensee \n    Boden \n    85.00 \n    254.0 \n    4.20 \n    48.52150 \n    571.500 \n    6.9 \n    0.84 \n    2.10 \n    Typ 4 \n  \n  \n    Chiem \n    Chiemsee \n    Chiem \n    25.60 \n    73.4 \n    1.26 \n    2.04800 \n    79.900 \n    9.2 \n    0.55 \n    3.80 \n    Typ 4 \n  \n  \n    Dober \n    Dobersdorfer See \n    Dober \n    5.40 \n    18.8 \n    2.30 \n    0.01690 \n    3.120 \n    63.9 \n    0.64 \n    27.30 \n    Typ 14 \n  \n  \n    Muegg \n    Großer Müggelsee \n    Muegg \n    4.85 \n    7.5 \n    0.20 \n    0.03500 \n    7.200 \n    189.9 \n    0.17 \n    32.90 \n    Typ 11 \n  \n  \n    Ploen \n    Großer Plöner See \n    Ploen \n    12.40 \n    58.0 \n    3.10 \n    0.37200 \n    29.970 \n    62.3 \n    0.22 \n    8.80 \n    Typ 13 \n  \n  \n    Kumme \n    Kummerower See \n    Kumme \n    8.10 \n    23.3 \n    1.50 \n    0.26300 \n    32.500 \n    65.3 \n    0.78 \n    16.60 \n    Typ 11 \n  \n  \n    Mueritz \n    Müritz (Außenmüritz) \n    Mueritz \n    6.50 \n    28.1 \n    6.00 \n    0.68000 \n    105.300 \n    19.7 \n    0.11 \n    6.30 \n    Typ 14 \n  \n  \n    MuerB \n    Müritz (Binnenmüritz) \n    MuerB \n    9.80 \n    30.3 \n    6.00 \n    0.03800 \n    3.910 \n    34.2 \n    0.11 \n    6.70 \n    Typ 10 \n  \n  \n    Plaue \n    Plauer See \n    Plaue \n    6.80 \n    25.5 \n    3.00 \n    0.30000 \n    38.400 \n    26.0 \n    0.09 \n    6.80 \n    Typ 10 \n  \n  \n    Sacro \n    Sacrower See \n    Sacro \n    18.01 \n    36.0 \n    15.00 \n    0.01930 \n    1.072 \n    79.8 \n    0.04 \n    8.60 \n    Typ 10 \n  \n  \n    Schar \n    Scharmützelsee \n    Schar \n    9.00 \n    29.5 \n    16.00 \n    0.10823 \n    12.090 \n    35.3 \n    0.12 \n    10.40 \n    Typ 13 \n  \n  \n    SchwA \n    Schweriner See (Außensee) \n    SchwA \n    9.40 \n    52.4 \n    10.00 \n    0.33100 \n    35.200 \n    100.0 \n    0.23 \n    11.70 \n    Typ 13 \n  \n  \n    SchwI \n    Schweriner See (Innensee) \n    SchwI \n    13.50 \n    44.6 \n    5.30 \n    0.35600 \n    26.400 \n    246.5 \n    0.19 \n    5.86 \n    Typ 13 \n  \n  \n    Starn \n    Starnberger See \n    Starn \n    53.20 \n    127.8 \n    21.00 \n    2.99900 \n    56.400 \n    5.9 \n    0.32 \n    1.84 \n    Typ 3 \n  \n  \n    Stech \n    Stechlinsee \n    Stech \n    22.80 \n    68.0 \n    32.00 \n    0.09700 \n    4.250 \n    15.8 \n    0.04 \n    2.60 \n    Typ 13 \n  \n  \n    Stein \n    Steinhuder Meer \n    Stein \n    1.35 \n    2.9 \n    2.30 \n    0.04200 \n    29.100 \n    53.3 \n    0.12 \n    29.00 \n    Typ 11 \n  \n\n\n\n\n\nmean and maximum depth (m): z_mean, z_max; retention time (years): t_ret; volume (10^9 m^3); area (km^2), total phosphorus P (µg/L): p_tot; nitrogen-N (mg/L): n_no3, chlorophyll (µg/L): chl, water framework directive lake type: wfd_type \n\n\nData set from UBA = Umweltbundesamt = German Federal Environmental Agency"
  },
  {
    "objectID": "slides/10-multivariate-1.html#code-to-read-the-data",
    "href": "slides/10-multivariate-1.html#code-to-read-the-data",
    "title": "10-Multivariate methods I",
    "section": "Code to read the data",
    "text": "Code to read the data\n\nlakes <- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/elements/main/data/uba/lakes-combined-data.csv\")\nvalid_columns <- c(\"name\", \"shortname\", \"z_mean\", \"z_max\",  \"t_ret\", \"volume\", \n  \"area\", \"p_tot\", \"n_no3\", \"chl\", \"wfd_type\")\nlakes <- lakes[valid_columns] |> na.omit()\nrow.names(lakes) <- lakes$shortname\n\nlake_ids <- lakes[c(\"name\", \"shortname\")]\nlakedata <- lakes[, -c(1, 2)]\n\n## remove wfd_type for now\nlakedata$wfd_type <- NULL\n\n\nData files downloadable from https://github.com/tpetzoldt/elements/tree/main/data/uba\nSource of original data: Uwweltbundesamt https://www.uba.de\nTerms of use, see: Copyright and description of derived data"
  },
  {
    "objectID": "slides/10-multivariate-1.html#data-transformation-and-normalization",
    "href": "slides/10-multivariate-1.html#data-transformation-and-normalization",
    "title": "10-Multivariate methods I",
    "section": "Data transformation and normalization",
    "text": "Data transformation and normalization\n\npar(mfrow = c(1, 4), mar = c(6, 4, 3, 1), las = 2)\nboxplot(lakedata, main = \"raw data\")\nboxplot(scale(lakedata), main = \"normalized\")\nboxplot(scale(sqrt(lakedata)), main = \"sqrt + normalized\")\nboxplot(scale(log(lakedata)), main = \"log + normalized\")\n\n\n\nscale() performs normalisation (z-transformation)\naim: make different scales better comparable"
  },
  {
    "objectID": "slides/10-multivariate-1.html#principal-component-analysis-pca",
    "href": "slides/10-multivariate-1.html#principal-component-analysis-pca",
    "title": "10-Multivariate methods I",
    "section": "Principal Component Analysis: PCA",
    "text": "Principal Component Analysis: PCA\n\n\nidentify cvovariance or correlation structure\nrotate coordinate system, so that it points in the diretions of maximum variance\n\\(k\\) dimensions in original space are transformed into \\(k\\) orthogonal (rectangular) coordinates in principal components space.\nworks with any number of dimensions\nvisualisation by a 3D example"
  },
  {
    "objectID": "slides/10-multivariate-1.html#correlation-structure-of-the-lakes-data-set",
    "href": "slides/10-multivariate-1.html#correlation-structure-of-the-lakes-data-set",
    "title": "10-Multivariate methods I",
    "section": "Correlation structure of the lakes data set",
    "text": "Correlation structure of the lakes data set\n\n\n\nround(cor(lakedata), 2)\n\n       z_mean z_max t_ret volume  area p_tot n_no3   chl\nz_mean   1.00  0.97  0.20   0.81  0.78 -0.17  0.48 -0.49\nz_max    0.97  1.00  0.07   0.88  0.87 -0.26  0.47 -0.54\nt_ret    0.20  0.07  1.00  -0.12 -0.18  0.48 -0.41 -0.04\nvolume   0.81  0.88 -0.12   1.00  0.98 -0.20  0.44 -0.27\narea     0.78  0.87 -0.18   0.98  1.00 -0.26  0.45 -0.32\np_tot   -0.17 -0.26  0.48  -0.20 -0.26  1.00 -0.32  0.47\nn_no3    0.48  0.47 -0.41   0.44  0.45 -0.32  1.00 -0.15\nchl     -0.49 -0.54 -0.04  -0.27 -0.32  0.47 -0.15  1.00\n\n\n\n\n\nLet’s pick 3 variables for a 3D visualization: z_mix, z_max and volume.\nUse log-transformation to make them symetrically distributed."
  },
  {
    "objectID": "slides/10-multivariate-1.html#z_mix-z_max-and-volume-are-highly-corelated",
    "href": "slides/10-multivariate-1.html#z_mix-z_max-and-volume-are-highly-corelated",
    "title": "10-Multivariate methods I",
    "section": "z_mix, z_max and volume are highly corelated",
    "text": "z_mix, z_max and volume are highly corelated\n\n\n\n\n\\(\\rightarrow\\) We see that the three variables carry redundant information."
  },
  {
    "objectID": "slides/10-multivariate-1.html#how-rotation-of-axes-works",
    "href": "slides/10-multivariate-1.html#how-rotation-of-axes-works",
    "title": "10-Multivariate methods I",
    "section": "How rotation of axes works",
    "text": "How rotation of axes works\n\nOriginal coordinates\n\n\n\n\n\n\n\n\n\nPCA rotated coordinates\n\n\n\n\n\n\n\n\n\n\nThis slide contains interactive 3D graphics, that can be rotated with the mouse.\nRotate the left image, so that the points on both size show similar patterns.\nRotate the right image to show PC 3\n\n\\(\\rightarrow\\) Most of the 3D information of the data can be vizualized in 2D.\nNote: log-transformed variables were used in this example."
  },
  {
    "objectID": "slides/10-multivariate-1.html#pca-is-an-orthogonal-model-ii-regression",
    "href": "slides/10-multivariate-1.html#pca-is-an-orthogonal-model-ii-regression",
    "title": "10-Multivariate methods I",
    "section": "PCA is an orthogonal (model II) regression",
    "text": "PCA is an orthogonal (model II) regression\n\n\n\nPC1, PC2, PC3 are the principal components\nOLS is ordinary least squares regression (linear model lm)"
  },
  {
    "objectID": "slides/10-multivariate-1.html#now-analyse-all-numeric-variables",
    "href": "slides/10-multivariate-1.html#now-analyse-all-numeric-variables",
    "title": "10-Multivariate methods I",
    "section": "Now analyse all numeric variables",
    "text": "Now analyse all numeric variables\n\n\n\n\n\n \n  \n      \n    name \n    shortname \n    z_mean \n    z_max \n    t_ret \n    volume \n    area \n    p_tot \n    n_no3 \n    chl \n  \n \n\n  \n    Ammer \n    Ammersee \n    Ammer \n    37.60 \n    81.1 \n    2.70 \n    1.75000 \n    46.600 \n    7.3 \n    1.09 \n    2.80 \n  \n  \n    Arend \n    Arendsee \n    Arend \n    28.60 \n    48.7 \n    50.00 \n    0.14700 \n    5.140 \n    375.0 \n    0.05 \n    22.30 \n  \n  \n    Boden \n    Bodensee \n    Boden \n    85.00 \n    254.0 \n    4.20 \n    48.52150 \n    571.500 \n    6.9 \n    0.84 \n    2.10 \n  \n  \n    Chiem \n    Chiemsee \n    Chiem \n    25.60 \n    73.4 \n    1.26 \n    2.04800 \n    79.900 \n    9.2 \n    0.55 \n    3.80 \n  \n  \n    Dober \n    Dobersdorfer See \n    Dober \n    5.40 \n    18.8 \n    2.30 \n    0.01690 \n    3.120 \n    63.9 \n    0.64 \n    27.30 \n  \n  \n    Muegg \n    Großer Müggelsee \n    Muegg \n    4.85 \n    7.5 \n    0.20 \n    0.03500 \n    7.200 \n    189.9 \n    0.17 \n    32.90 \n  \n  \n    Ploen \n    Großer Plöner See \n    Ploen \n    12.40 \n    58.0 \n    3.10 \n    0.37200 \n    29.970 \n    62.3 \n    0.22 \n    8.80 \n  \n  \n    Kumme \n    Kummerower See \n    Kumme \n    8.10 \n    23.3 \n    1.50 \n    0.26300 \n    32.500 \n    65.3 \n    0.78 \n    16.60 \n  \n  \n    Mueritz \n    Müritz (Außenmüritz) \n    Mueritz \n    6.50 \n    28.1 \n    6.00 \n    0.68000 \n    105.300 \n    19.7 \n    0.11 \n    6.30 \n  \n  \n    MuerB \n    Müritz (Binnenmüritz) \n    MuerB \n    9.80 \n    30.3 \n    6.00 \n    0.03800 \n    3.910 \n    34.2 \n    0.11 \n    6.70 \n  \n  \n    Plaue \n    Plauer See \n    Plaue \n    6.80 \n    25.5 \n    3.00 \n    0.30000 \n    38.400 \n    26.0 \n    0.09 \n    6.80 \n  \n  \n    Sacro \n    Sacrower See \n    Sacro \n    18.01 \n    36.0 \n    15.00 \n    0.01930 \n    1.072 \n    79.8 \n    0.04 \n    8.60 \n  \n  \n    Schar \n    Scharmützelsee \n    Schar \n    9.00 \n    29.5 \n    16.00 \n    0.10823 \n    12.090 \n    35.3 \n    0.12 \n    10.40 \n  \n  \n    SchwA \n    Schweriner See (Außensee) \n    SchwA \n    9.40 \n    52.4 \n    10.00 \n    0.33100 \n    35.200 \n    100.0 \n    0.23 \n    11.70 \n  \n  \n    SchwI \n    Schweriner See (Innensee) \n    SchwI \n    13.50 \n    44.6 \n    5.30 \n    0.35600 \n    26.400 \n    246.5 \n    0.19 \n    5.86 \n  \n  \n    Starn \n    Starnberger See \n    Starn \n    53.20 \n    127.8 \n    21.00 \n    2.99900 \n    56.400 \n    5.9 \n    0.32 \n    1.84 \n  \n  \n    Stech \n    Stechlinsee \n    Stech \n    22.80 \n    68.0 \n    32.00 \n    0.09700 \n    4.250 \n    15.8 \n    0.04 \n    2.60 \n  \n  \n    Stein \n    Steinhuder Meer \n    Stein \n    1.35 \n    2.9 \n    2.30 \n    0.04200 \n    29.100 \n    53.3 \n    0.12 \n    29.00 \n  \n\n\n\n\n\n\n\ncolumns: called variables or species\nrows: observations or objects"
  },
  {
    "objectID": "slides/10-multivariate-1.html#pca-with-the-uba-lake-data",
    "href": "slides/10-multivariate-1.html#pca-with-the-uba-lake-data",
    "title": "10-Multivariate methods I",
    "section": "PCA with the UBA lake data",
    "text": "PCA with the UBA lake data\n\n\n\n\npc <- prcomp(scale(lakedata))\n\nEigenvalues (proportion of variance) indicate importance of components.\n\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2   PC3     PC4    PC5     PC6     PC7     PC8\nStandard deviation     2.0692 1.2735 1.047 0.76067 0.5499 0.30497 0.12230 0.10618\nProportion of Variance 0.5352 0.2027 0.137 0.07233 0.0378 0.01163 0.00187 0.00141\nCumulative Proportion  0.5352 0.7379 0.875 0.94729 0.9851 0.99672 0.99859 1.00000"
  },
  {
    "objectID": "slides/10-multivariate-1.html#pca-biplot",
    "href": "slides/10-multivariate-1.html#pca-biplot",
    "title": "10-Multivariate methods I",
    "section": "PCA Biplot",
    "text": "PCA Biplot\n\nbiplot(pc)"
  },
  {
    "objectID": "slides/10-multivariate-1.html#pca-with-sqrt-transformed-data",
    "href": "slides/10-multivariate-1.html#pca-with-sqrt-transformed-data",
    "title": "10-Multivariate methods I",
    "section": "PCA with sqrt transformed data",
    "text": "PCA with sqrt transformed data\n\n\nlakedata2 <- sqrt(lakedata)\npc <- prcomp(scale(lakedata2))\nsummary(pc)\n\nImportance of components:\n                         PC1    PC2    PC3     PC4     PC5     PC6     PC7    PC8\nStandard deviation     2.111 1.3059 0.9748 0.70830 0.50321 0.29742 0.17060 0.1266\nProportion of Variance 0.557 0.2132 0.1188 0.06271 0.03165 0.01106 0.00364 0.0020\nCumulative Proportion  0.557 0.7702 0.8889 0.95165 0.98330 0.99436 0.99800 1.0000\n\n\n\n\nThe PCA with the untransformed data looked very asymmetric, we repeat it with square root transformed data.\nhelps to get a better “resolution”\nmust be taken into account when interpreting the results\nlog transformation is also possible"
  },
  {
    "objectID": "slides/10-multivariate-1.html#biplot-of-sqrt-transformed-data-1.-3.-pc",
    "href": "slides/10-multivariate-1.html#biplot-of-sqrt-transformed-data-1.-3.-pc",
    "title": "10-Multivariate methods I",
    "section": "Biplot of sqrt transformed data (1.-3. PC)",
    "text": "Biplot of sqrt transformed data (1.-3. PC)\n\npar(mfrow=c(1, 2), mar=c(5, 4, 4, 2.4), las=1)\nbiplot(pc)\nbiplot(pc, choices=c(3, 2))"
  },
  {
    "objectID": "slides/10-multivariate-1.html#pca-with-the-vegan-package",
    "href": "slides/10-multivariate-1.html#pca-with-the-vegan-package",
    "title": "10-Multivariate methods I",
    "section": "PCA with the vegan package",
    "text": "PCA with the vegan package\n\npc <- rda(lakedata2, scale = TRUE)\nsummary(pc)\n\n\nCall:\nrda(X = lakedata2, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal               8          1\nUnconstrained       8          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                        PC1    PC2    PC3     PC4     PC5     PC6      PC7      PC8\nEigenvalue            4.456 1.7053 0.9503 0.50170 0.25322 0.08846 0.029105 0.016020\nProportion Explained  0.557 0.2132 0.1188 0.06271 0.03165 0.01106 0.003638 0.002003\nCumulative Proportion 0.557 0.7702 0.8889 0.95165 0.98330 0.99436 0.997997 1.000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  3.414953 \n\n\nSpecies scores\n\n           PC1     PC2      PC3      PC4       PC5      PC6\nz_mean -1.0598  0.4289 -0.21113  0.23873 -0.008753  0.18441\nz_max  -1.1318  0.3420 -0.12121  0.10258  0.072128  0.04149\nt_ret   0.0321  1.1531 -0.07499  0.10386 -0.291203 -0.15956\nvolume -1.0984 -0.1075 -0.36435 -0.28489 -0.088096  0.07254\narea   -1.0368 -0.2858 -0.25303 -0.44554 -0.025908 -0.17654\np_tot   0.7169  0.2934 -0.85503  0.06207  0.345841 -0.05495\nn_no3  -0.7044 -0.7316 -0.18149  0.60719 -0.072833 -0.13748\nchl     0.8938 -0.3753 -0.59964 -0.02895 -0.381711  0.09702\n\n\nSite scores (weighted sums of species scores)\n\n             PC1     PC2      PC3      PC4      PC5      PC6\nAmmer   -0.83902 -0.6075  0.51716  1.81616  0.22343 -0.15112\nArend    0.61718  1.8219 -1.92971  0.44353 -0.51334 -0.46420\nBoden   -2.56671 -0.3066 -1.32007 -1.14721 -0.41861  0.35821\nChiem   -0.64712 -0.5918  0.55561  0.35138  0.48915  0.23752\nDober    0.57998 -1.0029 -0.35592  1.09957 -1.11137  0.09904\nMuegg    0.98336 -0.8687 -1.03143 -0.33065  0.31544  1.71907\nPloen    0.05148 -0.1788  0.12415 -0.07719  0.74087  0.42564\nKumme    0.20231 -1.0792 -0.29790  0.92094 -0.25802 -1.13785\nMueritz  0.01805 -0.1716  0.69453 -1.37602 -0.04136 -1.55657\nMuerB    0.32308  0.1145  0.83401 -0.12085  0.39231  0.75409\nPlaue    0.21775 -0.2308  0.80820 -0.98103  0.44885  0.06816\nSacro    0.40494  0.8219  0.29088 -0.04609  0.23456  1.24642\nSchar    0.34038  0.3961  0.50515 -0.19722 -0.82282 -0.52792\nSchwA    0.19197  0.1243 -0.30591 -0.03972  0.15528 -1.05932\nSchwI    0.24471  0.2379 -0.69605  0.04506  2.41490 -0.72493\nStarn   -0.95511  0.9136  0.55372  0.66291 -0.52409  0.55554\nStech   -0.05030  1.4337  1.12433  0.06976 -0.49103  0.09703\nStein    0.88307 -0.8259 -0.07074 -1.09331 -1.23415  0.06121\n\n\n\nThe two first PCs explain PC 1 = 77% of variance of the square root transformed observations."
  },
  {
    "objectID": "slides/10-multivariate-1.html#variance-importance-and-biplot",
    "href": "slides/10-multivariate-1.html#variance-importance-and-biplot",
    "title": "10-Multivariate methods I",
    "section": "Variance importance and biplot",
    "text": "Variance importance and biplot\n\nImportance of components\n\nPC 1 = 56%\nPC 2 = 21%\nPC 3 = 12%\n\n\\(\\Rightarrow\\) The two first PCs explain PC 1 = 77% of variance of the square root transformed observations.\n\n\n\nbiplot(pc)"
  },
  {
    "objectID": "slides/10-multivariate-1.html#interpretation",
    "href": "slides/10-multivariate-1.html#interpretation",
    "title": "10-Multivariate methods I",
    "section": "Interpretation",
    "text": "Interpretation\n\nVariables\n\nonly long arrows can be interpreted\nsame direction: positive correlation\nopposite direction: negative correlation\nrectangular: no correlation\n\nObservations\n\nidentification of groups and extreme cases\n\nCombined View\n\nrelative size of variable for observations\narrow direction: high values\nopposite direction: low values\naround middle: average\n\n\\(\\Rightarrow\\) Important: always read perpendicular to PCs and arrows!"
  },
  {
    "objectID": "slides/10-multivariate-1.html#biplot-in-3d",
    "href": "slides/10-multivariate-1.html#biplot-in-3d",
    "title": "10-Multivariate methods I",
    "section": "Biplot in 3D",
    "text": "Biplot in 3D\n\n\nVariance\n\nPC 1 = 56%\nPC 2 = 21%\nPC 3 = 12%\nsum of PC 4 … 8 = 11%\n\n\n\\(\\rightarrow\\) Use the mouse to rotate and zoom."
  },
  {
    "objectID": "slides/10-multivariate-1.html#nonmetric-multidimensional-scaling-nmds",
    "href": "slides/10-multivariate-1.html#nonmetric-multidimensional-scaling-nmds",
    "title": "10-Multivariate methods I",
    "section": "Nonmetric Multidimensional Scaling: NMDS",
    "text": "Nonmetric Multidimensional Scaling: NMDS\n\nmd <- metaMDS(lakedata2, scale = TRUE, distance = \"euclid\", trace=FALSE)\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\nThe NMDS tries to map the higher dimensions even better to 2D or 3D. However, it accepts some distortion. More about this in the next chapter."
  },
  {
    "objectID": "slides/10-multivariate-1.html#the-code",
    "href": "slides/10-multivariate-1.html#the-code",
    "title": "10-Multivariate methods I",
    "section": "The code",
    "text": "The code\n\nlakes <- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/elements/main/data/uba/lakes-combined-data.csv\")\nvalid_columns <- c(\"name\", \"shortname\", \"z_mean\", \"z_max\",  \"t_ret\", \"volume\", \n  \"area\", \"p_tot\", \"n_no3\", \"chl\", \"wfd_type\")\n\n# set row names and limit data set to complete records without missing data\nrow.names(lakes) <- lakes$shortname\nlakes <- na.omit(lakes[valid_columns])\n\n# only the numerical variables\nlakedata <- lakes[c(\"z_mean\", \"z_max\",  \"t_ret\", \"volume\", \n                     \"area\", \"p_tot\", \"n_no3\", \"chl\")] \n\n## PCA with the vegan package\nlibrary(\"vegan\")\nlakedata2 <- sqrt(lakedata)\npc <- rda(lakedata2, scale = TRUE)\nsummary(pc)\nbiplot(pc)\nbiplot(pc, choices=c(3, 2))\n\n## 3D Plot\nlibrary(\"vegan3d\")\nordirgl(pc, col = \"yellow\")\norgltext(pc, display = \"species\", col=\"red\")\norgltext(pc, display = \"sites\", col=\"blue\", pos=4)\nview3d(theta = 5, phi = 15, fov=30, zoom=1)\n\n## NMDS\nmd <- metaMDS(lakedata2, scale = TRUE, distance = \"euclid\", trace=FALSE)\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\nData and full source code is available from https://github.com/tpetzoldt/elements/"
  },
  {
    "objectID": "slides/10-multivariate-1.html#summary",
    "href": "slides/10-multivariate-1.html#summary",
    "title": "10-Multivariate methods I",
    "section": "Summary",
    "text": "Summary\n\n\nMultivariate methods can be used to analyze data sets, where multiple variables depend on each other.\nPCA is a dimension reduction technique that tries to map high-dimensional data to a lower number of dimensions.\nIt can be used as an explorative technique to find relationships between data.\nCompared to creating multiple plots between all variables, it helps to see the overall picture, saves time and avoids statistical fishing.\nThe next chapter will introduce more methods.\n\n\nFurther reading\n\nOksanen (2010)\nBorcard et al. (2018)"
  },
  {
    "objectID": "slides/10-multivariate-1.html#references",
    "href": "slides/10-multivariate-1.html#references",
    "title": "10-Multivariate methods I",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\nBorcard, D., Gillet, F., & Legendre, P. (2018). Numerical ecology with R. Springer International Publishing. https://doi.org/10.1007/978-3-319-71404-2\n\n\nOksanen, J. (2010). Multivariate analysis of ecological communities in R: Vegan tutorial.\n\n\nUmweltbundesamt. (2021). Kenndaten ausgewählter Seen Deutschlands. https://www.umweltbundesamt.de/daten/wasser/zustand-der-seen#okologischer-zustand-der-seen\n\n\nWinkelmann, C., Hellmann, C., Worischka, S., Petzoldt, T., & Benndorf, J. (2011). Fish predation affects the structure of a benthic community. Freshwater Biology, 56(6), 1030–1046. https://doi.org/10.1111/j.1365-2427.2010.02543.x"
  },
  {
    "objectID": "slides/07-anova.html#anova-analysis-of-variances",
    "href": "slides/07-anova.html#anova-analysis-of-variances",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA – Analysis of Variances",
    "text": "ANOVA – Analysis of Variances\n\n\nTesting of complex hypothesis as a whole, e.g.:\n\nmore than two samples (multiple test problem),\nseveral multiple factors (multiway ANOVA)\nelimination of covariates (ANCOVA)\nfixed and/or random effects (variance decomposition methods, mixed effects models)\n\nDifferent application scenarios:\n\nexplorative use: Which influence factors are important?\ndescriptive use: Fitting of models for process description and forecasting.\nsignificance tests.\n\nANOVA methods are (in most cases) based on linear models."
  },
  {
    "objectID": "slides/07-anova.html#a-practical-example",
    "href": "slides/07-anova.html#a-practical-example",
    "title": "07-One and two-way ANOVA",
    "section": "A practical example",
    "text": "A practical example\n\nFind a suitable medium for growth experiments with green algae\n\ncheap, easy to handle\nsuitable for students courses and classroom experiments\n\n\n\n\n\nIdea\n\nuse a commercial fertilizer with the main nutrients N and P\nmineral water with trace elements\ndoes non-sparkling mineral water contain enough \\(\\mathrm{CO_2}\\)?\ntest how to improve (\\(\\mathrm{CO_2}\\)) availability for photosynthesis"
  },
  {
    "objectID": "slides/07-anova.html#application",
    "href": "slides/07-anova.html#application",
    "title": "07-One and two-way ANOVA",
    "section": "Application",
    "text": "Application\n\n7 Different treatments\n\n\nfertilizer solution in closed bottles\nfertilizer solution in open bottles (\\(\\mathrm{CO_2}\\) from air)\nfertilizer + sugar (organic C source)\nfertilizer + additional \\(\\mathrm{HCO_3^-}\\) (add \\(\\mathrm{CaCO_3}\\) to sparkling mineral water)\na standard algae growth medium (“Basal medium”) for comparison\ndeionized (“destilled”) water and\ntap water for comparison"
  },
  {
    "objectID": "slides/07-anova.html#experimental-design",
    "href": "slides/07-anova.html#experimental-design",
    "title": "07-One and two-way ANOVA",
    "section": "Experimental design",
    "text": "Experimental design\n\n\neach treatment with 3 replicates\nrandomized placement on a shaker\n16:8 light:dark-cycle\nmeasurement directly in the bottles using a self-made turbidity meter"
  },
  {
    "objectID": "slides/07-anova.html#results",
    "href": "slides/07-anova.html#results",
    "title": "07-One and two-way ANOVA",
    "section": "Results",
    "text": "Results\n\n\n\n\n Fertilizer – Open Bottle – F. + Sugar – F. + CaCO3 – Basal medium – A. dest – Tap water"
  },
  {
    "objectID": "slides/07-anova.html#the-data-set",
    "href": "slides/07-anova.html#the-data-set",
    "title": "07-One and two-way ANOVA",
    "section": "The data set",
    "text": "The data set\n\n\n\n\n\n\n\n\nTable 1:  Growth from day 2 to day 6 (relative units) \n \n  \n    treat \n    replicate 1 \n    replicate 2 \n    replicate 3 \n  \n \n\n  \n    Fertilizer \n    0.020 \n    -0.217 \n    -0.273 \n  \n  \n    F. open \n    0.940 \n    0.780 \n    0.555 \n  \n  \n    F.+sugar \n    0.188 \n    -0.100 \n    0.020 \n  \n  \n    F.+CaCO3 \n    0.245 \n    0.236 \n    0.456 \n  \n  \n    Bas.med. \n    0.699 \n    0.727 \n    0.656 \n  \n  \n    A.dest \n    -0.010 \n    0.000 \n    -0.010 \n  \n  \n    Tap water \n    0.030 \n    -0.070 \n    NA \n  \n\n\n\n\n\n\n\nNA means “not available”, i.e. a missing value\nthe crosstable structure is compact and esy to read, but not suitable for data analysis\n\\(\\Rightarrow\\) convert it to long format"
  },
  {
    "objectID": "slides/07-anova.html#data-in-long-format",
    "href": "slides/07-anova.html#data-in-long-format",
    "title": "07-One and two-way ANOVA",
    "section": "Data in long format",
    "text": "Data in long format\n\n\nAdvantages\n\nlooks “stupid” but is better for data analysis\ndependent variable growth and explanation variable treat clearly visible\nmodel formula: growth ~ treat\neasily extensible to \\(>1\\) explanation variable\n\n\n\n\n\n\n\n \n  \n    treat \n    rep \n    growth \n  \n \n\n  \n    Fertilizer \n    1 \n    0.020 \n  \n  \n    Fertilizer \n    2 \n    -0.217 \n  \n  \n    Fertilizer \n    3 \n    -0.273 \n  \n  \n    F. open \n    1 \n    0.940 \n  \n  \n    F. open \n    2 \n    0.780 \n  \n  \n    F. open \n    3 \n    0.555 \n  \n  \n    F.+sugar \n    1 \n    0.188 \n  \n  \n    F.+sugar \n    2 \n    -0.100 \n  \n  \n    F.+sugar \n    3 \n    0.020 \n  \n  \n    F.+CaCO3 \n    1 \n    0.245 \n  \n  \n    F.+CaCO3 \n    2 \n    0.236 \n  \n  \n    F.+CaCO3 \n    3 \n    0.456"
  },
  {
    "objectID": "slides/07-anova.html#the-data-in-r",
    "href": "slides/07-anova.html#the-data-in-r",
    "title": "07-One and two-way ANOVA",
    "section": "The data in R",
    "text": "The data in R\n\n\nalgae <- data.frame(\n  treat  = factor(c(\"Fertilizer\", \"Fertilizer\", \"Fertilizer\", \n             \"F. open\", \"F. open\", \"F. open\", \n             \"F.+sugar\", \"F.+sugar\", \"F.+sugar\", \n             \"F.+CaCO3\", \"F.+CaCO3\", \"F.+CaCO3\", \n             \"Bas.med.\", \"Bas.med.\", \"Bas.med.\", \n             \"A.dest\", \"A.dest\", \"A.dest\", \n             \"Tap water\", \"Tap water\"),\n             levels=c(\"Fertilizer\", \"F. open\", \"F.+sugar\", \n                    \"F.+CaCO3\", \"Bas.med.\", \"A.dest\", \"Tap water\")),\n  rep   = c(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2), \n  growth = c(0.02, -0.217, -0.273, 0.94, 0.78, 0.555, 0.188, -0.1, 0.02, \n             0.245, 0.236, 0.456, 0.699, 0.727, 0.656, -0.01, 0, -0.01, 0.03, -0.07)\n)\n\n… can be entered directly in the code. A csv-file in long format is also possible."
  },
  {
    "objectID": "slides/07-anova.html#boxplot",
    "href": "slides/07-anova.html#boxplot",
    "title": "07-One and two-way ANOVA",
    "section": "Boxplot",
    "text": "Boxplot\n\nboxplot(growth ~ treat, data = algae)\nabline(h = 0, lty = \"dashed\", col = \"grey\")"
  },
  {
    "objectID": "slides/07-anova.html#stripchart",
    "href": "slides/07-anova.html#stripchart",
    "title": "07-One and two-way ANOVA",
    "section": "Stripchart",
    "text": "Stripchart\n\nstripchart(growth ~ treat, data = algae, vertical = TRUE)\n\n\nBetter, because we have only 2-3 replicates. Boxplot needs more."
  },
  {
    "objectID": "slides/07-anova.html#turn-scientific-question-into-a-statistical-hypothesis",
    "href": "slides/07-anova.html#turn-scientific-question-into-a-statistical-hypothesis",
    "title": "07-One and two-way ANOVA",
    "section": "Turn scientific question into a statistical hypothesis",
    "text": "Turn scientific question into a statistical hypothesis\n\nScientific Questions\n\nAre the treatments different?\nWhich medium is the best?\nIs the best medium significantly better than the others?\n\n Statistical Hypotheses\n\n\\(H_0\\): growth is the same in all treatments\n\\(H_A\\): differences between media"
  },
  {
    "objectID": "slides/07-anova.html#why-cant-we-apply-just-several-t-tests",
    "href": "slides/07-anova.html#why-cant-we-apply-just-several-t-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Why can’t we apply just several t-tests?",
    "text": "Why can’t we apply just several t-tests?\n\n\nIf we have 7 treatments and want to test all against each other, we would need:\n\n\\[7 \\cdot (7 - 1) / 2 = 21 \\qquad\\text{tests.}\\]\n\nIf we set \\(\\alpha = 0.05\\) we get 5% false positives. \\(\\Rightarrow\\) One of 20 tests is on average a false positive\nIf we do \\(N\\) tests, we increase the overall \\(\\alpha\\) error to \\(N\\cdot\\alpha\\) in the worst case.\nThis is called alpha-error-inflation or the Bonferroni law:\n\n\\[\n\\alpha_{total} \\le \\sum_{i=1}^{N} \\alpha_i = N \\cdot \\alpha\n\\]\nIf we ignore the Bonferroni law, we end in statistical fishing and get spurious results just by chance. See https://xkcd.com/882/."
  },
  {
    "objectID": "slides/07-anova.html#anova-analysis-of-variances-1",
    "href": "slides/07-anova.html#anova-analysis-of-variances-1",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA: Analysis of variances",
    "text": "ANOVA: Analysis of variances\n\nBasic Idea\n\nSplit the total variance into effect(s) and errors:\n\n\n\\[\ns_y^2 = s^2_\\mathrm{effect} + s^2_{\\varepsilon}\n\\]\n\n\nSomewhat surprising: we use variances to compare mean values.\nExplanation: differences of means contribute to the total variance of the whole sample.\nVariance components can be called variance within (\\(s^2_\\varepsilon\\)) and variance between samples.\nThe way how to separate variances is a linear model."
  },
  {
    "objectID": "slides/07-anova.html#example",
    "href": "slides/07-anova.html#example",
    "title": "07-One and two-way ANOVA",
    "section": "Example",
    "text": "Example\n\nTwo brands of Clementine fruits from a shop “E”, that we encode as “EB” and “EP”. We want to know whether the premium brand (“P”) and the basic brand (“B”) have a different weight.\n\nclem <- data.frame(\n  brand = c(\"EP\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \n            \"EB\", \"EB\", \"EB\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EB\", \"EP\"),\n  weight = c(88, 96, 100, 96, 90, 100, 92, 92, 102, 99, 86, 89, 99, 89, 75, 80, \n             81, 96, 82, 98, 80, 107, 88))\n\n We encode one sample (“EB”) with 1 and the other sample (“EP”) with 2:\n\nclem$code <- as.numeric(factor(clem$brand))\nclem$code\n\n [1] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2"
  },
  {
    "objectID": "slides/07-anova.html#then-we-fit-a-linear-regression",
    "href": "slides/07-anova.html#then-we-fit-a-linear-regression",
    "title": "07-One and two-way ANOVA",
    "section": "Then we fit a linear regression:",
    "text": "Then we fit a linear regression:\n\nplot(weight ~ code, data = clem, axes = FALSE)\nm <- lm(weight ~ code, data = clem)\naxis(1, at = c(1,2), labels = c(\"EB\", \"EP\")); axis(2); box()\nabline(m, col = \"blue\")"
  },
  {
    "objectID": "slides/07-anova.html#variance-components",
    "href": "slides/07-anova.html#variance-components",
    "title": "07-One and two-way ANOVA",
    "section": "Variance components",
    "text": "Variance components\n\nWe fit a linear model and compare the variances:\n\nm <- lm(weight ~ code, data = clem)\n\ntotal variance\n\n(var_tot <- var(clem$weight))\n\n[1] 68.98814\n\n\nresidual variance (= within variance)\n\n(var_res <- var(residuals(m)))\n\n[1] 43.25\n\n\nexplained variance (= between variance)\n\nvar_tot - var_res\n\n[1] 25.73814\n\n\nNow we can analyse whether the between variance is big enough to justify a significant effect.\nThis is called an ANOVA."
  },
  {
    "objectID": "slides/07-anova.html#anova",
    "href": "slides/07-anova.html#anova",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA",
    "text": "ANOVA\n\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value   Pr(>F)   \ncode       1 566.24  566.24  12.497 0.001963 **\nResiduals 21 951.50   45.31                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nA t-test for comparison\n\nt.test(weight ~ code, data = clem, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by code\nt = 3.5351, df = 21, p-value = 0.001963\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n  4.185911 16.147423\nsample estimates:\nmean in group 1 mean in group 2 \n       95.50000        85.33333 \n\n\n\\(\\Rightarrow\\) the p-values are exactly the same."
  },
  {
    "objectID": "slides/07-anova.html#anova-with-more-than-2-samples",
    "href": "slides/07-anova.html#anova-with-more-than-2-samples",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA with more than 2 samples",
    "text": "ANOVA with more than 2 samples\nBack to the algae growth data. Let’s call the linear model m:\n\nm <- lm(growth ~ treat, data = algae)\n\n\n\nWe can print the coefficients of the linear model with summary(m)\nBut we are interested in the overall effect and use anova\n\n\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: growth\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \ntreat      6 2.35441 0.39240  25.045 1.987e-06 ***\nResiduals 13 0.20368 0.01567                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe ANOVA table shows F-tests testing for significance of all factors.\nIn the table above, we have only one single factor.\n\n\\(\\Rightarrow\\) We see that the treatment had a significant effect."
  },
  {
    "objectID": "slides/07-anova.html#model-formulae",
    "href": "slides/07-anova.html#model-formulae",
    "title": "07-One and two-way ANOVA",
    "section": "Model formulae",
    "text": "Model formulae\n\n\n\n\n\n\n\nModel Type\nFormula\n\n\n\n\nNull model\ny ~ 1\n\n\nSimple linear regression\ny ~ x\n\n\nLinear without intercept\ny ~ x - 1\n\n\nMultiple regression\ny ~ x1 + x2 + x3\n\n\nMultiple regression with interaction\ny ~ x1 * x2 * x3\n\n\nTransformed with ‘as is’ function\ny ~ x + I(x^2), y ~ x + sin(x^2)\n\n\nOrthogonal polynomial of order 3\ny ~ poly(x, 3)\n\n\nANOVA with and without interaction\ny ~ f1 * f2, y ~ f1 + f2\n\n\nANCOVA with Interaction\ny ~ x * f\n\n\nMultiple regression with interaction\ny ~ x * f + x1 + x2\n\n\nConditional or nested ANOVA\ny ~ x + (x | b), y ~ x + (1 | a / b)\n\n\nGAM with smoother ‘s’\ny ~ s(x) + f\n\n\n…\n…\n\n\n\n\n\n\n\n\ny = response variable (depedent, target)\nx = metric explanation variable (predictor, independent)\nf = factor variable (nominal)"
  },
  {
    "objectID": "slides/07-anova.html#posthoc-tests",
    "href": "slides/07-anova.html#posthoc-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Posthoc tests",
    "text": "Posthoc tests\n\nThe test showed, that the factor “treatment” had a significant effect.\nWe don’t know yet, which factor levels were different.\n\nTukey HSD test is the most common.\n\ntk <- TukeyHSD(aov(m))\ntk\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = m)\n\n$treat\n                            diff         lwr         upr     p adj\nF. open-Fertilizer    0.91500000  0.56202797  1.26797203 0.0000103\nF.+sugar-Fertilizer   0.19266667 -0.16030537  0.54563870 0.5211198\nF.+CaCO3-Fertilizer   0.46900000  0.11602797  0.82197203 0.0069447\nBas.med.-Fertilizer   0.85066667  0.49769463  1.20363870 0.0000231\nA.dest-Fertilizer     0.15000000 -0.20297203  0.50297203 0.7579063\nTap water-Fertilizer  0.13666667 -0.25796806  0.53130140 0.8837597\nF.+sugar-F. open     -0.72233333 -1.07530537 -0.36936130 0.0001312\nF.+CaCO3-F. open     -0.44600000 -0.79897203 -0.09302797 0.0102557\nBas.med.-F. open     -0.06433333 -0.41730537  0.28863870 0.9943994\nA.dest-F. open       -0.76500000 -1.11797203 -0.41202797 0.0000721\nTap water-F. open    -0.77833333 -1.17296806 -0.38369860 0.0001913\nF.+CaCO3-F.+sugar     0.27633333 -0.07663870  0.62930537 0.1727182\nBas.med.-F.+sugar     0.65800000  0.30502797  1.01097203 0.0003363\nA.dest-F.+sugar      -0.04266667 -0.39563870  0.31030537 0.9994197\nTap water-F.+sugar   -0.05600000 -0.45063473  0.33863473 0.9985686\nBas.med.-F.+CaCO3     0.38166667  0.02869463  0.73463870 0.0307459\nA.dest-F.+CaCO3      -0.31900000 -0.67197203  0.03397203 0.0879106\nTap water-F.+CaCO3   -0.33233333 -0.72696806  0.06230140 0.1247914\nA.dest-Bas.med.      -0.70066667 -1.05363870 -0.34769463 0.0001792\nTap water-Bas.med.   -0.71400000 -1.10863473 -0.31936527 0.0004507\nTap water-A.dest     -0.01333333 -0.40796806  0.38130140 0.9999997"
  },
  {
    "objectID": "slides/07-anova.html#graphical-output",
    "href": "slides/07-anova.html#graphical-output",
    "title": "07-One and two-way ANOVA",
    "section": "Graphical output",
    "text": "Graphical output\n\npar(las = 1)             # las = 1 make y annotation horizontal\npar(mar = c(4, 10, 3, 1)) # more space at the left for axis annotation\nplot(tk)"
  },
  {
    "objectID": "slides/07-anova.html#anova-assumptions-and-diagnostics",
    "href": "slides/07-anova.html#anova-assumptions-and-diagnostics",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA assumptions and diagnostics",
    "text": "ANOVA assumptions and diagnostics\n\nANOVA has same assumptions as the linear model.\n\n\nIndependence of errors\nVariance homogeneity\nApproximate normality of errors\n\nGraphical checks are preferred.\n\n\n\npar(mfrow=c(2, 2))\nplot(m)"
  },
  {
    "objectID": "slides/07-anova.html#numerical-tests",
    "href": "slides/07-anova.html#numerical-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Numerical tests",
    "text": "Numerical tests\n\n\nTest variance homogeneity\n\nF-test compares only two variances.\nSeveral tests for multiple variances available, e.g. Bartlett, Levene, Fligner-Killeen\nRecommended: Fligner-Killeen-test\n\n\nfligner.test(growth ~ treat, \n             data = algae)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  growth by treat\nFligner-Killeen:med chi-squared = 4.2095, df = 6, p-value = 0.6483\n\n\n\n\nTest of normal distribution\n\nThe Shapiro-Wilks test can be misleading.\nUse a graphical method!\n\n\nqqnorm(residuals(m))\nqqline(residuals(m))"
  },
  {
    "objectID": "slides/07-anova.html#one-way-anova-with-heterogeneous-variances",
    "href": "slides/07-anova.html#one-way-anova-with-heterogeneous-variances",
    "title": "07-One and two-way ANOVA",
    "section": "One-way ANOVA with heterogeneous variances",
    "text": "One-way ANOVA with heterogeneous variances\n\n\nextension of the Welch test for \\(\\ge 2\\) samples\nin R called oneway.test\n\n\noneway.test(growth ~ treat, data = algae)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  growth and treat\nF = 115.09, num df = 6.0000, denom df = 4.6224, p-value = 6.57e-05"
  },
  {
    "objectID": "slides/07-anova.html#two-way-anova",
    "href": "slides/07-anova.html#two-way-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Two-way ANOVA",
    "text": "Two-way ANOVA\n\n\n\n\nExample from a statistics text book (Crawley, 2002)\nEffects of diet and coat color on growth of Hamsters in gramm per time\n\n\n\n\ndiet\nlight coat\ndark coat\n\n\n\n\nA\n8.3\n6.6\n\n\nA\n8.7\n7.2\n\n\nB\n8.1\n6.9\n\n\nB\n8.5\n8.3\n\n\nC\n9.1\n7.9\n\n\nC\n9.0\n9.2\n\n\n\n\n\n\n\n\n\nfactorial experiment (with replicates): each factor combination has more than one observation.\nwithout replication:\n\nno replicates per factor combination\nthis is possible, but does not allow identification of interactions"
  },
  {
    "objectID": "slides/07-anova.html#enter-data-in-long-format",
    "href": "slides/07-anova.html#enter-data-in-long-format",
    "title": "07-One and two-way ANOVA",
    "section": "Enter data in long format",
    "text": "Enter data in long format\n\n\n\nhams <- data.frame(No = 1:12,\n                   growth = c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,\n                            8.3, 8.7, 8.1, 8.5, 9.1, 9.0),\n                   diet = rep(c(\"A\", \"B\", \"C\"), each=2),\n                   coat = rep(c(\"light\", \"dark\"), each=6)\n                   )\n\n\n\n\n\n\n\n \n  \n    No \n    growth \n    diet \n    coat \n  \n \n\n  \n    1 \n    6.6 \n    A \n    light \n  \n  \n    2 \n    7.2 \n    A \n    light \n  \n  \n    3 \n    6.9 \n    B \n    light \n  \n  \n    4 \n    8.3 \n    B \n    light \n  \n  \n    5 \n    7.9 \n    C \n    light \n  \n  \n    6 \n    9.2 \n    C \n    light \n  \n  \n    7 \n    8.3 \n    A \n    dark \n  \n  \n    8 \n    8.7 \n    A \n    dark \n  \n  \n    9 \n    8.1 \n    B \n    dark \n  \n  \n    10 \n    8.5 \n    B \n    dark \n  \n  \n    11 \n    9.1 \n    C \n    dark \n  \n  \n    12 \n    9.0 \n    C \n    dark"
  },
  {
    "objectID": "slides/07-anova.html#linear-model-and-anova",
    "href": "slides/07-anova.html#linear-model-and-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Linear model and ANOVA",
    "text": "Linear model and ANOVA\n\nANOVA\n\nm <- lm(growth ~ coat * diet, data = hams)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: growth\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \ncoat       1 2.61333 2.61333  7.2258 0.03614 *\ndiet       2 2.66000 1.33000  3.6774 0.09069 .\ncoat:diet  2 0.68667 0.34333  0.9493 0.43833  \nResiduals  6 2.17000 0.36167                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-anova.html#interaction-plot",
    "href": "slides/07-anova.html#interaction-plot",
    "title": "07-One and two-way ANOVA",
    "section": "Interaction plot",
    "text": "Interaction plot\n\nwith(hams, interaction.plot(diet, coat, growth, \n                            col = c(\"brown\", \"orange\"), lty = 1, lwd = 2))"
  },
  {
    "objectID": "slides/07-anova.html#diagnostics",
    "href": "slides/07-anova.html#diagnostics",
    "title": "07-One and two-way ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\nAssumptions\n\nIndependence of measurements (within samples)\nVariance homogeneity of residuals\nNormal distribution of residuals\n\n Test of assumptions needs residuals of the fitted model. \\(\\Rightarrow\\) Fit the ANOVA model first, then check if it was correct!\n\nDiagnostic tools\n\nBox plot\nPlot of residuals vs. mean values\nQ-Q-plot of residuals\nFligner-Killeen test (alternative: some people recommend the Levene-Test)"
  },
  {
    "objectID": "slides/07-anova.html#diagnostics-ii",
    "href": "slides/07-anova.html#diagnostics-ii",
    "title": "07-One and two-way ANOVA",
    "section": "Diagnostics II",
    "text": "Diagnostics II\n\npar(mfrow=c(1, 2))\npar(cex=1.2, las=1)\nqqnorm(residuals(m))\nqqline(residuals(m))\n\nplot(residuals(m)~fitted(m))\nabline(h=0)\n\n\n\nfligner.test(growth ~ interaction(coat, diet), data=hams)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  growth by interaction(coat, diet)\nFligner-Killeen:med chi-squared = 10.788, df = 5, p-value = 0.05575\n\n\nResiduals: look ok and p-value of the Fligner test \\(< 0.05\\), \\(\\rightarrow\\) looks fine."
  },
  {
    "objectID": "slides/07-anova.html#notes",
    "href": "slides/07-anova.html#notes",
    "title": "07-One and two-way ANOVA",
    "section": "Notes",
    "text": "Notes\nLinear regression or ANOVA?\n\nessentially the same\nindependent variables are metric: linear model\nindependent variables are nominal (= factor): ANOVA\nmix of metric and nominal variables: ANCOVA\n\nUse of pre-tests\nPre-tests are in general questionable for theoretical reasons:\n\nThe null hypotheses \\(H_0\\) can only be rejected and not ultimately confirmed.\nIf the sample size is large, normality of residuals is not required\nIf \\(p\\) is close to the threshold and sample size is small, we would be left in uncertainty.\n\nAll this can only be overcome with careful thinking and some experience.\nIt is always a good idea to discuss results with colleagues and supervisors."
  },
  {
    "objectID": "slides/07-anova.html#sequential-holm-bonferroni-method",
    "href": "slides/07-anova.html#sequential-holm-bonferroni-method",
    "title": "07-One and two-way ANOVA",
    "section": "Sequential Holm-Bonferroni method",
    "text": "Sequential Holm-Bonferroni method\n\nAlso called Holm procedure (Holm, 1979)\nEasy to use\nCan be applied to any multiple test problem\nLess conservative that ordinary Bonferroni correction, but …\n… still a very conservative approach\nsee also Wikipedia\n\nAlgorithm\n\nSelect smallest \\(p\\) out of all \\(n\\) \\(p\\)-values\nIf \\(p \\cdot n < \\alpha\\) \\(\\Rightarrow\\) significant, else STOP\nSet \\(n − 1 \\rightarrow n\\), remove smallest \\(p\\) from the list and go to step 1."
  },
  {
    "objectID": "slides/07-anova.html#example-1",
    "href": "slides/07-anova.html#example-1",
    "title": "07-One and two-way ANOVA",
    "section": "Example",
    "text": "Example\nGrowth rate per day (\\(d^{-1}\\)) of blue-green algae cultures (Pseudanabaena) after adding toxic peptides from another blue-green algae (Microcystis).\nThe original hypothesis was that Microcystin LR (MCYST) or a derivative of it (Substance A) inhibits growth.\n\nmcyst <-  data.frame(treat = factor(c(rep(\"Control\", 5),\n                                       rep(\"MCYST\", 5),\n                                       rep(\"Subst A\", 5)),\n                                levels=c(\"Control\", \"MCYST\", \"Subst A\")),\n                      mu   = c(0.086, 0.101, 0.086, 0.086, 0.099,\n                               0.092, 0.088, 0.093, 0.088, 0.086,\n                               0.095, 0.102, 0.106, 0.106, 0.106)\n                     )"
  },
  {
    "objectID": "slides/07-anova.html#approach-1-one-way-anova",
    "href": "slides/07-anova.html#approach-1-one-way-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Approach 1: one-way ANOVA",
    "text": "Approach 1: one-way ANOVA\n\npar(mar=c(4, 8, 2, 1), las=1)\nm <- lm(mu ~ treat, data=mcyst)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: mu\n          Df     Sum Sq    Mean Sq F value   Pr(>F)   \ntreat      2 0.00053293 2.6647e-04   8.775 0.004485 **\nResiduals 12 0.00036440 3.0367e-05                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(TukeyHSD(aov(m)))"
  },
  {
    "objectID": "slides/07-anova.html#approach-2-multiple-t-tests-with-sequential-bonferroni-correction",
    "href": "slides/07-anova.html#approach-2-multiple-t-tests-with-sequential-bonferroni-correction",
    "title": "07-One and two-way ANOVA",
    "section": "Approach 2: multiple t-Tests with sequential Bonferroni correction",
    "text": "Approach 2: multiple t-Tests with sequential Bonferroni correction\nWe separate the data set in single subsets:\n\nControl <- mcyst$mu[mcyst$treat == \"Control\"]\nMCYST   <- mcyst$mu[mcyst$treat == \"MCYST\"]\nSubstA  <- mcyst$mu[mcyst$treat == \"Subst A\"]\n\nand perform 3 t-Tests:\n\np1 <- t.test(Control, MCYST)$p.value\np2 <- t.test(Control, SubstA)$p.value\np3 <- t.test(MCYST, SubstA)$p.value\n\nThe following shows the raw p-values without correction:\n\nc(p1, p2, p3)\n\n[1] 0.576275261 0.027378832 0.001190592\n\n\nand with Holm correction:\n\np.adjust(c(p1, p2, p3))\n\n[1] 0.576275261 0.054757664 0.003571775"
  },
  {
    "objectID": "slides/07-anova.html#conclusions",
    "href": "slides/07-anova.html#conclusions",
    "title": "07-One and two-way ANOVA",
    "section": "Conclusions",
    "text": "Conclusions\nStatistical methods\n\nIn case of Holm-corrected t-tests, only a single p-value (MCYST vs. Subst A) remains significant. This indicates that in this case, Holm’s method is more conservative than TukeyHSD (only one compared to two significant) effects.\nAn ANOVA with posthoc test is in general preferred,\nbut the sequential Holm-Bonferroni can be helpful in special cases.\nMoreover, it demonstrates clearly that massive multiple testing needs to be avoided.\n\n\\(\\Rightarrow\\) ANOVA is to be preferred, when possible.\nInterpretation\n\nRegarding our original hypothesis, we can see that MCYST and SubstA did not inhibit growth of Pseudanabaena. In fact SubstA stimulated growth.\nThis was contrary to our expectations – the biological reason was then found 10 years later.\n\nMore about this can be found in Jähnichen et al. (2001), Jähnichen et al. (2007), Jähnichen et al. (2011), Zilliges et al. (2011) or Dziallas & Grossart (2011)."
  },
  {
    "objectID": "slides/07-anova.html#ancova",
    "href": "slides/07-anova.html#ancova",
    "title": "07-One and two-way ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\nStatistical question\n\nComparison of regression lines\nSimilar to ANOVA, but contains also metric variables (covariates)\n\nExample\nAnnette Dobson’s birthweight data. A data set from a statistics textbook (Dobson, 2013), birth weight of boys and girls in dependence of the pregnancy week."
  },
  {
    "objectID": "slides/07-anova.html#the-birthweight-data-set",
    "href": "slides/07-anova.html#the-birthweight-data-set",
    "title": "07-One and two-way ANOVA",
    "section": "The birthweight data set",
    "text": "The birthweight data set\nThe data set is found at different places on the internet and in different versions.\nHere the version that is found in an R demo: demo(lm.glm)\n\n## Birth Weight Data see stats/demo/lm.glm.R\ndobson <- data.frame(\n  week = c(40, 38, 40, 35, 36, 37, 41, 40, 37, 38, 40, 38,\n     40, 36, 40, 38, 42, 39, 40, 37, 36, 38, 39, 40),\n  weight = c(2968, 2795, 3163, 2925, 2625, 2847, 3292, 3473, 2628, 3176,\n        3421, 2975, 3317, 2729, 2935, 2754, 3210, 2817, 3126, 2539,\n        2412, 2991, 2875, 3231),\n  sex = gl(2, 12, labels=c(\"M\", \"F\"))\n)"
  },
  {
    "objectID": "slides/07-anova.html#anette-dobsons-birthweight-data",
    "href": "slides/07-anova.html#anette-dobsons-birthweight-data",
    "title": "07-One and two-way ANOVA",
    "section": "Anette Dobson’s birthweight data",
    "text": "Anette Dobson’s birthweight data\nWhy not just using a t-test?\n\nboxplot(weight ~ sex,data=dobson, ylab=\"weight\")\n\nt.test(weight ~ sex, data=dobson, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by sex\nt = 0.97747, df = 22, p-value = 0.339\nalternative hypothesis: true difference in means between group M and group F is not equal to 0\n95 percent confidence interval:\n -126.3753  351.7086\nsample estimates:\nmean in group M mean in group F \n       3024.000        2911.333 \n\n\nThe box plot shows much overlap and the difference is not significant, because the t-test ignores important information: the pregnancy week."
  },
  {
    "objectID": "slides/07-anova.html#ancova-makes-use-of-covariates",
    "href": "slides/07-anova.html#ancova-makes-use-of-covariates",
    "title": "07-One and two-way ANOVA",
    "section": "ANCOVA makes use of covariates",
    "text": "ANCOVA makes use of covariates\n\nm <- lm(weight ~ week * sex, data = dobson)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nweek       1 1013799 1013799 31.0779 1.862e-05 ***\nsex        1  157304  157304  4.8221   0.04006 *  \nweek:sex   1    6346    6346  0.1945   0.66389    \nResiduals 20  652425   32621                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-anova.html#pitfalls-of-anova-and-ancova-described-so-far",
    "href": "slides/07-anova.html#pitfalls-of-anova-and-ancova-described-so-far",
    "title": "07-One and two-way ANOVA",
    "section": "Pitfalls of ANOVA and ANCOVA described so far",
    "text": "Pitfalls of ANOVA and ANCOVA described so far\n\n\nHeterogeneity of variance\n\np-values can be biased (i.e. misleading or wrong)\nuse of a one-way ANOVA for uneaqual variances (in R: oneway.test)\n\nUnbalanced case: unequal number of samples for each factor combination \\(\\rightarrow\\) ANOVA results depend on the order of factors in the model formula.\n\nClassical method: Type II or Type III ANOVA\nModern approach: model selection and likelihood ratio tests"
  },
  {
    "objectID": "slides/07-anova.html#type-ii-and-type-iii-anova",
    "href": "slides/07-anova.html#type-ii-and-type-iii-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Type II and type III ANOVA",
    "text": "Type II and type III ANOVA\n\n\nfunction Anova (with upper case A) in package car\nHelp file of function Anova:\n\n\n“Type-II tests are calculated according to the principle of marginality, testing each term after all others, except ignoring the term’s higher-order relatives; so-called type-III tests violate marginality, testing each term in the model after all of the others.”\n\n\nConclusion: use Type II and don’t try to interpret single terms in case of significant interactions."
  },
  {
    "objectID": "slides/07-anova.html#type-ii-anova-example",
    "href": "slides/07-anova.html#type-ii-anova-example",
    "title": "07-One and two-way ANOVA",
    "section": "Type II ANOVA: Example",
    "text": "Type II ANOVA: Example\n\n\nlibrary(\"car\")\nm <- lm(growth ~ coat * diet, data = hams)\nAnova(m, type=\"II\")\n\nAnova Table (Type II tests)\n\nResponse: growth\n           Sum Sq Df F value  Pr(>F)  \ncoat      2.61333  1  7.2258 0.03614 *\ndiet      2.66000  2  3.6774 0.09069 .\ncoat:diet 0.68667  2  0.9493 0.43833  \nResiduals 2.17000  6                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-anova.html#model-selection-a-paradigm-change",
    "href": "slides/07-anova.html#model-selection-a-paradigm-change",
    "title": "07-One and two-way ANOVA",
    "section": "Model selection – a paradigm change",
    "text": "Model selection – a paradigm change\n\nProblem:\n\nIn complicated models, p-values depend on number (and sometimes of order) of included factors and interactions.\nThe \\(H_0\\)-based approach becomes confusing, e.g. because of contradictory p-values.\n\nAlternative approach:\nComparison of different model candidates instead of p-value based testing.\n\nModel with all potentiall effects → full model,\nOmit single factors → reduced models (several!),\nNo influence factors (ony mean value) → null model.\nWhich model is the best → minimal adequate model?"
  },
  {
    "objectID": "slides/07-anova.html#how-can-we-measure-which-model-is-the-best",
    "href": "slides/07-anova.html#how-can-we-measure-which-model-is-the-best",
    "title": "07-One and two-way ANOVA",
    "section": "How can we measure which model is the best?",
    "text": "How can we measure which model is the best?\n\nCompromise between model fit and model complexity (number of parameters, k).\n\nGoodness of fit: Likelihood L (measures how good the data match a given model).\nLog Likelihood: makes the criterion additive.\nAIC (Akaike Information Criterion):\n\n\\[AIC = −2 \\ln(L) + 2k\\]\n\nBIC (Bayesian Information Criterion), takes sample size into account (\\(n\\)):\n\n\\[BIC = −2 \\ln(L) + k · \\ln(n)\\]\nThe model with the smallest AIC (or BIC) is the minimal adequate (i.e. optimal) model."
  },
  {
    "objectID": "slides/07-anova.html#model-selection-and-likelihood-ratio-tests",
    "href": "slides/07-anova.html#model-selection-and-likelihood-ratio-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Model Selection and Likelihood Ratio Tests",
    "text": "Model Selection and Likelihood Ratio Tests\nApproach\n\nFit several models individually\nCompare the models pairwise with ANOVA (likelihood ratio test)\n\nData and example\n\nhams <- data.frame(No=1:12,\n                   growth=c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,\n                            8.3, 8.7, 8.1, 8.5, 9.1, 9.0),\n                   diet= rep(c(\"A\", \"B\", \"C\"), each=2),\n                   coat= rep(c(\"light\", \"dark\"), each=6)\n                   )\n\n\nm1 <- lm(growth ~ diet * coat, data=hams)\nm2 <- lm(growth ~ diet + coat, data=hams)\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: growth ~ diet * coat\nModel 2: growth ~ diet + coat\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1      6 2.1700                           \n2      8 2.8567 -2  -0.68667 0.9493 0.4383\n\n\n\nLikelihood ratio test compares two models (anova with > 1 model)\nModel with interaction (m1) not significantly better than model without interaction (m2)."
  },
  {
    "objectID": "slides/07-anova.html#aic-based-model-selection",
    "href": "slides/07-anova.html#aic-based-model-selection",
    "title": "07-One and two-way ANOVA",
    "section": "AIC-based model selection",
    "text": "AIC-based model selection\n\nPairwise model comparison is cumbersome, especially for large number of models.\nSolution\nUse Akaike Information Criterion and select the optimal (or: minimal adequate) model.\n\nAIC(m1, m2)\n\n   df      AIC\nm1  7 27.53237\nm2  5 26.83151\n\n\nConclusion: take the simpler model (m2)\nExercise: Generate all possible models and select the model with minimum AIC."
  },
  {
    "objectID": "slides/07-anova.html#automatic-model-selection",
    "href": "slides/07-anova.html#automatic-model-selection",
    "title": "07-One and two-way ANOVA",
    "section": "Automatic Model Selection",
    "text": "Automatic Model Selection\n\nThe full model is supplied to the step function.\nThe model with the smallest AIC ist the optimal model:\n\n\n\nm1 <- lm(growth ~ diet * coat, data=hams)\nopt <- step(m1)\n\nStart:  AIC=-8.52\ngrowth ~ diet * coat\n\n            Df Sum of Sq    RSS     AIC\n- diet:coat  2   0.68667 2.8567 -9.2230\n<none>                   2.1700 -8.5222\n\nStep:  AIC=-9.22\ngrowth ~ diet + coat\n\n       Df Sum of Sq    RSS     AIC\n<none>              2.8567 -9.2230\n- diet  2    2.6600 5.5167 -5.3256\n- coat  1    2.6133 5.4700 -3.4275\n\n\n\n\n\nsummary(opt)\n\n\nCall:\nlm(formula = growth ~ diet + coat, data = hams)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6333 -0.3458 -0.1000  0.2333  0.8667 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   8.1667     0.3450  23.671 1.08e-08 ***\ndietB         0.2500     0.4225   0.592   0.5704    \ndietC         1.1000     0.4225   2.603   0.0315 *  \ncoatlight    -0.9333     0.3450  -2.705   0.0269 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5976 on 8 degrees of freedom\nMultiple R-squared:  0.6486,    Adjusted R-squared:  0.5169 \nF-statistic: 4.923 on 3 and 8 DF,  p-value: 0.03178"
  },
  {
    "objectID": "slides/07-anova.html#summary",
    "href": "slides/07-anova.html#summary",
    "title": "07-One and two-way ANOVA",
    "section": "Summary",
    "text": "Summary\n\nLinear models form the basis of many statistical methods\n\nLinear regression\nANOVA, ANCOVA, GLM, GAM, GLMM, . . .\nANOVA/ANCOVA instead of multiple testing\n\nANOVA is more powerful than multiple tests:\n\none big experiment needs less n than many small experiments together\nidentification of interaction effects\nelimination of co-variates\n\nModel selection vs. p-value based testing\n\nparadigm shift in statistics: AIC instead of p-value\nmore reliable, especially for imbalanced or complex designs\nbut: p-value based tests are sometimes easier to understand"
  },
  {
    "objectID": "slides/07-anova.html#five-ways-to-fix-statistics.-comment-on-nature",
    "href": "slides/07-anova.html#five-ways-to-fix-statistics.-comment-on-nature",
    "title": "07-One and two-way ANOVA",
    "section": "Five ways to fix statistics. Comment on Nature",
    "text": "Five ways to fix statistics. Comment on Nature\n\nLeek et al. (2017) https://doi.org/10.1038/d41586-017-07522-z\n\nJeff Leek: Adjust for human cognition\nBlakeley B. McShane & Andrew Gelman: Abandon statistical significance\nDavid Colquhoun: State false-positive risk, too\nMichèle B. Nuijten: Share analysis plans and results\nSteven N. Goodman: Change norms from within\n\nAnother blog post that aims to improve understanding: http://daniellakens.blogspot.de/2017/12/understanding-common-misconceptions.html?m=1\n\n\nMy conclusion: The p-value is still useful but apply it with great care."
  },
  {
    "objectID": "slides/07-anova.html#bibliography",
    "href": "slides/07-anova.html#bibliography",
    "title": "07-One and two-way ANOVA",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\nCrawley, M. J. (2002). Statistical computing. An introduction to data analysis using S-PLUS (pp. 1–761). Wiley. datasets: http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/\n\n\nDobson, A. J. (2013). Introduction to statistical modelling. Springer.\n\n\nDziallas, C., & Grossart, H.-P. (2011). Increasing Oxygen Radicals and Water Temperature Select for Toxic Microcystis sp. PLoS ONE, 6(9), e25569. https://doi.org/10.1371/journal.pone.0025569\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 65–70. https://www.jstor.org/stable/4615733\n\n\nJähnichen, S., Ihle, T., Petzoldt, T., & Benndorf, J. (2007). Impact of Inorganic Carbon Availability on Microcystin Production by Microcystis aeruginosa PCC 7806. Applied and Environmental Microbiology, 73(21), 6994–7002. https://doi.org/10.1128/AEM.01253-07\n\n\nJähnichen, S., Long, B. M., & Petzoldt, T. (2011). Microcystin production by Microcystis aeruginosa: Direct regulation by multiple environmental factors. Harmful Algae, 12, 95–104. https://doi.org/10.1016/j.hal.2011.09.002\n\n\nJähnichen, S., Petzoldt, T., & Benndorf, J. (2001). Evidence for control of microcystin dynamics in Bautzen Reservoir (Germany) by cyanobacterial population growth rates and dissolved inorganic carbon. Fundamental and Applied Limnology, 150(2), 177–196. https://doi.org/10.1127/archiv-hydrobiol/150/2001/177\n\n\nJohnson, G., Jerald, & Omland, K. S. (2004). Model Selection in Ecology and Evolution. Trends in Ecology and Evolution, 19(2), 101–108. https://doi.org/10.1016/j.tree.2003.10.013\n\n\nZilliges, Y., Kehr, J.-C., Meissner, S., Ishida, K., Mikkat, S., Hagemann, M., Kaplan, A., Börner, T., & Dittmann, E. (2011). The Cyanobacterial Hepatotoxin Microcystin Binds to Proteins and Increases the Fitness of Microcystis under Oxidative Stress Conditions. PLoS ONE, 6(3), e17615. https://doi.org/10.1371/journal.pone.0017615"
  },
  {
    "objectID": "slides/07-anova.html#model-formula-examples",
    "href": "slides/07-anova.html#model-formula-examples",
    "title": "07-One and two-way ANOVA",
    "section": "Model formula examples",
    "text": "Model formula examples\n\n\n\n\n\n\n\nModel Type\nFormula\n\n\n\n\nNull model\ny ~ 1\n\n\nSimple linear regression\ny ~ x\n\n\nLinear model without intercept\ny ~ x - 1\n\n\nMultiple regression, no interaction\ny ~ x1 + x2 + x3\n\n\nMultiple regression with interaction\ny ~ x1 * x2 * x3\n\n\nMultiple regression no 3x interaction\ny ~ x1 * x2 * x3 - x1 : x2 : x3\n\n\nTransformed with ‘as is’ function\ny ~ x + I(x^2), y ~ x + sin(x^2)\n\n\nOrthogonal polynomial of order 3\ny ~ poly(x, 3)\n\n\nANOVA with interaction\ny ~ f1 * f2\n\n\nANCOVA with Interaction\ny ~ x * f\n\n\nConditional or nested ANOVA\ny ~ x + (x | b), y ~ x + (1 | a / b)\n\n\nGAM with smoother s\ny ~ s(x) + f\n\n\n\n\n\n\n\n\ny = response variable (dependent, target)\nx = metric explanation variable (predictor, independent); f = factor variable (nominal)"
  }
]