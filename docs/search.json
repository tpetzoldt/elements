[
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Elements of applied statistics – a practical course",
    "section": "",
    "text": "This website contains a collection of material for introductory statistics courses with R. The aim is to provide insight in fundamental principles and a broad overview and enable students to select and understand particular books and online material to dig in deeper in the diverse and fascinating field of statistics."
  },
  {
    "objectID": "00-index.html#further-reading",
    "href": "00-index.html#further-reading",
    "title": "Elements of applied statistics – a practical course",
    "section": "Further reading",
    "text": "Further reading\n\nDalgaard, P., 2008: Introductory Statistics with R. Springer, New York, 2nd edition. https://link.springer.com/book/10.1007/b97671\nVerzani, J. (2019). Using R for introductory statistics. CRC press. https://doi.org/10.1111/anzs.12146\nKleiber, C. and Zeileis, A., 2008: Applied Econometrics with R, Springer Verlag, New York. https://link.springer.com/book/10.1007/978-0-387-77318-6\nWickham, H., Çetinkaya-Rundel, M and Grolemund, G, 2023: R for Data Science. https://r4ds.hadley.nz/"
  },
  {
    "objectID": "00-index.html#status",
    "href": "00-index.html#status",
    "title": "Elements of applied statistics – a practical course",
    "section": "Status",
    "text": "Status\nThis is an alpha version of slides for basic statistics courses. The selection of topics is somewhat arbitrary and without question very basic. It reflects my experience what I think is useful as door-opener.\nIt is based on former PDF slides and other material. It is still in an early stage of development, constructive comments and suggestions for improvement are welcome."
  },
  {
    "objectID": "00-index.html#author",
    "href": "00-index.html#author",
    "title": "Elements of applied statistics – a practical course",
    "section": "Author",
    "text": "Author\nhttps://tu-dresden.de/Members/thomas.petzoldt\nhttps://github.com/tpetzoldt\n2023-11-05"
  },
  {
    "objectID": "slides/01-introduction.html#goals-of-the-course",
    "href": "slides/01-introduction.html#goals-of-the-course",
    "title": "01-Introduction",
    "section": "Goals of the course",
    "text": "Goals of the course\n\n\nIntroduction to “Data Science”\nStatistical concepts and selected methods\n\nStatistical parameters\nDistributions and probability\nStatistical tests\nModel selection\n\nPractical experience\n\nData strutures\nBasics of the R language\nApplications with real and simulated data sets\n\n\n\\(\\Rightarrow\\) Practical understanding and “statistical feeling”,\n\\(\\rightarrow\\) More important than facts learned by heart."
  },
  {
    "objectID": "slides/01-introduction.html#topics",
    "href": "slides/01-introduction.html#topics",
    "title": "01-Introduction",
    "section": "Topics",
    "text": "Topics\n\n\nBasic Concepts of Statistics\nAn Introduction to R\nStatistical Parameters and Distributions\nLinear Models\nAnalysis of Variance\nNonlinear Regression\nTime Series Analysis\n(Multivariate Statistics)"
  },
  {
    "objectID": "slides/01-introduction.html#material",
    "href": "slides/01-introduction.html#material",
    "title": "01-Introduction",
    "section": "Material",
    "text": "Material\n\n\nSlides\nExercises\nCourse script for self study “Data Analysis with R – Selected Topics and Examples”\nNote: Slides, script and exercises are regularly updated, depending on the progress of the course.\nExam: Written exam at the end of the semester. \\(\\rightarrow\\) Attend the labs!\n\n\n\nQuestions?"
  },
  {
    "objectID": "slides/01-introduction.html#an-introductory-example",
    "href": "slides/01-introduction.html#an-introductory-example",
    "title": "01-Introduction",
    "section": "An introductory example",
    "text": "An introductory example\nDaily average discharge of River Elbe, pegel Dresden, river km 55.6\ndate,       discharge\n1806-01-01,  472\n1806-01-02, 1050\n1806-01-03, 1310\n1806-01-04, 1020\n1806-01-05,  767\n1806-01-06,  616\n...\n2020-10-11,  216\n2020-10-12,  204\n2020-10-13,  217\n2020-10-14,  288\n2020-10-15,  440\n2020-10-16,  601\n2020-10-17,  570\n2020-10-18,  516\n2020-10-19,  450\n2020-10-20,  422\n2020-10-21,  396\n2020-10-22,  372\n2020-10-23,  356\n2020-10-24,  357\n2020-10-25,  332\n2020-10-26,  303\n2020-10-27,  302\n2020-10-28,  316\n2020-10-29,  321\n2020-10-30,  331\n2020-10-31,  353\n2020-11-01,  395\n\\(>\\) 70,000 measurements. How can we analyse this and what does it mean?\nData Source: Bundesanstalt für Gewässerkunde"
  },
  {
    "objectID": "slides/01-introduction.html#plot-the-last-20-years",
    "href": "slides/01-introduction.html#plot-the-last-20-years",
    "title": "01-Introduction",
    "section": "Plot the last 20 years",
    "text": "Plot the last 20 years\n\nDischarge of the Elbe River, gauge station Dresden, data source BfG"
  },
  {
    "objectID": "slides/01-introduction.html#what-do-these-data-tell-us",
    "href": "slides/01-introduction.html#what-do-these-data-tell-us",
    "title": "01-Introduction",
    "section": "What do these data tell us?",
    "text": "What do these data tell us?\n\n\nWhat is the average discharge? → mean values\nHow much variation is in the data? → variance\nHow likely are droughts or floods? → distribution\nHow precise are our forecasts? → confidence intervals\nWhich factors influence discharge? → correlations"
  },
  {
    "objectID": "slides/01-introduction.html#how-to-start",
    "href": "slides/01-introduction.html#how-to-start",
    "title": "01-Introduction",
    "section": "How to start",
    "text": "How to start\n\n\nMean value: 224\nMedian value: 224\nStandard deviation: 253\nRange: 2, 4500\n\nWhich of these parameters are most appropriate?"
  },
  {
    "objectID": "slides/01-introduction.html#graphics",
    "href": "slides/01-introduction.html#graphics",
    "title": "01-Introduction",
    "section": "Graphics",
    "text": "Graphics"
  },
  {
    "objectID": "slides/01-introduction.html#boxplots",
    "href": "slides/01-introduction.html#boxplots",
    "title": "01-Introduction",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nNote the log scale of y!\nIn the right version, whiskers extend to the most extreme data point which is no more than 1.5 times the interquartile range from the box."
  },
  {
    "objectID": "slides/01-introduction.html#three-ways-to-work-with-statistics",
    "href": "slides/01-introduction.html#three-ways-to-work-with-statistics",
    "title": "01-Introduction",
    "section": "Three ways to work with statistics",
    "text": "Three ways to work with statistics\nDescriptive statistics and graphics\n\nplots, like in the examples\nmean values, standard deviations, …\ninterpret raw data\n\nHypothesis testing\n\ndistinguish effects from random fluctuations\nmake results more convincing\n\nStatistical modelling\n\nmeasure size of effects (e.g. climate trends)\nbuild models that aggregate dependencies\nmachine learning"
  },
  {
    "objectID": "slides/01-introduction.html#statistical-hypothesis-testing",
    "href": "slides/01-introduction.html#statistical-hypothesis-testing",
    "title": "01-Introduction",
    "section": "Statistical hypothesis testing",
    "text": "Statistical hypothesis testing\n\nHow likely is it, that our hypothesis is true?\n\n\nTurn scientific into statistical hypothesis\nEstimate probability (p value) of a given hypothesis Examples\nIs a medical treatment successful or not? → \\(\\chi^2\\)-test\nDoes a specific food diet increase yield of a fish farm? → t-test\nWhich factors (e.g. food, temperature, pH) of a combined treatment influence growth of aquatic animals? → ANOVA\n(How) does observed algal biomass depend on phosphorus?"
  },
  {
    "objectID": "slides/01-introduction.html#statistical-modeling",
    "href": "slides/01-introduction.html#statistical-modeling",
    "title": "01-Introduction",
    "section": "Statistical modeling",
    "text": "Statistical modeling\nFit a statistical model to the data\n\nSelect proper modelling strategy\nDesign statistical models\nMeasure effect size\nSelect the optimum model between different model candidates\n\nExamples\n\nFit a distribution to annual discharge data to estimate the 100 year flood.\nFit an ANOVA model to experimental data to see which factors influence the result most.\nFit a multiple linear model to climate data to see how much climate trends differ between geographical location."
  },
  {
    "objectID": "slides/01-introduction.html#example-compare-two-mean-values",
    "href": "slides/01-introduction.html#example-compare-two-mean-values",
    "title": "01-Introduction",
    "section": "Example: Compare two mean values",
    "text": "Example: Compare two mean values\n\n\nA given data set (Dobson, 1983) contains the birth weight (in g) of 12 boys and 12 girls.\nHas the weight difference something to do with the gender of the babies or is it a purely random fluctuation?"
  },
  {
    "objectID": "slides/01-introduction.html#example-correlation-and-regression",
    "href": "slides/01-introduction.html#example-correlation-and-regression",
    "title": "01-Introduction",
    "section": "Example: Correlation and regression",
    "text": "Example: Correlation and regression\n\n\n\n\n\n\nDependence of chlorophyll concentration in lakes on phosphorus, a regional data set from Koschel and Scheffler (1985) (left) and from Vollenweider and Kerekes (1980) (right).\nWhich of the two figures has greater predictive power? Why?\n\n\n\nThe parameter \\(r\\) is the Pearson correlation coefficient."
  },
  {
    "objectID": "slides/01-introduction.html#required-software",
    "href": "slides/01-introduction.html#required-software",
    "title": "01-Introduction",
    "section": "Required software",
    "text": "Required software\n\nA spreadsheet program, Excel or LibreOffice https://www.libreoffice.org/\nThe R system for data analysis and graphics https://www.r-project.org\nRStudio for making R more user-friendly https://posit.co/download/rstudio-desktop/"
  },
  {
    "objectID": "slides/01-introduction.html#why-r",
    "href": "slides/01-introduction.html#why-r",
    "title": "01-Introduction",
    "section": "Why R?",
    "text": "Why R?\n\n\nStatisticians call it “lingua franca” in computational statistics.\n\nExtremely powerful\nNo other system has so much statistics\nUsed in statistical research\n\nFree (OpenSource)\n\nFree to use\nFree to modify\nFree to contribute\n\nLess complicated than its first appearance:\n\nYes, it needs command line programming\nbut: already a single line can do much\nhuge number of books and online scripts\n\n\n\n\n\n\nIn contrast to other systems Copy & Paste is allowed! – just cite it."
  },
  {
    "objectID": "slides/01-introduction.html#books",
    "href": "slides/01-introduction.html#books",
    "title": "01-Introduction",
    "section": "Books",
    "text": "Books\nStatistics\n\nWell-readable introductions\n\nDalgaard, P., 2008: Introductory Statistics with R. Springer, New York, 2nd edition. (fulltext of the 1st edition freely available)\nVerzani, J. (2019). Using R for introductory statistics. CRC press. Fulltext available via DOI https://doi.org/10.1111/anzs.12146\n\nA very well understandable introduction into many fields: of statistics, especially regression and time series analysis:\n\nKleiber, C. and Zeileis, A., 2008: Applied Econometrics with R, Springer Verlag, New York. https://link.springer.com/book/10.1007/978-0-387-77318-6\n\n\nR Programming\n\nAn introduction to data science using the modern “tidyverse” approach:\n\nWickham, H., Çetinkaya-Rundel, M and Grolemund, G, 2023: R for Data Science. https://r4ds.hadley.nz/\n\n\n\n\n\n\n\n\nAnd lots of material available freely on the internet …"
  },
  {
    "objectID": "slides/02-terminology.html#basic-principles-and-terminology",
    "href": "slides/02-terminology.html#basic-principles-and-terminology",
    "title": "02-Basic Terminology",
    "section": "Basic Principles and Terminology",
    "text": "Basic Principles and Terminology\n\n\nGoals of statistical analyses\nDescriptive and experimental research\nThe principle of parsimony\nTypes of variables\nProbability\nSample and Population\nRandom and systematic errors\nPopulation and sample parameters"
  },
  {
    "objectID": "slides/02-terminology.html#goals-of-statistical-analyses",
    "href": "slides/02-terminology.html#goals-of-statistical-analyses",
    "title": "02-Basic Terminology",
    "section": "Goals of statistical analyses",
    "text": "Goals of statistical analyses\n\nSummarise, condense and describe data (descriptive statistics)\n\nwork efficiently with large data sets\nEstimate statistical parameters, mean values, variation, correlation\n\nCreate hypotheses from data (explorative statistics)\n\ndata mining and explorative statistics\ngraphical methods, multivariate statistics\n\nTest Hypotheses (statistical inference)\n\nclassical tests, ANOVA, correlation, . . .\nmodel selection\n\nPlan research (experimental design)\n\neffect size compared to random error\nexperimental layout and required sample size\n\nStatistical modelling\n\nmeasure effect size, find best explanation for a problem\npattern recognition, forecasting, machine learning"
  },
  {
    "objectID": "slides/02-terminology.html#descriptive-or-experimental-research",
    "href": "slides/02-terminology.html#descriptive-or-experimental-research",
    "title": "02-Basic Terminology",
    "section": "Descriptive or experimental research",
    "text": "Descriptive or experimental research\nDescriptive Research\n\nFind effects and relationships between data.\n\nobservation, monitoring, correlations\nthe research subject is not manipulated\n\n\nExperimental Research\n\nCan an expected effect be reproduced?\n\nmanipulation of single conditions\nelimination of disturbances (controlled boundary conditions)\nexperimental design as simple as possible\n\n\nStrong inference requires clear hypothesis and experimental research.\nWeak inference derived from observations and data.\n\\(\\rightarrow\\) descriptive research delivers the data for creating the hypotheses."
  },
  {
    "objectID": "slides/02-terminology.html#the-principle-of-parsimony",
    "href": "slides/02-terminology.html#the-principle-of-parsimony",
    "title": "02-Basic Terminology",
    "section": "The principle of parsimony",
    "text": "The principle of parsimony\nAttributed to an English philosopher from the 14th century (“Occams razor”)\n\nWhen you have two competing theories that make exactly the same predictions, the simpler one is the better.\n\nIn the context ofstatistical analysis and modeling:\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments should rely on only few assumptions\nmodels should be simplified until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\nOne of the most important scientific principles\n\\(\\rightarrow\\) But nature is complex, over-simplification has to be avoided.\n\nneeds critical reflection and discussion"
  },
  {
    "objectID": "slides/02-terminology.html#variables-and-parameters",
    "href": "slides/02-terminology.html#variables-and-parameters",
    "title": "02-Basic Terminology",
    "section": "Variables and parameters",
    "text": "Variables and parameters\n\n\ny = a + b \\(\\cdot\\) x\n\n\n\nvariables: everything that is measured or experimentally manipulated, e.g phosphorus concentration in a lake, air temperature, or abundance of animals.\nparameters: values that are estimated by a statistical model, e.g. mean, standard deviation, slope of a linear model.\n\nIndependent variables (explanation variables, predictors)\n\nare manually controlled or assumed to result from non-controllable factors\n\nDependent variables (response variables, target variables, predicted variables)\n\nthe variables of interest that we try to understand."
  },
  {
    "objectID": "slides/02-terminology.html#scales-of-variables",
    "href": "slides/02-terminology.html#scales-of-variables",
    "title": "02-Basic Terminology",
    "section": "Scales of variables",
    "text": "Scales of variables\n\nBinary (boolean variable): exactly two states: true/false, 1/0, present or absent.\nNominal: named entities, no order, {red, yellow, green}, list of species.\nOrdinal variables (ranks, ordered factors): values or terms with an order {1., 2., 3., …}; {oligotrophic, mesotrophic, eutrophic, polytrophic, hypertrophic}, but not “dystrophic”\nMetric: continuous (ideally without steps). Two sub-types:\n\nInterval scale: allows comparison and differences, but ratios make no sense. (20°C is 10 degrees warmer than als 10°C, but not double)\nRatio scale: data with an absolute zero, ratios make sense.\nA tree with 2m has double the hight of a tree with 1m.\n\n\nThe “level” of variables increases from binary to ratio scale. It is always possible to convert a higher to a lower level. This results in a certain amount of information loss but sometimes helps to do proper analysis.\n\nmetric \\(\\rightarrow\\) ordinal: ranking\nmetric or ordinal \\(\\rightarrow\\) binary: threshold\nnominal \\(\\rightarrow\\) binary: assign to two groups"
  },
  {
    "objectID": "slides/02-terminology.html#probability",
    "href": "slides/02-terminology.html#probability",
    "title": "02-Basic Terminology",
    "section": "Probability",
    "text": "Probability\nClassical definition\n\nprobability \\(p\\) is the chance of a specific event:\n\n\\[\np = \\frac{\\text{number of selected cases}}{\\text{number of all possible cases}}\n\\]\n\n1 or 6 on a dice \\(p=2/6\\)\nproblem if denominator becomes infinite\n\nAxiomatic definition\n\nAxiom I: \\(0 \\le p \\le 1\\)\nAxiom II: impossible events have \\(p=0\\), safe events have \\(p=1\\)\nAxiom III: for mutually exlusive events \\(A\\) and \\(B\\), i.e. in set theory \\(A \\bigcap B = \\emptyset\\) holds: \\(p(A \\bigcup B)= p(A) + p(B)\\)"
  },
  {
    "objectID": "slides/02-terminology.html#sample-and-population",
    "href": "slides/02-terminology.html#sample-and-population",
    "title": "02-Basic Terminology",
    "section": "Sample and Population",
    "text": "Sample and Population\n\nSample\nSubjects, from which we have measurements or observations\n\nPopulation\nSet of all subjects that had the same chance to become part of the sample.\n\\(\\Rightarrow\\) The population is defined by the way how samples are taken\n\\(\\Rightarrow\\) Samples should be representative for our intended observational subject."
  },
  {
    "objectID": "slides/02-terminology.html#sampling-strategies",
    "href": "slides/02-terminology.html#sampling-strategies",
    "title": "02-Basic Terminology",
    "section": "Sampling strategies",
    "text": "Sampling strategies\n\nRandom sampling\n\nIndividuals are selected at random from a given population.\nExamples:\n\nRandom selection of sample sites on a grid.\nRandom placement of experimental units on a shelf.\n\n\n Stratified sampling\n\nThe population is subdivided into classes of similar subjects (strata).\nThe strata are separately analysed and then the the information is weighted and combined to infer about the population.\nStratified sampling requires information about the size and representativity of the strata.\nExamples: election forecasts, depth layers in a lake, age classes for animals."
  },
  {
    "objectID": "slides/02-terminology.html#random-and-systematic-errors",
    "href": "slides/02-terminology.html#random-and-systematic-errors",
    "title": "02-Basic Terminology",
    "section": "Random and systematic errors",
    "text": "Random and systematic errors\n\nRandom errors\n\ncan be estimated with statistical methods\nare eliminated if sample size is large\nin large samples, big and small errors average out\n\nSystematic Errors also called bias\n\ncan often not easily be estimated with statistical methods alone\nknowledge about the considered system\nelimination requires calibration using standards, blind values or pairing"
  },
  {
    "objectID": "slides/02-terminology.html#population-and-sample-parameters",
    "href": "slides/02-terminology.html#population-and-sample-parameters",
    "title": "02-Basic Terminology",
    "section": "Population and sample parameters",
    "text": "Population and sample parameters\n\n“True” parameters of the population\n\nsymbolized with greek letters, (\\(\\mu, \\sigma, \\gamma\\, \\alpha, \\beta\\))\nusually unknown\nestimated from a sample\n\n“Calculated” parameters from a sample\n\nsymbolized with latin letters (\\(\\bar{x}\\), \\(s\\), \\(r^2\\), …)\nthe calculation is done from a sample\nstatisticians say “estimation” instead of “calculation”\nparameters can themselves be treated as a random variable"
  },
  {
    "objectID": "slides/02-terminology.html#expectation-value",
    "href": "slides/02-terminology.html#expectation-value",
    "title": "02-Basic Terminology",
    "section": "Expectation value",
    "text": "Expectation value\n\nA single measurement \\(x_i\\) of a random variable \\(X\\) can be written as the sum of the expected value \\(\\mathbf{E}(X)\\) of the random variable and a random error \\(\\varepsilon_i\\).\n\\[\\begin{align}\n  x_i &= \\mathbf{E}(X) + \\varepsilon_i\\\\\n  \\mathbf{E}(\\varepsilon)&=0\n\\end{align}\\]\nExample:\n\nfor a fair dice with 6 eyes, true mean \\(\\mu\\) should be 3.5\nin reality it is not exactly known if the dice is a perfect cubus\n\nExample: 3 people with 5 trials:\n\n\n\nsample 1:  3 3 2 4 1  mean: 2.6\n\n\nsample 2:  6 1 1 6 1  mean: 3\n\n\nsample 3:  6 5 6 6 5  mean: 5.6\n\n\n\nOverall mean: \\(\\bar{x} = 3.73\\) is close to \\(\\mu = 3.5\\)."
  },
  {
    "objectID": "slides/03-statparams.html#statistical-parameters",
    "href": "slides/03-statparams.html#statistical-parameters",
    "title": "03-Statistical Parameters",
    "section": "Statistical Parameters",
    "text": "Statistical Parameters\n\n\\(\\rightarrow\\) Remember: calculation of statistical parameters is called estimation\nProperties of statistical parameters\n\nUnbiasedness: the estimation converges towards the true value with increasing \\(n\\)\nEfficiency a relatively small \\(n\\) is sufficient for a good estimation\nRobustness the estimation is not much influenced by outliers or certain violations of statistical assumptions\n\nDepending on a particular question, different classes of parameters exist, especially measures of location (e.g. mean, median), variation (e.g. variance, standard deviation) or dependence (e.g. correlation)."
  },
  {
    "objectID": "slides/03-statparams.html#measures-of-location-i",
    "href": "slides/03-statparams.html#measures-of-location-i",
    "title": "03-Statistical Parameters",
    "section": "Measures of location I",
    "text": "Measures of location I\n\nArithmetic mean\n\\[\n  \\bar{x} = \\frac{1}{n} \\cdot {\\sum_{i=1}^n x_i}\n\\]\n\n\nGeometric mean\n\\[\n  G = \\sqrt[n]{\\prod_{i=1}^n x_i}\n\\]\nmore practical: logarithmic form:\n\\[\n  G =\\exp\\Bigg(\\frac{1}{n} \\cdot {\\sum_{i=1}^n \\ln{x_i}}\\Bigg)\n\\]\navoids huge numbers that make problems for the computer."
  },
  {
    "objectID": "slides/03-statparams.html#measures-of-location-ii",
    "href": "slides/03-statparams.html#measures-of-location-ii",
    "title": "03-Statistical Parameters",
    "section": "Measures of location II",
    "text": "Measures of location II\nHarmonic mean\n\\[\n    \\frac{1}{H}=\\frac{1}{n}\\cdot \\sum_{i=1}^n \\frac{1}{x_i} \\quad; x_i>0\n\\]\nExample:\nYou drive with 50km/h to the university and with 100km/h back home. What is the mean velocity?\nResult:\n1/((1/50 + 1/100)/2) = 1/((0.02 + 0.01)/2) = 1/0.015 = 66.67"
  },
  {
    "objectID": "slides/03-statparams.html#median-central-value",
    "href": "slides/03-statparams.html#median-central-value",
    "title": "03-Statistical Parameters",
    "section": "Median (central value)",
    "text": "Median (central value)\n \\(n\\) uneven: sort data, take the middle value\n\\[\\tilde{x} = x_{(n+1)/2}\\] \\(n\\) even: sort data, take average of the two middle values\n\\[\\tilde{x} = \\frac{x_{n/2}+x_{n/2+1}}{2}\\]\nExample\n\n\n\n\n\n\n\n\n\n\nsample with 7 values\n2.9, 7.9, 4.1, 8.8, 9.4, 0.5, 5.3\n\n\nordered sample\n0.5, 2.9, 4.1, 5.3, 7.9, 8.8, 9.4\n\n\n\n\n\n\n\n\\(\\Rightarrow\\) median: \\(\\tilde{x} = 5.3\\)\n\\(\\Rightarrow\\) mean: \\(\\bar{x} = 5.5571429\\)"
  },
  {
    "objectID": "slides/03-statparams.html#trimmed-mean",
    "href": "slides/03-statparams.html#trimmed-mean",
    "title": "03-Statistical Parameters",
    "section": "Trimmed mean",
    "text": "Trimmed mean\n\n\nalso called “truncated mean”\ncompromize between the arithmetic mean and median\nA certain percentage of smallest and biggest values is ignored (e.g. 10% or 25%) before calculating the arithmetic mean\nused also in sports\n\nExample: sample with 20 values, exclude 10% at both sides\n\n\n\n0.4, 0.5, 1, 2.5, 2.9, 3.3, 4.1, 4.5, 4.6, 5.3, 5.5, 5.7, 6.8, 7.9, 8.8, 8.9, 9, 9.4, 9.6, 46\n\\(\\rightarrow\\) arithmetic mean: \\(\\bar{x}=7.335\\) \\(\\rightarrow\\) trimmed mean: \\(\\bar{x}_{t, 0.1}=5.6375\\)\n\nmedian and trimmed mean are less influenced by outliers and skewnes \\(\\rightarrow\\) more robust\nbut somewhat less efficient"
  },
  {
    "objectID": "slides/03-statparams.html#mode-modal-value",
    "href": "slides/03-statparams.html#mode-modal-value",
    "title": "03-Statistical Parameters",
    "section": "Mode (modal value)",
    "text": "Mode (modal value)\n\n\nmost frequent value of a sample\nstrict definition only valid for discrete (binary, nominal, ordinal) scales\nextension to continuous scale: binning or density estimation\n\nFirst guess: middle of most-frequent class."
  },
  {
    "objectID": "slides/03-statparams.html#mode-weighting-formula",
    "href": "slides/03-statparams.html#mode-weighting-formula",
    "title": "03-Statistical Parameters",
    "section": "Mode: weighting formula",
    "text": "Mode: weighting formula\n\n\\[\\begin{align}\n   D &= x_{lo}+\\frac{f_k-f_{k-1}}{2f_k-f_{k-1}-f_{k+1}}\\cdot w \\\\\n   D &= 18 + \\frac{29 - 15}{2 \\cdot 29 - 15 - 26} \\cdot 2 = 19.65\n\\end{align}\\]\n\\(f\\): class frequency, \\(w\\): class width\n\\(k\\): the index of the most abundant class, \\(x_{lo}\\) its lower limit."
  },
  {
    "objectID": "slides/03-statparams.html#mode-density-estimation",
    "href": "slides/03-statparams.html#mode-density-estimation",
    "title": "03-Statistical Parameters",
    "section": "Mode: density estimation",
    "text": "Mode: density estimation\n\nSomewhat more computer intensive, where the mode is the maximum of a kernel density estimate.\nThe mode from the density estimate is then \\(D=19.42\\)."
  },
  {
    "objectID": "slides/03-statparams.html#multi-modal-distribution",
    "href": "slides/03-statparams.html#multi-modal-distribution",
    "title": "03-Statistical Parameters",
    "section": "Multi-modal distribution",
    "text": "Multi-modal distribution\n\nExample: fish population with several age classes, cohorts)"
  },
  {
    "objectID": "slides/03-statparams.html#measures-of-variation",
    "href": "slides/03-statparams.html#measures-of-variation",
    "title": "03-Statistical Parameters",
    "section": "Measures of variation",
    "text": "Measures of variation\nVariance\n\\[\n  s^2_x = \\frac{SQ}{df}=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\n\\]\n\n\\(SQ\\): sum of squared differences from the mean \\(\\bar{x}\\)\n\\(df = n-1\\): degrees of freedom, \\(n\\): sample size\n\nStandard deviation\n\\[s=\\sqrt{s^2}\\] \\(\\rightarrow\\) same unit as the mean \\(\\bar{x}\\), so they can be directly compared.\n\n\nIn practice, \\(s^2\\) is often computed with:\n\\[\n  s^2_x = \\frac{\\sum{(x_i)^2}-(\\sum{x_i})^2/n}{n-1}\n\\]"
  },
  {
    "objectID": "slides/03-statparams.html#coefficient-of-variation-cv",
    "href": "slides/03-statparams.html#coefficient-of-variation-cv",
    "title": "03-Statistical Parameters",
    "section": "Coefficient of variation (\\(cv\\))",
    "text": "Coefficient of variation (\\(cv\\))\nIs the relative standard deviation:\n\n\\[\n  cv=\\frac{s}{\\bar{x}}\n\\]\n\n\nuseful to compare variations of different variables, independent of their measurement unit\nonly applicable for data with ratio scale, i.e. with an absolute zero (like meters)\nnot for variables like Celsius temperature or pH.\n\nExample\nLet’s assume we have the discharge of two rivers, one with a \\(cv=0.3\\), another one with \\(cv=0.8\\). We see that the 2nd has more extreme variation."
  },
  {
    "objectID": "slides/03-statparams.html#range",
    "href": "slides/03-statparams.html#range",
    "title": "03-Statistical Parameters",
    "section": "Range",
    "text": "Range\nThe range measures the difference between maximum and minimum of a sample:\n\n\\[\n  r_x = x_{max}-x_{min}\n\\]\n\nDisadvantage: very sensitive against outliers."
  },
  {
    "objectID": "slides/03-statparams.html#interquartile-range",
    "href": "slides/03-statparams.html#interquartile-range",
    "title": "03-Statistical Parameters",
    "section": "Interquartile range",
    "text": "Interquartile range\n\n\nIQR or \\(I_{50}\\) omits smallest and biggest 25%\nsample size of at least 12 values recommended\n\n\\[\n  I_{50}=Q_3-Q_1=P_{75}-P_{25}\n\\]\nOrdered sample\n\n\\(Q_1\\), \\(Q_3\\): 1st and 3rd quartiles\n\\(P_{25}, P_{75}\\): 25th and 75th percentile\ntypically used in boxplots\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor normally distributed samples, fixed relationship between \\(I_{50}\\) and \\(s\\):\n\\[\n  \\sigma = E(I_{50}/(2\\Phi^{-1}(3/4))) \\approx E(I_{50}/1.394) % 2*qnorm(3/4))\n\\]\nwhere \\(\\Phi^{-1}\\) is the quantile function of the normal distribution."
  },
  {
    "objectID": "slides/03-statparams.html#median-absolute-deviation",
    "href": "slides/03-statparams.html#median-absolute-deviation",
    "title": "03-Statistical Parameters",
    "section": "Median absolute deviation",
    "text": "Median absolute deviation\n\nThe median of the absolute differences between median and values.\n\\[\n  MAD = \\text{median}(|\\text{median} - x_i|)\n\\]\n\nfrequently used in some communities, rarely used in our field\nsome programs rescale the MAD with a factor \\(1.4826\\) to approximate the standard deviation.\n\n\\(\\rightarrow\\) Be careful and check the software docs!"
  },
  {
    "objectID": "slides/03-statparams.html#standard-error-of-the-mean",
    "href": "slides/03-statparams.html#standard-error-of-the-mean",
    "title": "03-Statistical Parameters",
    "section": "Standard error of the mean",
    "text": "Standard error of the mean\n\n\\[\n  s_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\n\\]\n\n\nmeasures the accuracy of the mean\nplays a central role for calculation of confidence intervals and statistical tests\n\nRule of thumb for a sample size of about \\(n > 30\\):\n\n“Two sigma rule”: the true mean is with 95% in the range of \\(\\bar{x} \\pm 2 s_\\bar{x}\\)"
  },
  {
    "objectID": "slides/04-distributions.html#distributions",
    "href": "slides/04-distributions.html#distributions",
    "title": "04-Distributions",
    "section": "Distributions",
    "text": "Distributions\n\nProbability distribution\n\nmathematical function\nprobabilities of occurrence of different possible outcomes for an experiment\n\n\\(\\rightarrow\\) https://en.wikipedia.org/wiki/Probability_distribution\n\nCharacteristics of a probability distribution\n\na specific shape (distribution type, a mathematical formula)\ncan be described by its parameters (e.g. mean \\(\\mu\\) and standard deviation \\(\\sigma\\)).\n\nProbability distributions are one of the core concepts in statistics and many statistics courses start with coin tossing or dice rolls. We begin with a small classroom experiment.\n\n\nResearchers found out that coin flipping does not exactly result in a 50 : 50 chance. \\(\\rightarrow\\) youtube video"
  },
  {
    "objectID": "slides/04-distributions.html#what-is-your-favorite-number",
    "href": "slides/04-distributions.html#what-is-your-favorite-number",
    "title": "04-Distributions",
    "section": "What is your favorite number?",
    "text": "What is your favorite number?\nIn a classroom experiment, students of an international course were asked for their favorite number from 1 to 9.\n\n\n\n\n\nnumber\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nfreqency\n1\n2\n8\n1\n2\n2\n20\n2\n4\n\n\n\n\n\n\nThe resulting distribution is:\n\nempirical: data from an experiment\ndiscrete: only discrete numbers (1, 2, 3 …, 9) possible, no fractions"
  },
  {
    "objectID": "slides/04-distributions.html#computer-simulations-with-random-numbers",
    "href": "slides/04-distributions.html#computer-simulations-with-random-numbers",
    "title": "04-Distributions",
    "section": "Computer simulations with random numbers",
    "text": "Computer simulations with random numbers\n\nInstead of real-world experiments, we use often computer simulations.\n\ndisadvantage: somewhat abstract\nadvantage: we can simulate data from distributions with known properties\n\nPurpose\n\nget a feeling about randomness, how a sample from a given “theory” can look like\nexplore and test statistical methods and to train understanding\na tool for experimental design\ntesting application and power of an analysis beforehand"
  },
  {
    "objectID": "slides/04-distributions.html#continuos-uniform-distribution-mathbfu0-1",
    "href": "slides/04-distributions.html#continuos-uniform-distribution-mathbfu0-1",
    "title": "04-Distributions",
    "section": "Continuos uniform distribution \\(\\mathbf{U}(0, 1)\\)",
    "text": "Continuos uniform distribution \\(\\mathbf{U}(0, 1)\\)\n\n\nsame probability of occurence in a given interval\ne.g. \\((0, 1)\\)\nin R: runif, random, uniform\n\n\nrunif(10)\n\n [1] 0.9848991 0.1434716 0.0952776 0.6742983 0.3389204 0.0207042 0.5973770\n [8] 0.9880765 0.4740885 0.1684444\n\n\n\n\n\n \n\nbinning: subdivide values into classes"
  },
  {
    "objectID": "slides/04-distributions.html#density-function-of-mathbfux_min-x_max",
    "href": "slides/04-distributions.html#density-function-of-mathbfux_min-x_max",
    "title": "04-Distributions",
    "section": "Density function of \\(\\mathbf{U}(x_{min}, x_{max})\\)",
    "text": "Density function of \\(\\mathbf{U}(x_{min}, x_{max})\\)\n\n\ndensity \\(f(X)\\), sometimes abbreviated as “PDF” (probability density function):\n\n\\[\nf(x) = \\begin{cases}\n         \\frac{1}{x_{max}-x_{min}} & \\text{for } x \\in [x_{min},x_{max}] \\\\\n         0                     & \\text{otherwise}\n       \\end{cases}\n\\]\n\narea under the curve (i.e. the integral) = 1.0\n100% of the events are between \\(-\\infty\\) and \\(+\\infty\\) and for \\(\\mathbf{U}(x_{min}, x_{max})\\)in the interval \\((x_{min}, y_{max})\\)"
  },
  {
    "objectID": "slides/04-distributions.html#cumulative-distribution-function-of-mathbfux_min-x_max",
    "href": "slides/04-distributions.html#cumulative-distribution-function-of-mathbfux_min-x_max",
    "title": "04-Distributions",
    "section": "Cumulative distribution function of \\(\\mathbf{U}(x_{min}, x_{max}\\)",
    "text": "Cumulative distribution function of \\(\\mathbf{U}(x_{min}, x_{max}\\)\n\n\nIn general: The CDF is the integral of the density function:\n\\[\nF(x) =\\int_{-\\infty}^{x} f(x) dx\n\\] The total area (total probability) is \\(1.0\\):\n\\[\nF(x) =\\int_{-\\infty}^{+\\infty} f(x) dx = 1\n\\]\nFor the uniform distribution, it is:\n\\[\nF(x) = \\begin{cases}\n         0                     & \\text{for } x < x_{min} \\\\\n         \\frac{x-x_{min}}{x_{max}-x_{min}} & \\text{for } x \\in [x_{min},x_{max}] \\\\\n         1                     & \\text{for } x > x_{max}\n       \\end{cases}\n\\]"
  },
  {
    "objectID": "slides/04-distributions.html#quantile-function",
    "href": "slides/04-distributions.html#quantile-function",
    "title": "04-Distributions",
    "section": "Quantile function",
    "text": "Quantile function\n\n… the inverse of the cumulative distribution function.\n\n\n\n\n\n\nCumulative density function\n\n\n\n\n\n\n\nQuantile function\n\nExample: In which range can we find 95% of a uniform distribution \\(\\mathbf{U}(40,60)\\)?"
  },
  {
    "objectID": "slides/04-distributions.html#summary-uniform-distribution",
    "href": "slides/04-distributions.html#summary-uniform-distribution",
    "title": "04-Distributions",
    "section": "Summary: Uniform distribution",
    "text": "Summary: Uniform distribution"
  },
  {
    "objectID": "slides/04-distributions.html#the-normal-distribution-mathbfnmu-sigma",
    "href": "slides/04-distributions.html#the-normal-distribution-mathbfnmu-sigma",
    "title": "04-Distributions",
    "section": "The normal distribution \\(\\mathbf{N}(\\mu, \\sigma)\\)",
    "text": "The normal distribution \\(\\mathbf{N}(\\mu, \\sigma)\\)\n\nof high theoretical importance due to the central limit theorem (CLT)\nresults from adding a large number of random values of same order of magnitude.\n\nThe density function of the normal distribution is mathematically beautiful.\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\, \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\]\n\nC.F. Gauss, Gauss curve and formula on a German DM banknote from 1991–2001 (Wikipedia, CC0)"
  },
  {
    "objectID": "slides/04-distributions.html#random-numbers-and-density-function",
    "href": "slides/04-distributions.html#random-numbers-and-density-function",
    "title": "04-Distributions",
    "section": "Random numbers and density function",
    "text": "Random numbers and density function"
  },
  {
    "objectID": "slides/04-distributions.html#density-and-quantiles-of-the-standard-normal-distribution",
    "href": "slides/04-distributions.html#density-and-quantiles-of-the-standard-normal-distribution",
    "title": "04-Distributions",
    "section": "Density and quantiles of the standard normal distribution",
    "text": "Density and quantiles of the standard normal distribution\n\n\nin theory, 50% of the values are below and 50% above the mean value\n95% are between \\(\\pm 2 \\sigma\\)"
  },
  {
    "objectID": "slides/04-distributions.html#cumulative-distribution-function-quantile-function",
    "href": "slides/04-distributions.html#cumulative-distribution-function-quantile-function",
    "title": "04-Distributions",
    "section": "Cumulative distribution function – Quantile function",
    "text": "Cumulative distribution function – Quantile function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantile\n1\n1.64\n1.96\n2.0\n2.33\n2.57\n3\n\\(\\mu \\pm z\\cdot \\sigma\\)\n\n\n\n\none-sided\n\n0.95\n0.975\n0.977\n0.99\n0.995\n0.9986\n\\(1-\\alpha\\)\n\n\ntwo-sided\n0.68\n0.90\n0.95\n0.955\n0.98\n0.99\n0.997\n\\(1-\\alpha/2\\)"
  },
  {
    "objectID": "slides/04-distributions.html#standard-normal-scaling-and-shifting",
    "href": "slides/04-distributions.html#standard-normal-scaling-and-shifting",
    "title": "04-Distributions",
    "section": "Standard normal, scaling and shifting",
    "text": "Standard normal, scaling and shifting\n\n\n\\(\\mu\\) is the shift parameter that moves the whole bell shaped curve along the \\(x\\) axis\n\\(\\sigma\\) is the scale parameter to stretch or compress in the direction of \\(x\\)"
  },
  {
    "objectID": "slides/04-distributions.html#standardization-z-transformation",
    "href": "slides/04-distributions.html#standardization-z-transformation",
    "title": "04-Distributions",
    "section": "Standardization (\\(z\\)-transformation)",
    "text": "Standardization (\\(z\\)-transformation)\n\nAny normal distribution can be shifted scaled to form a standard normal with \\(\\mu=0, \\sigma=1\\)\n\n\nNormal distribution\n\n\n\n\n\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\, \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\]\n\n\n \\[\nz = \\frac{x-\\mu}{\\sigma}\n\\] \\(\\longrightarrow\\) \\(\\longrightarrow\\) \\(\\longrightarrow\\)\n\n\nStandard normal distribution\n\n\n\n\n\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}} \\, \\mathrm{e}^{-\\frac{1}{2}x^2}\n\\]"
  },
  {
    "objectID": "slides/04-distributions.html#t-distribution-mathbftx-df",
    "href": "slides/04-distributions.html#t-distribution-mathbftx-df",
    "title": "04-Distributions",
    "section": "t-Distribution \\(\\mathbf{t}(x, df)\\)",
    "text": "t-Distribution \\(\\mathbf{t}(x, df)\\)\n\n\nadditional parameter “degrees of freedom” (df); used for statistical tests\nconverges to the normal distribution for \\(df \\rightarrow \\infty\\)"
  },
  {
    "objectID": "slides/04-distributions.html#dependency-of-the-t-value-on-the-number-of-df",
    "href": "slides/04-distributions.html#dependency-of-the-t-value-on-the-number-of-df",
    "title": "04-Distributions",
    "section": "Dependency of the t-value on the number of df",
    "text": "Dependency of the t-value on the number of df\n\n\n\n\n\n\ndf\n1.00\n4.00\n9.00\n19.00\n29.00\n99.00\n999.00\n\n\nt\n12.71\n2.78\n2.26\n2.09\n2.05\n1.98\n1.96"
  },
  {
    "objectID": "slides/04-distributions.html#logarithmic-normal-distribution-lognormal",
    "href": "slides/04-distributions.html#logarithmic-normal-distribution-lognormal",
    "title": "04-Distributions",
    "section": "Logarithmic normal distribution (lognormal)",
    "text": "Logarithmic normal distribution (lognormal)\n\n\n\n\n\nmany processes in nature do not follow a normal distribution\nlimited by zero on the left side\nlarge extreme values on the right side\n\nExamples: discharge of rivers, nutrient concentrations, algae biomass in a lakes"
  },
  {
    "objectID": "slides/04-distributions.html#parent-distribution-of-the-lognormal",
    "href": "slides/04-distributions.html#parent-distribution-of-the-lognormal",
    "title": "04-Distributions",
    "section": "Parent distribution of the lognormal",
    "text": "Parent distribution of the lognormal\n\n\nlog from values of a lognormal distribution \\(\\rightarrow\\) normal parent distribution.\nlognormal distribution is described by parameters of log-transformed data \\(\\bar{x}_L\\) and \\(s_L\\)\nthe the antilog of \\(\\bar{x}_L\\) is the geometric mean"
  },
  {
    "objectID": "slides/04-distributions.html#binomial-distribution",
    "href": "slides/04-distributions.html#binomial-distribution",
    "title": "04-Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\n\nnumber of successful trials out of \\(n\\) total trials with success probability \\(p\\).\nHow many “6” with probability \\(1/6\\) in 3 trials?\nmedicine, toxicology, comparison of percent numbers\nsimilar, but without replacement: hypergeometric distribution in lottery"
  },
  {
    "objectID": "slides/04-distributions.html#poisson-distribution",
    "href": "slides/04-distributions.html#poisson-distribution",
    "title": "04-Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\n\ndistribution of rare events, a discrete distribution\nmean and variance are equal (\\(\\mu = \\sigma^2\\)), resulting parameter “Lamda” (\\(\\lambda\\))\n\nExamples: bacteria counting on a grid, waiting queues, failure models\nQuasi-poisson if \\(\\mu \\neq \\sigma^2\\) * If \\(s^2 > \\bar{x}\\): overdispersion * if \\(s^2 < \\bar{x}\\): underdispersion"
  },
  {
    "objectID": "slides/04-distributions.html#confidence-interval",
    "href": "slides/04-distributions.html#confidence-interval",
    "title": "04-Distributions",
    "section": "Confidence interval",
    "text": "Confidence interval\n– depends only on \\(\\lambda\\) resp. the number of counted units (\\(k\\))\n\nTypical error of cell counting: 95% confidence interval\n\n\n\n\n\ncounts\n2\n3\n5\n10\n50\n100\n200\n400\n1000\n\n\nlower\n0\n1\n2\n5\n37\n81\n173\n362\n939\n\n\nupper\n7\n9\n12\n18\n66\n122\n230\n441\n1064"
  },
  {
    "objectID": "slides/04-distributions.html#testing-for-distribution",
    "href": "slides/04-distributions.html#testing-for-distribution",
    "title": "04-Distributions",
    "section": "Testing for distribution",
    "text": "Testing for distribution\n\nSometimes we want to know whether a data set belongs to a specific type of distribution. Though this sounds quite easy, it appears quite difficult for theoretical reasons:\n\nstatistical tests check for deviations from the null hypothesis\nbut here we want to test the opposite, if \\(H_0\\) is true\n\nThis is in fact impossible, because “not significant” means only that a potential effect is either not existent or just too small to be detected. On the opposite, “significantly different” includes a certain probability of false positives.\nHowever, most statistical tests do not require perfect agreement with a certain distribution:\n\nt-test and ANOVA assume normality of residuals\ndue to the CLT, the distribution of sums and mean values converges to normal"
  },
  {
    "objectID": "slides/04-distributions.html#shapiro-wilks-w-test",
    "href": "slides/04-distributions.html#shapiro-wilks-w-test",
    "title": "04-Distributions",
    "section": "Shapiro-Wilks-W-Test",
    "text": "Shapiro-Wilks-W-Test\n\\(\\rightarrow\\) standard test for normality testing\n\nx <- rnorm(100)\nshapiro.test(x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99064, p-value = 0.7165\n\n\n\n– the \\(p\\)-value is greater than 0.05, so we would keep \\(H_0\\) and conclude that nothing speaks against acceptance of the normal\nInterpration of the Shapiro-Wilks-test needs to be done with care:\n\nfor small \\(n\\), the tes is not sensitive enough\nfar large \\(n\\) it is over-sensitive\n\n\n\nThe \\(\\chi^2\\) (Chi-squared) or Kolmogorov-Smirnov-tests are nowadays rarely used for this purpose, but still important for other test problems."
  },
  {
    "objectID": "slides/04-distributions.html#graphical-examination-of-normality",
    "href": "slides/04-distributions.html#graphical-examination-of-normality",
    "title": "04-Distributions",
    "section": "Graphical examination of normality",
    "text": "Graphical examination of normality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\): theoretical quantiles where a value should be found if the distribution is normal\n\\(y\\): normalized and ordered measured values (\\(z\\)-scores)\nscaled in the unit of standard deviations\nnormal distribution if the points follow a straight line"
  },
  {
    "objectID": "slides/04-distributions.html#transformation",
    "href": "slides/04-distributions.html#transformation",
    "title": "04-Distributions",
    "section": "Transformation",
    "text": "Transformation\n\nallows to apply methods designed for normally distributed data for non-normal cases\nvery common in the in the past, still sometimes useful\nmodern methods can handle other distributions directly, such as binomial, gamma, or Poisson.\n\nTransformations for right-skewed data\n\n$x’=(x)\nx’=(x+a)$\n\\(x'=(x+a)^c\\) (\\(a\\) between 0.5 and 1)\n\\(x'=1/x\\) (“very powerful”, i.e. to extreme in most cases)\n\\(x'=a - 1/\\sqrt{x}\\) (to make scale more convenient)\n\\(x'=1/\\sqrt{x}\\) (compromise between \\(\\ln\\) and \\(1/x\\))\n\\(x'=a+bx^c\\) (very general, includes powers and roots)"
  },
  {
    "objectID": "slides/04-distributions.html#transformations-ii",
    "href": "slides/04-distributions.html#transformations-ii",
    "title": "04-Distributions",
    "section": "Transformations II",
    "text": "Transformations II\nTransformations for count data\n\n\\(x'=\\sqrt{3/8+x}\\) (counts: 1, 2, 3 \\(\\rightarrow\\) 0.61, 1.17, 1.54, 1.84, )\n\\(x'=\\lg(x+3/8)\\)\n\\(x'=\\ln(\\ln(x))\\) for giant numbers\n\n\\(\\rightarrow\\) consider a GLM with family Poisson or quasipoisson instead\nRatios and percentages\n\n\\(x'=\\arcsin \\sqrt{x/n}\\)\n\\(x'=\\arcsin \\sqrt{\\frac{x+3/8}{n+3/4}}\\)\n\n\\(\\rightarrow\\) consider a GLM with family binomial instead"
  },
  {
    "objectID": "slides/04-distributions.html#rank-transformation",
    "href": "slides/04-distributions.html#rank-transformation",
    "title": "04-Distributions",
    "section": "Rank transformation",
    "text": "Rank transformation\nExample: Spearman correlation\n\n Data set\n\nx <- c(1, 2, 3, 5, 4, 5 ,6,  7)\ny <- c(1, 2, 4, 3, 4, 6, 8, 20)\n\nRanks\n\nrank(x)\n\n[1] 1.0 2.0 3.0 5.5 4.0 5.5 7.0 8.0\n\nrank(y)\n\n[1] 1.0 2.0 4.5 3.0 4.5 6.0 7.0 8.0\n\n\nTwo ways of calculation\n\ncor(x, y, method = \"spearman\")\n\n[1] 0.8915663\n\ncor(rank(x), rank(y))\n\n[1] 0.8915663"
  },
  {
    "objectID": "slides/04-distributions.html#the-central-limit-theorem-clt",
    "href": "slides/04-distributions.html#the-central-limit-theorem-clt",
    "title": "04-Distributions",
    "section": "The central limit theorem (CLT)",
    "text": "The central limit theorem (CLT)\n\nSums of a large number \\(n\\) of independent and identically distributed random values are normally distributed, independently on the type of the original distribution.\n\n\nwe can use methods assuming normal normal distribution for non-normal data\n\nif we have a large data set\nif the original distribution is not “too skewed”\n\nrequired number \\(n\\) depends on the skewness of the original distribution\n\n\n\nReason: Methods like t-test or ANOVA are based on mean values."
  },
  {
    "objectID": "slides/04-distributions.html#a-simulation-experiment",
    "href": "slides/04-distributions.html#a-simulation-experiment",
    "title": "04-Distributions",
    "section": "A Simulation experiment",
    "text": "A Simulation experiment\n\n\n\ngenerate a matrix with 100 rows and 25 columns of uniformly distributed random numbers\ncompute the row sums\n\n\npar(mfrow=c(2, 1), las=1)\nset.seed(42)\nx  <- matrix(runif(25 * 100), nrow = 25)\n\n# View(x) # uncomment this to show the matrix\n\nxs <- colSums(x)\nhist(x)\nhist(xs)\n\n\\(\\rightarrow\\) row sums are approximately normal distributed"
  },
  {
    "objectID": "slides/04-distributions.html#sample-intervals-and-confidence-intervals",
    "href": "slides/04-distributions.html#sample-intervals-and-confidence-intervals",
    "title": "04-Distributions",
    "section": "Sample intervals and Confidence intervals",
    "text": "Sample intervals and Confidence intervals\n\n\nsample interval: characterizes the distribution of the data from the parameters of the sample (e.g. mean, standard deviation)\nstandard deviation \\(s_x\\) measures the variability of the original data\nreconstruct the original distribution if its type is known (e.g. normal, lognormal)\n\n\n\nconfidence interval: characterizes the precision of a statistical parameter, based on its standard error\nUsing \\(\\bar{x}\\) and \\(s_\\bar{x}\\), estimate the interval where we find \\(\\mu\\) with a certain probability\nless dependent on the original distribution of the data due to the CLT"
  },
  {
    "objectID": "slides/04-distributions.html#confidence-intervals-of-the-mean",
    "href": "slides/04-distributions.html#confidence-intervals-of-the-mean",
    "title": "04-Distributions",
    "section": "Confidence intervals of the mean",
    "text": "Confidence intervals of the mean\n\nStandard error\n\\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n\\]\n\\(\\rightarrow\\) variability of the mean is half, if we increase the sample size four times (\\(2^2\\))\nFurthermore, it is possible to estimate the interval in which the true mean is found with 95,% probability, that is the 95,% confidence interval:\nEstimation of the 95% confidence interval:\n\\[\nCI_{95\\%} = \\bigg(\\bar{x} - z_{0.975} \\cdot \\frac{s}{\\sqrt{n}},\n                 \\bar{x} + z_{0.975} \\cdot \\frac{s}{\\sqrt{n}}\\bigg)\n\\]\nwith \\(z_{1-\\alpha/2} = z_{0.975} =\\) \\(1.96\\).\n\n\\(\\rightarrow\\) \\(2\\sigma\\) rule"
  },
  {
    "objectID": "slides/04-distributions.html#use-the-t-distribution-for-small-samples",
    "href": "slides/04-distributions.html#use-the-t-distribution-for-small-samples",
    "title": "04-Distributions",
    "section": "Use the t-distribution for small samples",
    "text": "Use the t-distribution for small samples\n\\[\nCI_{95\\%} = \\bigg(\\bar{x} - t_{0.975, n-1} \\cdot \\frac{s}{\\sqrt{n}},\n                 \\bar{x} + t_{0.975, n-1} \\cdot \\frac{s}{\\sqrt{n}}\\bigg)\n\\]\n\nnecessary for small samples: \\(n\\lessapprox 30\\), \\(n-1\\) degrees of freedom\ncan also be used for \\(n>30\\)\n\\(t\\)-quantile can be found in tables or calculated with the qt()function in R.\n\nExample with \\(\\mu=50\\) and \\(\\sigma=10\\):\n\nset.seed(123)\nn <- 10\nx <- rnorm(n, 50, 10)\nm <- mean(x); s <- sd(x)\nse <- s/sqrt(n)\n# lower and upper confidence limits\nm + qt(c(0.025, 0.975), n-1) * se\n\n[1] 43.92330 57.56922\n\n\nThe true mean (\\(\\mu\\)=50) is in the interval CI = (43.9, 57.6)."
  },
  {
    "objectID": "slides/04-distributions.html#outliers",
    "href": "slides/04-distributions.html#outliers",
    "title": "04-Distributions",
    "section": "Outliers",
    "text": "Outliers\n\nextremely large or extremely small values are sometimes called “outliers”\nbut, potential outliers can be “extreme values” from a skewed distribution. Excluding them, can be scientific misconduct.\na “true” outlier is a value that is not from the population we want to analyze, e.g. a serious measurement error if someone forgot to add a chemical in an analysis.\nit can also be something interesting, e.g. the result of new phenomenon\n\n\\(\\Rightarrow\\) It can be wrong to exclude values only because they are “too big” or “too small”.\n\\(\\rightarrow\\) Try to find the reason, why values are extreme!\n\n\\(4 \\sigma\\)-rule\n\ncheck if a value is more that 4 standard deviations away from the mean value.\nsample size should be \\(n \\ge 10\\), \\(\\bar{x}\\) and \\(s\\) are calculated without the potential outlier.\nsimilar “rules of thumb” can be found in statistics textbooks."
  },
  {
    "objectID": "slides/04-distributions.html#outlier-test-for-linear-models-with-bonferroni-correction",
    "href": "slides/04-distributions.html#outlier-test-for-linear-models-with-bonferroni-correction",
    "title": "04-Distributions",
    "section": "Outlier test for linear models with Bonferroni correction",
    "text": "Outlier test for linear models with Bonferroni correction\n\nFor linear models and GLMs we can use the Bonferroni outlier test from package car.\n\n\nlibrary(car)\nx <- c(rnorm(20), 12) # the 21st value (=12) is an outlier\noutlierTest(lm(x~1))  # x ~ 1 is the null model\n\n   rstudent unadjusted p-value Bonferroni p\n21 11.66351         4.1822e-10   8.7826e-09\n\n\n\\(\\rightarrow\\) The 21st value is identified as an outlier:\n Alternative to outlier tests\n\nuse robust parameters and methods,\n\ne.g. median or trimmed mean instead of the arithmetic mean,\nrobust linear regression rlm instead of lm\nrank-based methods like Spearman correlation\n\nImportant outliers may be omitted in an analysis, but the the number and extent of outliers must be mentioned!"
  },
  {
    "objectID": "slides/04-distributions.html#extreme-values-in-boxplots",
    "href": "slides/04-distributions.html#extreme-values-in-boxplots",
    "title": "04-Distributions",
    "section": "Extreme values in boxplots",
    "text": "Extreme values in boxplots\n\n\nextreme values outside the whiskers if more than 1.5 times distant from the box limits, compared to the width of the interquartile box.\nsometimes called “outliers”.\nI prefer the term “extreme value”, because they can be regular observations from a skewed or heavy tailed distribution."
  },
  {
    "objectID": "slides/04-distributions.html#example",
    "href": "slides/04-distributions.html#example",
    "title": "04-Distributions",
    "section": "Example",
    "text": "Example\n\npar(mfrow=c(1, 3), las=1)\nelbe <- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/elbe.csv\")\ndischarge <- elbe$discharge\nboxplot(discharge, main=\"Boxplot of discharge\")\nhist(discharge)\nhist(log(discharge - 70))\n\n\nDischarge data of the Elbe River in Dresden in \\(\\mathrm m^3 s^{-1}\\), data source: Bundesanstalt für Gewässerkunde (BFG), see terms and conditions.\n\nleft: large number of extreme values, are these outliers?\nmiddle: distribution is right-skewed\nright: transformation (3-parametric lognormal) \\(\\rightarrow\\) symmetric distribution, no outliers!"
  },
  {
    "objectID": "slides/05-classtests.html#statistical-test",
    "href": "slides/05-classtests.html#statistical-test",
    "title": "05-Classical Tests",
    "section": "Statistical test",
    "text": "Statistical test\n\nA statistical hypothesis test is a method of statistical inference.\n\nCommonly, two samples are compared, or a sample is compared against properties from an idealized model.\nA hypothesis \\(H_a\\) for the statistical relationship between the two data sets, is compared to an idealized null hypothesis H0 that proposes no relationship between two data sets.\nThe comparison is considered statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probability – the significance level.\n\nadapted from: https://en.wikipedia.org/wiki/Statistical hypothesis testing"
  },
  {
    "objectID": "slides/05-classtests.html#effect-size-and-significance",
    "href": "slides/05-classtests.html#effect-size-and-significance",
    "title": "05-Classical Tests",
    "section": "Effect size and significance",
    "text": "Effect size and significance\n\nIn case of relative mean differences, the relative effect size is:\n\n\\[\n  \\delta = \\frac{\\bar{\\mu}_1-\\bar{\\mu}_2}{\\sigma}=\\frac{\\Delta}{\\sigma}\n\\]\n\nwith:\n\nmean values of two populations \\(\\mu_1, \\mu_2\\)\neffect size \\(\\Delta\\)\nrelative effect size \\(\\delta\\) (also called Cohen’s d)\nsignificance means that an observed effect is unlikely the result of pure random variation."
  },
  {
    "objectID": "slides/05-classtests.html#null-hypothesis-and-alternative-hypothesis",
    "href": "slides/05-classtests.html#null-hypothesis-and-alternative-hypothesis",
    "title": "05-Classical Tests",
    "section": "Null hypothesis and alternative hypothesis",
    "text": "Null hypothesis and alternative hypothesis\n\n\\(H_0\\) null hypothesis: two populations are not different with respect to a certain property.\n\nAssumption: observed effect occured purely at random, true effect is zero.\n\n\\(H_a\\) alternative hypothesis (experimental hypothesis): existence of a certain effect.\n\nAn alternative hypothesis is never completely true or “proven”.\nAcceptance of \\(H_A\\) means only than \\(H_0\\) is unlikely.\n\n“Not significant” means either no effect or sample size too small!\n\nNote: Different meaning of significance (H0 unlikely) and relevance (effect large enough to play a role in practice)."
  },
  {
    "objectID": "slides/05-classtests.html#the-p-value",
    "href": "slides/05-classtests.html#the-p-value",
    "title": "05-Classical Tests",
    "section": "The p-value",
    "text": "The p-value\n\nThe interpretation of the p-value was often confused in the past, even in statistics textbooks, so it is good to refer to a clear definition:\n\n\nThe p-value is defined as the probability of obtaining a result equal to or ‘more extreme’ than what was actually observed, when the null hypothesis is true.\n\n\n\n\nhttps://en.wikipedia.org/wiki/P-value:\nHubbard, R. (2004). Blurring the Distinctions Between p’s and a’s in Psychological Research, Theory Psychology June 2004 vol. 14 no. 3 295-327. DOI: 10.1177/0959354304043638"
  },
  {
    "objectID": "slides/05-classtests.html#alpha-and-beta-errors",
    "href": "slides/05-classtests.html#alpha-and-beta-errors",
    "title": "05-Classical Tests",
    "section": "Alpha and beta errors",
    "text": "Alpha and beta errors\n\n\n\n\n\n\n\n\n\nReality\nDecision of the test\ncorrect?\nprobability\n\n\n\n\n\\(H_0\\) = true\nnot significant\nyes\n\\(1-\\alpha\\)\n\n\n\\(H_0\\) = false\nsignificant\nyes\n\\(1-\\beta\\) (power)\n\n\n\\(H_0\\) = true\nsignificant\nno\n\\(\\alpha\\)-error\n\n\n\\(H_0\\) = false\nnot significant\nno\n\\(\\beta\\)-error\n\n\n\n\n\n\n\n\n\n1.\\(H_0\\) falsely rejected (error of the first kind or \\(\\alpha\\)-error)\n\nwe claim an effect, that does not exist, e.g. a drug with no effect\n\n2.\\(H_0\\) falsely retained (error of the second kind or \\(\\beta\\)-error)\n\ntypical case in small studies, where effert was not enough to detect existing effects\n\nUse in practice\n\ncommon convention in environmental sciences: \\(\\alpha=0.05\\), must be set beforehand\n\\(\\beta=f(\\alpha, \\text{effectsize}, \\text{sample size}, \\text{kind of test})\\) (should be \\(\\ge 0.2\\))"
  },
  {
    "objectID": "slides/05-classtests.html#significance-and-relevance",
    "href": "slides/05-classtests.html#significance-and-relevance",
    "title": "05-Classical Tests",
    "section": "Significance and relevance",
    "text": "Significance and relevance\n\nSignificance is not the only important. It is often more important to focus on effect size and relevance.\nWhen evaluating significant effects, it is always necessary to look also at the effect size. While significance means that the null hypothesis \\(H_0\\) is unlikely in a statistical sense, relevance means that the effect size is large enough to play a role in practice. This means that whether an effect can be relevant or not depends on its effect size and the field of application.\nLet’s for example consider a vaccination. If a vaccine had a significant effect in a clinical test, but protected only 10 out of 1000 people, one would not consider this effect as relevant and not produce this vaccine.\nOn the other hand, even small effects can be relevant. So if a toxic substance would have an effect on 1 out of 1000 people to produce cancer, we would consider this as relevant. To detect this as a significant effect would need an epidemiological study with a large number of people. But as it is highly relevant, it is worth the effort."
  },
  {
    "objectID": "slides/05-classtests.html#take-home-messages",
    "href": "slides/05-classtests.html#take-home-messages",
    "title": "05-Classical Tests",
    "section": "Take home messages",
    "text": "Take home messages\n\n\nA p-value measures the probability that a purely random effect would be equally or more extreme than an observed effect.\n“Significant” means that an observed result is unlikely to have occurred by chance alone.\n“Not significant” means either “no effect” or “sample size too small”.\nDon’t focus on p-values alone. Never forget to report also sample size, effect size and relevance of your results.\nWith large data sets, the p-value loses importance. It is then easy to find significant effects, but it is often very small and not relevant in practice.\nThe p-value is an important tool in classical statistics, but its abuse can lead to mis-interpretation."
  },
  {
    "objectID": "slides/05-classtests.html#one-sample-t-test",
    "href": "slides/05-classtests.html#one-sample-t-test",
    "title": "05-Classical Tests",
    "section": "One sample t-Test",
    "text": "One sample t-Test\n\ntests if a sample is from a population with given mean value \\(\\mu\\)\nbased on checking if the population mean \\(\\mu\\) is in the confidence interval of \\(\\bar{x}\\)\n\n\nLet’s assume a sample of size with \\(n=10, \\bar{x}=5.5, s=1\\) and \\(\\mu=5\\).\nEstimate the 95% confidence interval of \\(\\bar{x}\\):\n\n\\[\nCI = \\bar{x} \\pm t_{1-\\alpha/2, n-1} \\cdot s_{\\bar{x}}\n\\] with \\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\qquad \\text{(standard error)}\n\\]\nDifferent ways of calculation shown at the next slides"
  },
  {
    "objectID": "slides/05-classtests.html#remember-standard-deviation-and-standard-error",
    "href": "slides/05-classtests.html#remember-standard-deviation-and-standard-error",
    "title": "05-Classical Tests",
    "section": "Remember: standard deviation and standard error",
    "text": "Remember: standard deviation and standard error\n\nFigure 1: Visualization of a one-sample t-test. Left: original distribution of the data measured by standard deviation, right: distribution of mean values, measured by its standard error.\\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\qquad \\text{(standard error)}\n\\]\n\nstandard error < standard deviation\nmeasures precision of the mean value\nCLT!\n\nThe test is based on the distribution of the means, not the original data."
  },
  {
    "objectID": "slides/05-classtests.html#method-1-is-mu-in-the-confidence-interval",
    "href": "slides/05-classtests.html#method-1-is-mu-in-the-confidence-interval",
    "title": "05-Classical Tests",
    "section": "Method 1: Is \\(\\mu\\) in the confidence interval?",
    "text": "Method 1: Is \\(\\mu\\) in the confidence interval?\n\n\nSample: \\(n=10, \\bar{x}=5.5, s=1\\) and \\(\\mu=5\\)\nLet \\(\\alpha = 0.05\\), we get a two-sided 95% confidence interval with:\n\n\\[\\bar{x} \\pm t_{0,975, n-1} \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\n5.5 + c(-1, 1) * qt(0.975, 10-1) * 1/sqrt(10)\n\n[1] 4.784643 6.215357\n\n\n\n\n\nCheck if \\(\\mu=5.0\\) is in this interval.\nYes, it is inside \\(\\Rightarrow\\) difference not significant."
  },
  {
    "objectID": "slides/05-classtests.html#method-2-comparison-with-a-tabulated-t-value",
    "href": "slides/05-classtests.html#method-2-comparison-with-a-tabulated-t-value",
    "title": "05-Classical Tests",
    "section": "Method 2: Comparison with a tabulated t-value",
    "text": "Method 2: Comparison with a tabulated t-value\n\nRearrange the equation of the confidence interval, to calculate an observed \\(t_{obs}\\)\n\n\\[\nt_{obs} = |\\bar{x}-\\mu | \\cdot \\frac{1}{s_{\\bar{x}}} = \\frac{|\\bar{x}-\\mu |}{s} \\cdot \\sqrt{n} = \\frac{|5.5 -5.0|}{1.0} \\cdot \\sqrt{10}\n\\]\nWe can calculate this in R:\n\nt <- abs(5.5 - 5.0) / 1.0 * sqrt(10)\nt\n\n[1] 1.581139\n\n\n\nCompare \\(t_{obs}\\) with a tabulated value\n\n\n“Old style”: find critical t-value in a table for given \\(\\alpha\\) and degrees of freedom (\\(n-1\\))\nFor \\(\\alpha=0.05\\) and two-sided, this is: \\(t_{1-\\alpha/2, n-1} = 2.26\\).\n\nComparison: \\(1.58 < 2.26\\) \\(\\Rightarrow\\) no significant difference between \\(\\bar{x}\\) and \\(\\mu\\)."
  },
  {
    "objectID": "slides/05-classtests.html#method-3-calculation-of-the-p-value-from-t_obs",
    "href": "slides/05-classtests.html#method-3-calculation-of-the-p-value-from-t_obs",
    "title": "05-Classical Tests",
    "section": "Method 3: Calculation of the p-value from \\(t_{obs}\\)",
    "text": "Method 3: Calculation of the p-value from \\(t_{obs}\\)\n\n\nuse computerized probability function (pt) instead of table lookup\n\\(t = t_{obs}\\) and the degrees of freedom (\\(n-1\\)):\n\n\n\n2 * (1 - pt(t, df = 10 - 1)) # 2 * (1 - p) is re-arranged from 1-alpha/2\n\n[1] 0.1483047\n\n\nThis p-value = 0.1483047 is greater than \\(0.05\\) so we consider the difference as not significant.\n\nFAQ: less than or greater than?\n\n\n\n\n\n\n\n\n\np-value\n\\(\\text{p-value} < \\alpha\\)\nnull hypothesis unlikely\nsignificant\n\n\ntest statistic\n\\(t_{obs} > t_{1-\\alpha/2, n-1}\\)\neffect exceeds Confint.\nsignificant"
  },
  {
    "objectID": "slides/05-classtests.html#method-4-built-in-t-test-function-in-r",
    "href": "slides/05-classtests.html#method-4-built-in-t-test-function-in-r",
    "title": "05-Classical Tests",
    "section": "Method 4: Built-in t-test function in R",
    "text": "Method 4: Built-in t-test function in R\nThe same can be done much easier with the computer in R.\nLet’s assume we have a sample with \\(\\bar{x}=5, s=1\\):\n\n## define sample\nx <- c(5.5, 3.5, 5.4, 5.3, 6, 7.2, 5.4, 6.3, 4.5, 5.9)\n\n## perform one-sample t-test\nt.test(x, mu=5)\n\n\n    One Sample t-test\n\ndata:  x\nt = 1.5811, df = 9, p-value = 0.1483\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 4.784643 6.215357\nsample estimates:\nmean of x \n      5.5 \n\n\nThe test returns the observed t-value, the 95% confidence interval and the p-value.\nAn important difference is, that this method needs the original data, while the other methods need only mean, standard deviation and sample size."
  },
  {
    "objectID": "slides/05-classtests.html#two-sample-t-test",
    "href": "slides/05-classtests.html#two-sample-t-test",
    "title": "05-Classical Tests",
    "section": "Two sample t-test",
    "text": "Two sample t-test\nThe two-sample t-test compares two independent samples:\n\nx1 <- c(5.3, 6.0, 7.1, 6.4, 5.7, 4.9, 5.0, 4.6, 5.7, 4.0, 4.5, 6.5)\nx2 <- c(5.8, 7.1, 5.8, 7.0, 6.7, 7.7, 9.2, 6.0, 7.2, 7.8, 7.8, 5.7)\nt.test(x1, x2)\n\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = -3.7185, df = 21.611, p-value = 0.001224\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.3504462 -0.6662205\nsample estimates:\nmean of x mean of y \n 5.475000  6.983333 \n\n\nThat means that both samples differ significantly (\\(p < 0.05\\)). It has to be mentioned that R did not perform the “ordinary” t-test but the Welch test (also termed heteroscedastic t-test), for which the variances of both samples are not required to be identical."
  },
  {
    "objectID": "slides/05-classtests.html#hypothesis-and-formula-of-the-two-sample-t-test",
    "href": "slides/05-classtests.html#hypothesis-and-formula-of-the-two-sample-t-test",
    "title": "05-Classical Tests",
    "section": "Hypothesis and formula of the two-sample t-test",
    "text": "Hypothesis and formula of the two-sample t-test\n\n\n\\(H_0\\) \\(\\mu_1 = mu_2\\)\n\\(H_a\\) the two means are different\ntest criterion\n\\[\nt_{obs} =\\frac{|\\bar{x}_1-\\bar{x}_2|}{s_{tot}} \\cdot \\sqrt{\\frac{n_1 n_2}{n_1+n_2}}\n\\]\n\n\n\n\n\n\n\n\npooled standard deviation\n\\[\ns_{tot} = \\sqrt{{({n}_1 - 1)\\cdot s_1^2 + ({n}_2 - 1)\\cdot s_2^2\n\\over ({n}_1 + {n}_2 - 2)}}\n\\]\nassumptions: independence, equal variances, normal distribution"
  },
  {
    "objectID": "slides/05-classtests.html#the-welch-test",
    "href": "slides/05-classtests.html#the-welch-test",
    "title": "05-Classical Tests",
    "section": "The Welch test",
    "text": "The Welch test\n\nKnown as t-test for samples with unequal variance, works also for equal variance!\nTest criterion:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{s^2_{\\bar{x}_1} + s^2_{\\bar{x}_2}}}\n\\]\nStandard error of each sample:\n\\[\ns_{\\bar{x}_i} = \\frac{s_i}{\\sqrt{n_i}}\n\\] Corrected degrees of freedom:\n\\[\n\\text{df} = \\frac{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}{\\frac{s^4_1}{n^2_1(n_1-1)} + \\frac{s^4_2}{n^2_2(n_2-1)}}\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#equality-of-variance-f-test",
    "href": "slides/05-classtests.html#equality-of-variance-f-test",
    "title": "05-Classical Tests",
    "section": "Equality of variance: F-test",
    "text": "Equality of variance: F-test\n\n\\(H_0\\): \\(\\sigma_1^2 = \\sigma_2^2\\)\n\\(H_a\\): variances unequal\nTest criterion:\n\\[F = \\frac{s_1^2}{s_2^2} \\]\n\nlarger of the two variances in the enumerator \\((s^2_1 > s^2_2)\\)\nseparate degrees of freedom (\\(n-1\\))\n\n\n\n\n\n\n\n\n\nExample\n\n\\(s_1=1\\), \\(s_2 =2\\), \\(n_1=5, n_2=10, F=\\frac{2^2}{1^2}=4\\)\ndeg. of freedom: \\(9 \\atop 4\\)\n\n\\(\\Rightarrow\\) \\(F_{9, 4, \\alpha=0.975} = 8.9 > 4 \\quad\\rightarrow\\) not significant"
  },
  {
    "objectID": "slides/05-classtests.html#homogeneity-of-variances-with-2-samples",
    "href": "slides/05-classtests.html#homogeneity-of-variances-with-2-samples",
    "title": "05-Classical Tests",
    "section": "Homogeneity of variances with > 2 samples",
    "text": "Homogeneity of variances with > 2 samples\n\n\n\n\n\n\n\n\nBartlett’s test:\n\nbartlett.test(list(x1, x2, x3))\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  list(x1, x2, x3)\nBartlett's K-squared = 7.7136, df = 2, p-value = 0.02114\n\n\n Fligner-Killeen test (recommended):\n\nfligner.test(list(x1, x2, x3))\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  list(x1, x2, x3)\nFligner-Killeen:med chi-squared = 2.2486, df = 2, p-value = 0.3249\n\n\n\n\n\n\n\n\n\n\n\ntests are often used to check assumptions of the ANOVA"
  },
  {
    "objectID": "slides/05-classtests.html#recommendation-for-two-sample-t-tests",
    "href": "slides/05-classtests.html#recommendation-for-two-sample-t-tests",
    "title": "05-Classical Tests",
    "section": "Recommendation for two sample t-tests",
    "text": "Recommendation for two sample t-tests\n\nTraditional procedure:\n\nTest for equal variances using the F-test: var.test(x, y)\nIf variances are equal: t.test(x, y, var.equal=TRUE)\notherwise, use t.test(x, y) (= Welch test)\nCheck if both samples follow a normal distribution.\n\n\nMore modern recommendation:\n\nDon’t use pre-tests\nUse always the Welch test: t.test(x, y)\nCheck approximate normal distribution by using box-plots. Not necessary if \\(n\\) is large.\n\nsee also: Wikipedia"
  },
  {
    "objectID": "slides/05-classtests.html#paired-t-test",
    "href": "slides/05-classtests.html#paired-t-test",
    "title": "05-Classical Tests",
    "section": "Paired t-Test",
    "text": "Paired t-Test\n\nsometimes also called “t-test of dependent samples”\n\nthe term “dependent” can be misleading, better “pairwise”\nvalues within samples must still be independent\n\nexamples: left arm / right arm; before / after\nis essentially a one-sample t-test of pairwise differences against \\(\\mu=0\\)\n\nadvantage: eliminates “covariate”\n\n\n\nx1 <- c(2, 3, 4, 5, 6)\nx2 <- c(3, 4, 7, 6, 8)\nt.test(x1, x2, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  x1 and x2\nt = -1.372, df = 8, p-value = 0.2073\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.28924  1.08924\nsample estimates:\nmean of x mean of y \n      4.0       5.6 \n\n\np=0.20, not significant\n\n\n\nx1 <- c(2, 3, 4, 5, 6)\nx2 <- c(3, 4, 7, 6, 8)\nt.test(x1, x2, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  x1 and x2\nt = -4, df = 4, p-value = 0.01613\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.710578 -0.489422\nsample estimates:\nmean difference \n           -1.6 \n\n\np=0.016, significant\n\nIt can be seen that the paired t-test has a greater discriminatory power in this case."
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney-and-wilcoxon-test",
    "href": "slides/05-classtests.html#mann-whitney-and-wilcoxon-test",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney and Wilcoxon-test",
    "text": "Mann-Whitney and Wilcoxon-test\n\n\nNon-parametric tests:\n\nNo assumptions about shape and parameters of distribution, but\ndistributions should be similar, otherwise test may be misleading.\n\nMann-Whitney U-test and Wilcoxon-test are very similar.\nIn a strict sense, “Wilcoxon” is the version for paired data\n\n Basic principle: Count of so-called “inversions” of ranks, where samples overlap\n\nSample A: 1, 3, 4, 5, 7\nSample B: 6, 8, 9, 10, 11\nBoth samples ordered together: 1, 3, 4, 5, 6, 7, 8, 9, 10, 11\nInversions: \\(\\rightarrow\\) \\(U = 1\\)"
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney-test-procedure-in-practice",
    "href": "slides/05-classtests.html#mann-whitney-test-procedure-in-practice",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney test procedure in practice",
    "text": "Mann-Whitney test procedure in practice\n\nAssign ranks \\(R_A\\) and \\(R_B\\) to both samples \\(A\\), and \\(B\\) with sample size \\(m\\) and \\(n\\).\nCalculate number of inversions \\(U\\):\n\n\\[\\begin{align*}\n     U_A &= m \\cdot n + \\frac{m (m + 1)}{2} - \\sum_{i=1}^m R_A \\\\\n     U_B &= m \\cdot n + \\frac{n (n + 1)}{2} - \\sum_{i=1}^n R_B \\\\\n     U   &= \\min(U_A, U_B)\n\\end{align*}\\]\n\nCritical values of \\(U\\) can be found in common statistics text books.\nNot necessary in R, p-value directly printed.\nNote: Use special version wilcox.exact with correction if sample has ties."
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney---wilcoxon-test-in-r",
    "href": "slides/05-classtests.html#mann-whitney---wilcoxon-test-in-r",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney - Wilcoxon-test in R",
    "text": "Mann-Whitney - Wilcoxon-test in R\n\n\nA <- c(1, 3, 4, 5, 7)\nB <- c(6, 8, 9, 10, 11)\n\nwilcox.test(A, B) # use optional argument `paired = TRUE` for paired data.\n\n\n    Wilcoxon rank sum exact test\n\ndata:  A and B\nW = 1, p-value = 0.01587\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nMann-Whitney - Wilcoxon-test with tie correction\n\napplied if the rank differences contain doubled values\n\n\nA <- c(1, 3, 4, 5, 7)\nB <- c(6, 8, 9, 10, 11)\n\n\nlibrary(\"exactRankTests\")\nwilcox.exact(A, B, paired=TRUE)\n\n\n    Exact Wilcoxon signed rank test\n\ndata:  A and B\nV = 0, p-value = 0.0625\nalternative hypothesis: true mu is not equal to 0"
  },
  {
    "objectID": "slides/05-classtests.html#permutation-methods",
    "href": "slides/05-classtests.html#permutation-methods",
    "title": "05-Classical Tests",
    "section": "Permutation methods",
    "text": "Permutation methods\n\nBasic principle: Estimation of a test statistic \\(x_{obs}\\) from sample,\nResampling: Simulate many \\(x_{i, sim}\\) from randomly permuted data set (\\(n = 999\\) or more)\nWhere does \\(x_{est}\\) appear within the ordered series of simulated values \\(x_{i, sim}\\)?\n\n\nLet \\(x_{obs}\\) be \\(4.5\\) in our example, then \\(\\Rightarrow\\) \\(p= 0.97\\)."
  },
  {
    "objectID": "slides/05-classtests.html#testing-for-distributions",
    "href": "slides/05-classtests.html#testing-for-distributions",
    "title": "05-Classical Tests",
    "section": "Testing for distributions",
    "text": "Testing for distributions\nNominal variables\n\n\\(\\chi^2\\)-test\nFisher’s exact test\nsee test for dependency\n\nOrdinal variables\n\nCramér-von-Mises-Test\n[\\(\\Rightarrow\\)] more powerful than \\(\\chi^2\\) or KS-test\n\nMetric scales\n\nKolmogorov-Smirnov-Test (KS-test)\nShapiro-Wilks-Test (for normal distribution)"
  },
  {
    "objectID": "slides/05-classtests.html#contingency-tables-for-nominal-variables",
    "href": "slides/05-classtests.html#contingency-tables-for-nominal-variables",
    "title": "05-Classical Tests",
    "section": "Contingency tables for nominal variables",
    "text": "Contingency tables for nominal variables\n\nContingency tables used for nominal (i.e. categorical or qualitative) data\nexamples: eye and hair color, the kind of medical treatment and the number of cured/not cured\nmportant: use absolute mesurements (true numbers!), not percentages or other calculated data (e.g. not something like biomass per area)\n\nData example\n\nDaphnia (water flee)-clones and their preferred depth in a lake\nfood algae in the deep water zone poor of oxygen\nonly adapted clones with high hemoglobin can dive to the deep water\n\n\n\n\nClone\nUpper layer\nDeep layer\n\n\n\n\nA\n50\n87\n\n\nB\n37\n78\n\n\nC\n72\n45"
  },
  {
    "objectID": "slides/05-classtests.html#calculation-of-the-chi2-test",
    "href": "slides/05-classtests.html#calculation-of-the-chi2-test",
    "title": "05-Classical Tests",
    "section": "Calculation of the \\(\\chi^2\\)-test",
    "text": "Calculation of the \\(\\chi^2\\)-test\n\nObserved frequencies \\(O_{ij}\\) and sums of rows and columns:\n\n\n\n\n\nClone A\nClone B\nClone C\nSum \\(s_i\\)\n\n\n\n\nEpilimnion\n50\n37\n72\n159\n\n\nHypolimnion\n87\n78\n45\n210\n\n\nSum {\\(s_j\\)}\n137\n115\n117\n\\(n=369\\)\n\n\n\n\n\n\n\n\n\n\n\nExpected frequencies \\(E_{ij}\\) under the \\(H_0\\): \\({\\mathbf E = s_i \\otimes s_j} / n\\)\n\n\n\n\n\nClone A\nClone B\nClone C\n\n\n\n\nEpilimnion\n59.0\n49.6\n50.4\n\n\nHypolimnion\n78.0\n65.4\n66.6\n\n\n\n\n\n\n\n\n\n\nCalculate \\(\\hat{\\chi}^2 = \\sum_{i, j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\) with \\((n_{row} - 1) \\cdot (n_{col} - 1)\\) degrees of freedom.\n\n\nCompare with critical \\(\\chi^2\\) from table.\nNote: The results are only reliable if all observed frequencies are$ $.\nFor smaller samples, use Fisher’s exact test"
  },
  {
    "objectID": "slides/05-classtests.html#the-chi2-test-in-r",
    "href": "slides/05-classtests.html#the-chi2-test-in-r",
    "title": "05-Classical Tests",
    "section": "The \\(\\chi^2\\)-test in R",
    "text": "The \\(\\chi^2\\)-test in R\n\nOrganize data in a matrix with 3 rows (for the clones) and 2 columns (for the depths):\n\n\n\n     [,1] [,2]\n[1,]   50   87\n[2,]   37   78\n[3,]   72   45\n\n\n\n\\(\\chi^2\\)-test\n\n\n\n\n    Pearson's Chi-squared test\n\ndata:  x\nX-squared = 24.255, df = 2, p-value = 5.408e-06\n\n\n\nor Fisher’s exact test:\n\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  x\np-value = 5.807e-06\nalternative hypothesis: two.sided\n\n\n\n\\(\\rightarrow\\) significant correlation between the clones and their location."
  },
  {
    "objectID": "slides/05-classtests.html#favorite-numbers-of-hse-students",
    "href": "slides/05-classtests.html#favorite-numbers-of-hse-students",
    "title": "05-Classical Tests",
    "section": "Favorite numbers of HSE students",
    "text": "Favorite numbers of HSE students\n\n\nNumbers from 1..9, \\(n=34\\)\n\\(H_0\\): equal probability of all numbers \\(1/9\\) (discrete uniform distribution)\n\\(H_A\\): some numbers are favored \\(\\rightarrow\\) departure from discrete uniform"
  },
  {
    "objectID": "slides/05-classtests.html#chisquare-test",
    "href": "slides/05-classtests.html#chisquare-test",
    "title": "05-Classical Tests",
    "section": "Chisquare test",
    "text": "Chisquare test\n\n\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  obsfreq\nX-squared = 13.647, df = 8, p-value = 0.09144\n\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 1000 replicates)\n\ndata:  obsfreq\nX-squared = 13.647, df = NA, p-value = 0.0969\n\n\n\n\n\none-sample \\(\\chi^2\\)-test. It tests for equality of frequency in all classes.\nThe simulation-based version of the test (with 1000 replicates) is slightly more precise than the standard \\(\\chi^2\\)-test, but both ar not significant."
  },
  {
    "objectID": "slides/05-classtests.html#cramér-von-mises-test",
    "href": "slides/05-classtests.html#cramér-von-mises-test",
    "title": "05-Classical Tests",
    "section": "Cramér-von-Mises-Test",
    "text": "Cramér-von-Mises-Test\n\n\\[\nT = n \\omega^2 = \\frac{1}{12n} + \\sum_{i=1}^n \\left[ \\frac{2i-1}{2n}-F(x_i) \\right]^2\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#cramér-von-mises-test-in-r",
    "href": "slides/05-classtests.html#cramér-von-mises-test-in-r",
    "title": "05-Classical Tests",
    "section": "Cramér-von-Mises-Test in R",
    "text": "Cramér-von-Mises-Test in R\n\nlibrary(dgof)\nobsfreq <- c(1, 1, 6, 2, 2, 5, 8, 6, 3)\n\n## CvM-test needs individual values, not class frequencies\nx <- rep(1:length(obsfreq), obsfreq)\nx\n\n [1] 1 2 3 3 3 3 3 3 4 4 5 5 6 6 6 6 6 7 7 7 7 7 7 7 7 8 8 8 8 8 8 9 9 9\n\n\n\n\n## create a cumulative function with equal probability of all cases\ncdf <- stepfun(1:9, cumsum(c(0, rep(1/9, 9))))\ncdf <- ecdf(1:9)\n\n## perform the test\ncvm.test(x, cdf)\n\n\n    Cramer-von Mises - W2\n\ndata:  x\nW2 = 0.51658, p-value = 0.03665\nalternative hypothesis: Two.sided\n\n\n\nThe Cramér-von-Mises-test works with the original, unbinned values\nUse of cumulative function respects order of classes \\(\\rightarrow\\) more powerful, than \\(\\chi^2\\)-test."
  },
  {
    "objectID": "slides/05-classtests.html#test-distribution-type-of-metric-variables",
    "href": "slides/05-classtests.html#test-distribution-type-of-metric-variables",
    "title": "05-Classical Tests",
    "section": "Test distribution type of metric variables",
    "text": "Test distribution type of metric variables\n\n\n\nhistogram, boxplot, quantile-quantile plot\nShapiro Wilk W-test\nBox-Cox method\nsee section Distributions"
  },
  {
    "objectID": "slides/05-classtests.html#correlation",
    "href": "slides/05-classtests.html#correlation",
    "title": "05-Classical Tests",
    "section": "Correlation",
    "text": "Correlation\n\nFrequencies of nominal variables\n\nχ2-test\nFisher’s exact test\n\n⇒ dependence between plant society and soil type\n(see before)\nOrdinal variables\n\nSpearman-Correlation\n\n⇒ rank numbers\nMetric scales\n\nPearson-correlation\nSpearman-correlation"
  },
  {
    "objectID": "slides/05-classtests.html#variance-and-covariance",
    "href": "slides/05-classtests.html#variance-and-covariance",
    "title": "05-Classical Tests",
    "section": "Variance and Covariance",
    "text": "Variance and Covariance\n\n\nVariance\n\nmeasures variation of a single variable\n\n\\[\n  s^2_x = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\n\\]\nCovariance\n\nmeasures how two variables change together\n\n\\[\n  q_{x,y} = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n\\]\nCorrelation: scaled to \\((-1, +1)\\)\n\\[\n  r_{x,y} = \\frac{q_{x,y}}{s_x \\cdot s_y}\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#correlation-coefficient-after-pearson",
    "href": "slides/05-classtests.html#correlation-coefficient-after-pearson",
    "title": "05-Classical Tests",
    "section": "Correlation coefficient after Pearson",
    "text": "Correlation coefficient after Pearson\n\n\n\nthe usual correlation coefficient that we all know\ntests for linear dependence\n\n\\[\nr_P=\\frac{\\sum{(x_i-\\bar{x})  (y_i-\\bar{y})}}\n       {\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n\\]\nOr:\n\\[\nr_P=\\frac {\\sum xy - \\sum y \\sum y / n}\n        {\\sqrt{(\\sum x^2-(\\sum x)^2/n)(\\sum y^2-(\\sum y)^2/n)}}\n\\] \nRange of values: \\(-1 \\le r_P \\le +1\\)\n\n\n\n\\(0\\)\nno interdependence\n\n\n\\(+1 \\text{resp.} -1\\)\nstrictly positive resp. negative dependence\n\n\n\\(0 < |r_P| < 1\\)\npositive resp. negative dependence"
  },
  {
    "objectID": "slides/05-classtests.html#which-size-of-correlation-indicates-dependency",
    "href": "slides/05-classtests.html#which-size-of-correlation-indicates-dependency",
    "title": "05-Classical Tests",
    "section": "Which size of correlation indicates dependency?",
    "text": "Which size of correlation indicates dependency?\n\n\n\n\n\n\n\\(r=0.4, \\quad p=0.0039\\)\n\n\n\n\n\n\n\n\\(r=0.85, \\quad p=0.07\\)"
  },
  {
    "objectID": "slides/05-classtests.html#significant-correlation",
    "href": "slides/05-classtests.html#significant-correlation",
    "title": "05-Classical Tests",
    "section": "Significant correlation?",
    "text": "Significant correlation?\n\\[\n\\hat{t}_{\\alpha/2;n-2} =\\frac{|r_P|\\sqrt{n-2}}{\\sqrt{1-r^2_P}}\n\\]\n\\(t=0.829 \\cdot \\sqrt{1000-2}/\\sqrt{1-0.829^2}=46.86, df=998\\)\n Quick test: critical values for \\(r_P\\)\n\n\n\n\\(n\\)\nd.f.\n\\(t\\)\n\n\n3\n1\n12.706\n\n\n5\n3\n3.182\n\n\n10\n8\n2.306\n\n\n20\n18\n2.101\n\n\n50\n48\n2.011\n\n\n100\n98\n1.984\n\n\n1000\n998\n1.962"
  },
  {
    "objectID": "slides/05-classtests.html#rank-correlation-according-to-spearman",
    "href": "slides/05-classtests.html#rank-correlation-according-to-spearman",
    "title": "05-Classical Tests",
    "section": "Rank-correlation according to Spearman",
    "text": "Rank-correlation according to Spearman\n\n\nmeasures monotonous (and not necessarily linear) dependence\nestimation from rank differences:\n\n\\[\nr_S=1-\\frac{6 \\sum d^2_i}{n(n^2-1)}\n\\]\n\nor, alternatively: Pearson-correlation of ranked data (necessary in case of ties).\nTest: for \\(n < 10\\) \\(\\rightarrow\\) table of critical values\n\nfor \\(10 \\leq n\\) \\(\\rightarrow\\) \\(t\\)-distribution\n\\[\n   \\hat{t}_{1-\\frac{\\alpha}{2};n-2}\n      =\\frac{|r_S|}{\\sqrt{1-r^2_S}} \\sqrt{n-2}\n\\]\n\n\nComputer statistics packages use a special algorithm (algorithm AS 89 according to Best and Roberts, 1975)."
  },
  {
    "objectID": "slides/05-classtests.html#example",
    "href": "slides/05-classtests.html#example",
    "title": "05-Classical Tests",
    "section": "Example",
    "text": "Example\n\n\n\n\\(x\\)\n\\(y\\)\n\\(R_x\\)\n\\(R_y\\)\n\\(d\\)\n\\(d^2\\)\n\n\n\n\n1\n2.7\n1\n1\n0\n0\n\n\n2\n7.4\n2\n2\n0\n0\n\n\n3\n20.1\n3\n3\n0\n0\n\n\n4\n500.0\n4\n5\n-1\n1\n\n\n5\n148.4\n5\n4\n+1\n1\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\\[\nr_S=1-\\frac{6 \\cdot 2}{5\\cdot (25-1)}=1-\\frac{12}{120}=0.9\n\\]\nFor comparison: \\(r_P=0.58\\)"
  },
  {
    "objectID": "slides/05-classtests.html#application-of-spearmans-r_s",
    "href": "slides/05-classtests.html#application-of-spearmans-r_s",
    "title": "05-Classical Tests",
    "section": "Application of Spearman’s-\\(r_S\\)",
    "text": "Application of Spearman’s-\\(r_S\\)\n\nAdvantages\n\ndistribution free (does not require normal distribution),\ndetects any dependence,\nnot much affected by outliers.\n\nDisadvantages:\n\ncertain information loss due to ranking,\nno information about type of dependency,\nno direct relationship to coefficient of determination.\n\nConclusion: \\(r_S\\) is nevertheless highly recommended!"
  },
  {
    "objectID": "slides/05-classtests.html#correlation-coefficients-in-r",
    "href": "slides/05-classtests.html#correlation-coefficients-in-r",
    "title": "05-Classical Tests",
    "section": "Correlation coefficients in R",
    "text": "Correlation coefficients in R\n\nPearson’s product-moment correlation coefficient\nSpearman’s rank correlation coefficient\n\n\nx <- c(1, 2, 3, 5, 7,  9)\ny <- c(3, 2, 5, 6, 8, 11)\ncor.test(x, y, method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 7.969, df = 4, p-value = 0.001344\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7439930 0.9968284\nsample estimates:\n      cor \n0.9699203 \n\n\nIf linearity or normality of residuals is doubtful, use a rank correlation\n\ncor.test(x, y, method=\"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  x and y\nS = 2, p-value = 0.01667\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9428571"
  },
  {
    "objectID": "slides/05-classtests.html#problematic-cases",
    "href": "slides/05-classtests.html#problematic-cases",
    "title": "05-Classical Tests",
    "section": "Problematic cases",
    "text": "Problematic cases"
  },
  {
    "objectID": "slides/05-classtests.html#outlook-more-than-two-independent-variables",
    "href": "slides/05-classtests.html#outlook-more-than-two-independent-variables",
    "title": "05-Classical Tests",
    "section": "Outlook: More than two independent variables",
    "text": "Outlook: More than two independent variables\n\nMultiple correlation\n\nExample: Chl-a=\\(f(X_1, X_2, X_3, X_4, \\dots)\\), where \\(X_i\\) is the biomass of the \\(i\\)th phytoplankton species.\nmultiple correlation coefficient\npartial correlation coefficient\nattractive method \\(\\leftrightarrow\\) but difficult in practice:\n\n“independent” variables may correlate with each other (multi-collinearity) \\(\\Rightarrow\\) bias of the multiple \\(r\\).\nnon-linearities are even more difficult to handle than in the two-sample case.\n\n\nRecommendation:\n\nUse multivariate Methods (NMDS, PCA, …) for a first overview,\napply multiple regression with care and use process knowledge."
  },
  {
    "objectID": "slides/05-classtests.html#determining-the-power-of-statistical-tests",
    "href": "slides/05-classtests.html#determining-the-power-of-statistical-tests",
    "title": "05-Classical Tests",
    "section": "Determining the power of statistical tests",
    "text": "Determining the power of statistical tests\n\nHow many replicates will I need?\n\nDepends on:\n\nthe relative effect size (effect/standard deviation, \\(\\delta=\\frac{(\\bar{x}_1-\\bar{x}_2)}{s}\\),\nthe sample size \\(n\\),\nand the pre-defined significance level \\(\\alpha\\).\nand the used method\n\nThe smaller \\(\\alpha\\), \\(n\\) and \\(\\Delta\\), the bigger the type II error (\\(\\beta\\) error).\nThis is the probability to overlook effects despite of their existence."
  },
  {
    "objectID": "slides/05-classtests.html#power-analysis-1",
    "href": "slides/05-classtests.html#power-analysis-1",
    "title": "05-Classical Tests",
    "section": "Power analysis",
    "text": "Power analysis\n\nFormula for minimum sample size in the one-sample case:\n\\[\nn = \\bigg(\\frac{z_\\alpha + z_{1-\\beta}}{\\Delta}\\bigg)^2\n\\]\n\n\\(z\\): the quantiles (qnorm) of the standard normal distribution for \\(\\alpha\\) and for \\(1-\\beta\\)\n\\(\\delta=\\Delta / s\\): relative effect size.\n\nExample\nTwo-tailed test with \\(\\alpha=0.025\\) and \\(\\beta=0.2\\)\n\\(\\rightarrow\\) \\(z_\\alpha = 1.96\\), \\(z_\\beta=0.84\\), then:\n\\[\nn= (1.96 \\pm 0.84)^2 \\cdot 1/\\delta^2 \\approx 8 /\\delta^2\n\\]"
  },
  {
    "objectID": "slides/05-classtests.html#power-of-the-t-test",
    "href": "slides/05-classtests.html#power-of-the-t-test",
    "title": "05-Classical Tests",
    "section": "Power of the t-test",
    "text": "Power of the t-test\nThe power of a t-test, or the minimum sample size, can be calculated with: power.t.test():\n\npower.t.test(n=5, delta=0.5, sig.level=0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.1038399\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\rightarrow\\) power = 0.10 * for \\(n=5\\) an existing effect of \\(0.5\\sigma\\) is only detected in 1 out of 10 cases.\nFor a power of 80% at \\(n=5\\) we need an effect size of at least \\(2\\sigma\\):\n\npower.t.test(n=5, power=0.8, sig.level=0.05)\n\nFor a weak effect of \\(0.5\\sigma\\) we need a sample size of \\(n\\ge64\\) in each group:\n\npower.t.test(delta=0.5,power=0.8,sig.level=0.05)\n\n\\(\\Rightarrow\\) either a large sample size or a large effect size."
  },
  {
    "objectID": "slides/06-linear.html#the-linear-model",
    "href": "slides/06-linear.html#the-linear-model",
    "title": "06-Linear Regression",
    "section": "The linear model",
    "text": "The linear model\n\\[\ny_i = \\alpha + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_p x_{i,p} + \\varepsilon_i\n\\]\n\nFundamental for many statistical methods\n\nlinear regression including some (at a first look) “nonlinear” functions\nANOVA, ANCOVA, GLM (simultanaeous testing of multiple samples or multiple factors)\nmultivariate statistics (e.g. PCA)\ntime series analysis (e.g. ARIMA)\nimputation (estimation of missing values)"
  },
  {
    "objectID": "slides/06-linear.html#method-of-least-squares",
    "href": "slides/06-linear.html#method-of-least-squares",
    "title": "06-Linear Regression",
    "section": "Method of least squares",
    "text": "Method of least squares\n\\[\nRSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n  \\varepsilon^2 \\qquad \\text{(residual sum of squares)}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n  \\text{total variance} &= \\text{explained variance} &+& \\text{residual variance}\\\\\n                    s^2_y &= s^2_{y|x}               &+& s^2_{\\varepsilon}\n\\end{align}\\]"
  },
  {
    "objectID": "slides/06-linear.html#the-coefficient-of-determination",
    "href": "slides/06-linear.html#the-coefficient-of-determination",
    "title": "06-Linear Regression",
    "section": "The coefficient of determination",
    "text": "The coefficient of determination\n\\[\\begin{align}\n     r^2 & = \\frac{\\text{explained variance}}{\\text{total variance}}\\\\\n         & = \\frac{s^2_{\\varepsilon}}{s^2_{y|x}}\\\\\n\\end{align}\\]\nIt can also be expressed as ratio of residual (RSS) and total (TSS) sum of squares:\n\\[\n    r^2 = 1-\\frac{s^2_{\\varepsilon}}{s^2_{y}} = 1-\\frac{RSS}{TSS} =  1- \\frac{\\sum(y_i -\\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\n\\]\n\nderived from “sum of squares”, scaled as relative variance\nidentical to the squared Pearon correlation \\(r^2\\) (in the linear case)\nvery useful interpretation: percentage of variance of the raw data explained by the model\n\nFor the example: \\(r^2= 1-\\) 15.3 \\(/\\) 40.8 \\(=\\) 0.625"
  },
  {
    "objectID": "slides/06-linear.html#minimization-of-rss",
    "href": "slides/06-linear.html#minimization-of-rss",
    "title": "06-Linear Regression",
    "section": "Minimization of RSS",
    "text": "Minimization of RSS\n\nAnalytical solution: minimize sum of squares (\\(\\sum \\varepsilon^2\\))\nLinear system of equations\nMinimum RSS \\(\\longleftarrow\\) partial 1st derivatives (\\(\\partial\\))\n\nFor \\(y=a \\cdot x + b\\) with 2 parameters: \\(\\frac{\\partial\\sum \\varepsilon^2}{\\partial{a}}=0\\), \\(\\frac{\\partial\\sum \\varepsilon^2}{\\partial{b}}=0\\):\n\n\\[\\begin{align}\n  \\frac{\\partial \\sum(\\hat{y_i} - y_i)^2}{\\partial a}     &= \\frac{\\partial \\sum(a + b \\cdot x_i - y_i)^2}{\\partial a} = 0\\\\\n      \\frac{\\partial \\sum(\\hat{y_i} - y_i)^2}{\\partial b} &= \\frac{\\partial \\sum(a + b \\cdot x_i - y_i)^2}{\\partial b} = 0\n\\end{align}\\]\nSolution of the linear system of equations:\n\\[\\begin{align}\nb &=\\frac {\\sum x_iy_i - \\frac{1}{n}(\\sum x_i \\sum y_i)} {\\sum x_i^2 - \\frac{1}{n}(\\sum x_i)^2}\\\\\na &=\\frac {\\sum y_i - b \\sum x_i}{n}\n\\end{align}\\]\n\nsolution for arbitrary number of parameters with matrix algebra"
  },
  {
    "objectID": "slides/06-linear.html#significance-of-the-regression",
    "href": "slides/06-linear.html#significance-of-the-regression",
    "title": "06-Linear Regression",
    "section": "Significance of the regression",
    "text": "Significance of the regression\n\n\\[\n\\hat{F}_{1;n-2;\\alpha}= \\frac{s^2_{explained}}{s^2_{residual}}\n                         = \\frac{r^2(n-2)}{1-r^2}\n\\] \nAssumptions\n\nValidity: the data maps to the research question\nAdditivity and linearity: \\(y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots\\)\nIndependence of errors: residuals around the regression line are independent\nEqual variance of errors: residuals homogeneously distributed around the regression line\nNormality of errors: the “assumption that is generally least important”\n\nSee: Gelman & Hill (2007) : Data analysis using regression …"
  },
  {
    "objectID": "slides/06-linear.html#diagnostics",
    "href": "slides/06-linear.html#diagnostics",
    "title": "06-Linear Regression",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\n\nNo regression analysis without graphical diagnostics!\n\nx-y-plot with regression line: is the variance homogeneous?\nPlot of residuals vs. fitted: are there still any remaining patterns?\nQ-Q-plot, histogram, : is distribution of residuals approximately normal?\n\nUse graphical methods for normality, don’t trust the Shapiro-Wilks in that case."
  },
  {
    "objectID": "slides/06-linear.html#confidence-intervals-of-the-parameters",
    "href": "slides/06-linear.html#confidence-intervals-of-the-parameters",
    "title": "06-Linear Regression",
    "section": "Confidence intervals of the parameters",
    "text": "Confidence intervals of the parameters\n\nbased on standard errors and the t-distribution, similar to CI of the mean\n\n\\[\\begin{align}\na & \\pm t_{1-\\alpha/2, n-2} \\cdot s_a\\\\\nb & \\pm t_{1-\\alpha/2, n-2} \\cdot s_b\n\\end{align}\\]\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4451 -1.0894 -0.4784  1.5065  3.1933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2.50740    0.87338   2.871   0.0102 *  \nx            2.04890    0.07427  27.589 3.51e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.885 on 18 degrees of freedom\nMultiple R-squared:  0.9769,    Adjusted R-squared:  0.9756 \nF-statistic: 761.1 on 1 and 18 DF,  p-value: 3.514e-16\n\n\n Example: CI of a: \\(a \\pm t_{1-\\alpha/2, n-2} \\cdot s_a = 2.5074 \\pm 2.09 \\cdot 0.87338\\)"
  },
  {
    "objectID": "slides/06-linear.html#confidence-interval-and-prediction-interval",
    "href": "slides/06-linear.html#confidence-interval-and-prediction-interval",
    "title": "06-Linear Regression",
    "section": "Confidence interval and prediction interval",
    "text": "Confidence interval and prediction interval\n\n\nConfidence interval:\n\nShows the area where the “true regression line” is expected by 95%.\nWidth of this band decreses with increasing \\(n\\)\nanalogous to the standard error\n\nPrediction interval:\n\nShows the range, in which the prediction for a single value is expected (by 95%).\nWidth is independent of sample size \\(n\\)\nanalogous to the standard deviation"
  },
  {
    "objectID": "slides/06-linear.html#confidence-intervals-for-linear-regression-code",
    "href": "slides/06-linear.html#confidence-intervals-for-linear-regression-code",
    "title": "06-Linear Regression",
    "section": "Confidence intervals for linear regression: Code",
    "text": "Confidence intervals for linear regression: Code\n\n## generate example data\nx <- 1:10\ny <- 2 + 0.5 * x + 0.5 * rnorm(x)\n\n## fit model\nreg <- lm(y ~ x)\nsummary(reg)\n\n## plot data and regression line\nplot(x,y, xlim = c(0, 10), ylim = c(0, 10), pch = 16)\nabline(reg, lwd = 2)\n\n## calcuate and plot intervals\nnewdata <- data.frame(x=seq(-1, 11, length=100))\nconflim <- predict(reg, newdata=newdata, interval = \"confidence\")\npredlim <- predict(reg, newdata=newdata, interval = \"prediction\")\n\nlines(newdata$x, conflim[,2], col = \"blue\")\nlines(newdata$x, conflim[,3], col = \"blue\")\nlines(newdata$x, predlim[,2], col = \"red\")\nlines(newdata$x, predlim[,3], col = \"red\")\n\n\nThe variable newdata:\n\nspans the range of x values in small steps to get a smooth curve\nsingle column with exactly the same name x as in the model formula\nfor multiple regression, one column per explanation variable"
  },
  {
    "objectID": "slides/06-linear.html#problematic-cases",
    "href": "slides/06-linear.html#problematic-cases",
    "title": "06-Linear Regression",
    "section": "Problematic cases",
    "text": "Problematic cases"
  },
  {
    "objectID": "slides/06-linear.html#identification-and-treatment-of-problematic-cases",
    "href": "slides/06-linear.html#identification-and-treatment-of-problematic-cases",
    "title": "06-Linear Regression",
    "section": "Identification and treatment of problematic cases",
    "text": "Identification and treatment of problematic cases\n\nRainbow-Test (linearity)\n\n## generate test data\nx <- 1:10\ny <- 2 + 0.5 * x + 0.5 * rnorm(x)\n\nlibrary(lmtest)\nraintest(y~x)\n\n\n    Rainbow test\n\ndata:  y ~ x\nRain = 0.79952, df1 = 5, df2 = 3, p-value = 0.6153\n\n\n\nBreusch-Pagan-test (variance homogeneity)\n\nbptest(y~x)\n\n\n    studentized Breusch-Pagan test\n\ndata:  y ~ x\nBP = 3.3989, df = 1, p-value = 0.06524"
  },
  {
    "objectID": "slides/06-linear.html#non-normality-and-outliers",
    "href": "slides/06-linear.html#non-normality-and-outliers",
    "title": "06-Linear Regression",
    "section": "Non-normality and outliers",
    "text": "Non-normality and outliers\n\n\nNon-normality\n\nless important than many people think (due to the CLT)\ntransformations (e.g. Box-Cox), polynomials, periodic functions \nuse of GLM’s (generalized linear models)\n\n\n\n\nOutliers (depends on pattern)\n\nuse of transformations (e.g. double log)\nuse of outlier-tests, e.g. outlierTest from package car\nrobust regression with IWLS (iteratively re-weighted least squares) from package MASS"
  },
  {
    "objectID": "slides/06-linear.html#robust-regression-with-iwls",
    "href": "slides/06-linear.html#robust-regression-with-iwls",
    "title": "06-Linear Regression",
    "section": "Robust regression with IWLS",
    "text": "Robust regression with IWLS\n\n\nIWLS: iterated re-weighted least squares\nOLS (ordinary least squares) is “normal” linear regression\nM-estimation and MM-estimation are two different approaches, details in Venables & Ripley (2013)\nrobust regression is preferred over outlier exclusion"
  },
  {
    "objectID": "slides/06-linear.html#code-of-the-iwls-regression",
    "href": "slides/06-linear.html#code-of-the-iwls-regression",
    "title": "06-Linear Regression",
    "section": "Code of the IWLS regression",
    "text": "Code of the IWLS regression\n\nlibrary(\"MASS\")\n\n## test data with 2 \"outliers\"\nx <- c(1, 2, 3, 3, 4, 5, 7, 7, 7, 8, 8, 9, 10, 14, 15, 15, 16, 17, 18, 18)\ny <- c(8.1, 20, 10.9, 8.4, 9.6, 16.1, 17.3, 15.3, 16, 15.9, 19.3, \n       21.3, 24.8, 31.3, 4, 31.9, 33.7, 36.5, 42.4, 38.5)\n\n## fit the models\nssq    <- lm(y ~ x)\niwls   <- rlm(y ~ x)\niwlsmm <- rlm(y ~ x, method = \"MM\")\n\n## plot the models\nplot(x, y, pch = 16, las = 1)\nabline(ssq, col = \"blue\", lty = \"dashed\")\nabline(iwls, col = \"red\")\nabline(iwlsmm, col = \"green\")\nlegend(\"topleft\", c(\"OLS\", \"IWLS-M\", \"IWLS-MM\"),\n       col = c(\"blue\", \"red\", \"green\"),\n       lty = c(\"dashed\", \"solid\", \"solid\"))"
  },
  {
    "objectID": "slides/06-linear.html#references",
    "href": "slides/06-linear.html#references",
    "title": "06-Linear Regression",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nFox, J., & Weisberg, S. (2018). An R companion to applied regression. Sage publications.\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevelhierarchical models (Vol. 1). Cambridge University Press New York, NY, USA.\n\n\nKleiber, C., & Zeileis, A. (2008). Applied econometrics with R. Springer.\n\n\nVenables, W. N., & Ripley, B. D. (2013). Modern applied statistics with S-PLUS (3rd ed.). Springer Science; Business Media."
  },
  {
    "objectID": "slides/07-anova.html#anova-analysis-of-variances",
    "href": "slides/07-anova.html#anova-analysis-of-variances",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA – Analysis of Variances",
    "text": "ANOVA – Analysis of Variances\n\n\nTesting of complex hypothesis as a whole, e.g.:\n\nmore than two samples (multiple test problem),\nseveral multiple factors (multiway ANOVA)\nelimination of covariates (ANCOVA)\nfixed and/or random effects (variance decomposition methods, mixed effects models)\n\nDifferent application scenarios:\n\nexplorative use: Which influence factors are important?\ndescriptive use: Fitting of models for process description and forecasting.\nsignificance tests.\n\nANOVA methods are (in most cases) based on linear models."
  },
  {
    "objectID": "slides/07-anova.html#a-practical-example",
    "href": "slides/07-anova.html#a-practical-example",
    "title": "07-One and two-way ANOVA",
    "section": "A practical example",
    "text": "A practical example\n\nFind a suitable medium for growth experiments with green algae\n\ncheap, easy to handle\nsuitable for students courses and classroom experiments\n\n\n\n\n\nIdea\n\nuse a commercial fertilizer with the main nutrients N and P\nmineral water with trace elements\ndoes non-sparkling mineral water contain enough \\(\\mathrm{CO_2}\\)?\ntest how to improve (\\(\\mathrm{CO_2}\\)) availability for photosynthesis"
  },
  {
    "objectID": "slides/07-anova.html#application",
    "href": "slides/07-anova.html#application",
    "title": "07-One and two-way ANOVA",
    "section": "Application",
    "text": "Application\n\n7 Different treatments\n\n\nfertilizer solution in closed bottles\nfertilizer solution in open bottles (\\(\\mathrm{CO_2}\\) from air)\nfertilizer + sugar (organic C source)\nfertilizer + additional \\(\\mathrm{HCO_3^-}\\) (add \\(\\mathrm{CaCO_3}\\) to sparkling mineral water)\na standard algae growth medium (“Basal medium”) for comparison\ndeionized (“destilled”) water and\ntap water for comparison"
  },
  {
    "objectID": "slides/07-anova.html#experimental-design",
    "href": "slides/07-anova.html#experimental-design",
    "title": "07-One and two-way ANOVA",
    "section": "Experimental design",
    "text": "Experimental design\n\n\neach treatment with 3 replicates\nrandomized placement on a shaker\n16:8 light:dark-cycle\nmeasurement directly in the bottles using a self-made turbidity meter"
  },
  {
    "objectID": "slides/07-anova.html#results",
    "href": "slides/07-anova.html#results",
    "title": "07-One and two-way ANOVA",
    "section": "Results",
    "text": "Results\n\n\n\n\n Fertilizer – Open Bottle – F. + Sugar – F. + CaCO3 – Basal medium – A. dest – Tap water"
  },
  {
    "objectID": "slides/07-anova.html#the-data-set",
    "href": "slides/07-anova.html#the-data-set",
    "title": "07-One and two-way ANOVA",
    "section": "The data set",
    "text": "The data set\n\n\n\n\n\n\n\n\nTable 1:  Growth from day 2 to day 6 (relative units) \n \n  \n    treat \n    replicate 1 \n    replicate 2 \n    replicate 3 \n  \n \n\n  \n    Fertilizer \n    0.020 \n    -0.217 \n    -0.273 \n  \n  \n    F. open \n    0.940 \n    0.780 \n    0.555 \n  \n  \n    F.+sugar \n    0.188 \n    -0.100 \n    0.020 \n  \n  \n    F.+CaCO3 \n    0.245 \n    0.236 \n    0.456 \n  \n  \n    Bas.med. \n    0.699 \n    0.727 \n    0.656 \n  \n  \n    A.dest \n    -0.010 \n    0.000 \n    -0.010 \n  \n  \n    Tap water \n    0.030 \n    -0.070 \n    NA \n  \n\n\n\n\n\n\n\nNA means “not available”, i.e. a missing value\nthe crosstable structure is compact and esy to read, but not suitable for data analysis\n\\(\\Rightarrow\\) convert it to long format"
  },
  {
    "objectID": "slides/07-anova.html#data-in-long-format",
    "href": "slides/07-anova.html#data-in-long-format",
    "title": "07-One and two-way ANOVA",
    "section": "Data in long format",
    "text": "Data in long format\n\n\nAdvantages\n\nlooks “stupid” but is better for data analysis\ndependent variable growth and explanation variable treat clearly visible\nmodel formula: growth ~ treat\neasily extensible to \\(>1\\) explanation variable\n\n\n\n\n\n\n\n \n  \n    treat \n    rep \n    growth \n  \n \n\n  \n    Fertilizer \n    1 \n    0.020 \n  \n  \n    Fertilizer \n    2 \n    -0.217 \n  \n  \n    Fertilizer \n    3 \n    -0.273 \n  \n  \n    F. open \n    1 \n    0.940 \n  \n  \n    F. open \n    2 \n    0.780 \n  \n  \n    F. open \n    3 \n    0.555 \n  \n  \n    F.+sugar \n    1 \n    0.188 \n  \n  \n    F.+sugar \n    2 \n    -0.100 \n  \n  \n    F.+sugar \n    3 \n    0.020 \n  \n  \n    F.+CaCO3 \n    1 \n    0.245 \n  \n  \n    F.+CaCO3 \n    2 \n    0.236 \n  \n  \n    F.+CaCO3 \n    3 \n    0.456"
  },
  {
    "objectID": "slides/07-anova.html#the-data-in-r",
    "href": "slides/07-anova.html#the-data-in-r",
    "title": "07-One and two-way ANOVA",
    "section": "The data in R",
    "text": "The data in R\n\n\nalgae <- data.frame(\n  treat  = factor(c(\"Fertilizer\", \"Fertilizer\", \"Fertilizer\", \n             \"F. open\", \"F. open\", \"F. open\", \n             \"F.+sugar\", \"F.+sugar\", \"F.+sugar\", \n             \"F.+CaCO3\", \"F.+CaCO3\", \"F.+CaCO3\", \n             \"Bas.med.\", \"Bas.med.\", \"Bas.med.\", \n             \"A.dest\", \"A.dest\", \"A.dest\", \n             \"Tap water\", \"Tap water\"),\n             levels=c(\"Fertilizer\", \"F. open\", \"F.+sugar\", \n                    \"F.+CaCO3\", \"Bas.med.\", \"A.dest\", \"Tap water\")),\n  rep   = c(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2), \n  growth = c(0.02, -0.217, -0.273, 0.94, 0.78, 0.555, 0.188, -0.1, 0.02, \n             0.245, 0.236, 0.456, 0.699, 0.727, 0.656, -0.01, 0, -0.01, 0.03, -0.07)\n)\n\n… can be entered directly in the code. A csv-file in long format is also possible."
  },
  {
    "objectID": "slides/07-anova.html#boxplot",
    "href": "slides/07-anova.html#boxplot",
    "title": "07-One and two-way ANOVA",
    "section": "Boxplot",
    "text": "Boxplot\n\nboxplot(growth ~ treat, data = algae)\nabline(h = 0, lty = \"dashed\", col = \"grey\")"
  },
  {
    "objectID": "slides/07-anova.html#stripchart",
    "href": "slides/07-anova.html#stripchart",
    "title": "07-One and two-way ANOVA",
    "section": "Stripchart",
    "text": "Stripchart\n\nstripchart(growth ~ treat, data = algae, vertical = TRUE)\n\n\nBetter, because we have only 2-3 replicates. Boxplot needs more."
  },
  {
    "objectID": "slides/07-anova.html#turn-scientific-question-into-a-statistical-hypothesis",
    "href": "slides/07-anova.html#turn-scientific-question-into-a-statistical-hypothesis",
    "title": "07-One and two-way ANOVA",
    "section": "Turn scientific question into a statistical hypothesis",
    "text": "Turn scientific question into a statistical hypothesis\n\nScientific Questions\n\nAre the treatments different?\nWhich medium is the best?\nIs the best medium significantly better than the others?\n\n Statistical Hypotheses\n\n\\(H_0\\): growth is the same in all treatments\n\\(H_A\\): differences between media"
  },
  {
    "objectID": "slides/07-anova.html#why-cant-we-apply-just-several-t-tests",
    "href": "slides/07-anova.html#why-cant-we-apply-just-several-t-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Why can’t we apply just several t-tests?",
    "text": "Why can’t we apply just several t-tests?\n\n\nIf we have 7 treatments and want to test all against each other, we would need:\n\n\\[7 \\cdot (7 - 1) / 2 = 21 \\qquad\\text{tests.}\\]\n\nIf we set \\(\\alpha = 0.05\\) we get 5% false positives. \\(\\Rightarrow\\) One of 20 tests is on average a false positive\nIf we do \\(N\\) tests, we increase the overall \\(\\alpha\\) error to \\(N\\cdot\\alpha\\) in the worst case.\nThis is called alpha-error-inflation or the Bonferroni law:\n\n\\[\n\\alpha_{total} \\le \\sum_{i=1}^{N} \\alpha_i = N \\cdot \\alpha\n\\]\nIf we ignore the Bonferroni law, we end in statistical fishing and get spurious results just by chance. See https://xkcd.com/882/."
  },
  {
    "objectID": "slides/07-anova.html#anova-analysis-of-variances-1",
    "href": "slides/07-anova.html#anova-analysis-of-variances-1",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA: Analysis of variances",
    "text": "ANOVA: Analysis of variances\n\nBasic Idea\n\nSplit the total variance into effect(s) and errors:\n\n\n\\[\ns_y^2 = s^2_\\mathrm{effect} + s^2_{\\varepsilon}\n\\]\n\n\nSomewhat surprising: we use variances to compare mean values.\nExplanation: differences of means contribute to the total variance of the whole sample.\nVariance components can be called variance within (\\(s^2_\\varepsilon\\)) and variance between samples.\nThe way how to separate variances is a linear model."
  },
  {
    "objectID": "slides/07-anova.html#example",
    "href": "slides/07-anova.html#example",
    "title": "07-One and two-way ANOVA",
    "section": "Example",
    "text": "Example\n\nTwo brands of Clementine fruits from a shop “E”, that we encode as “EB” and “EP”. We want to know whether the premium brand (“P”) and the basic brand (“B”) have a different weight.\n\nclem <- data.frame(\n  brand = c(\"EP\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \n            \"EB\", \"EB\", \"EB\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EB\", \"EP\"),\n  weight = c(88, 96, 100, 96, 90, 100, 92, 92, 102, 99, 86, 89, 99, 89, 75, 80, \n             81, 96, 82, 98, 80, 107, 88))\n\n We encode one sample (“EB”) with 1 and the other sample (“EP”) with 2:\n\nclem$code <- as.numeric(factor(clem$brand))\nclem$code\n\n [1] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2"
  },
  {
    "objectID": "slides/07-anova.html#then-we-fit-a-linear-regression",
    "href": "slides/07-anova.html#then-we-fit-a-linear-regression",
    "title": "07-One and two-way ANOVA",
    "section": "Then we fit a linear regression:",
    "text": "Then we fit a linear regression:\n\nplot(weight ~ code, data = clem, axes = FALSE)\nm <- lm(weight ~ code, data = clem)\naxis(1, at = c(1,2), labels = c(\"EB\", \"EP\")); axis(2); box()\nabline(m, col = \"blue\")"
  },
  {
    "objectID": "slides/07-anova.html#variance-components",
    "href": "slides/07-anova.html#variance-components",
    "title": "07-One and two-way ANOVA",
    "section": "Variance components",
    "text": "Variance components\n\nWe fit a linear model and compare the variances:\n\nm <- lm(weight ~ code, data = clem)\n\ntotal variance\n\n(var_tot <- var(clem$weight))\n\n[1] 68.98814\n\n\nresidual variance (= within variance)\n\n(var_res <- var(residuals(m)))\n\n[1] 43.25\n\n\nexplained variance (= between variance)\n\nvar_tot - var_res\n\n[1] 25.73814\n\n\nNow we can analyse whether the between variance is big enough to justify a significant effect.\nThis is called an ANOVA."
  },
  {
    "objectID": "slides/07-anova.html#anova",
    "href": "slides/07-anova.html#anova",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA",
    "text": "ANOVA\n\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value   Pr(>F)   \ncode       1 566.24  566.24  12.497 0.001963 **\nResiduals 21 951.50   45.31                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nA t-test for comparison\n\nt.test(weight ~ code, data = clem, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by code\nt = 3.5351, df = 21, p-value = 0.001963\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n  4.185911 16.147423\nsample estimates:\nmean in group 1 mean in group 2 \n       95.50000        85.33333 \n\n\n\\(\\Rightarrow\\) the p-values are exactly the same."
  },
  {
    "objectID": "slides/07-anova.html#anova-with-more-than-2-samples",
    "href": "slides/07-anova.html#anova-with-more-than-2-samples",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA with more than 2 samples",
    "text": "ANOVA with more than 2 samples\nBack to the algae growth data. Let’s call the linear model m:\n\nm <- lm(growth ~ treat, data = algae)\n\n\n\nWe can print the coefficients of the linear model with summary(m)\nBut we are interested in the overall effect and use anova\n\n\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: growth\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \ntreat      6 2.35441 0.39240  25.045 1.987e-06 ***\nResiduals 13 0.20368 0.01567                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe ANOVA table shows F-tests testing for significance of all factors.\nIn the table above, we have only one single factor.\n\n\\(\\Rightarrow\\) We see that the treatment had a significant effect."
  },
  {
    "objectID": "slides/07-anova.html#posthoc-tests",
    "href": "slides/07-anova.html#posthoc-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Posthoc tests",
    "text": "Posthoc tests\n\nThe test showed, that the factor “treatment” had a significant effect.\nWe don’t know yet, which factor levels were different.\n\nTukey HSD test is the most common.\n\ntk <- TukeyHSD(aov(m))\ntk\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = m)\n\n$treat\n                            diff         lwr         upr     p adj\nF. open-Fertilizer    0.91500000  0.56202797  1.26797203 0.0000103\nF.+sugar-Fertilizer   0.19266667 -0.16030537  0.54563870 0.5211198\nF.+CaCO3-Fertilizer   0.46900000  0.11602797  0.82197203 0.0069447\nBas.med.-Fertilizer   0.85066667  0.49769463  1.20363870 0.0000231\nA.dest-Fertilizer     0.15000000 -0.20297203  0.50297203 0.7579063\nTap water-Fertilizer  0.13666667 -0.25796806  0.53130140 0.8837597\nF.+sugar-F. open     -0.72233333 -1.07530537 -0.36936130 0.0001312\nF.+CaCO3-F. open     -0.44600000 -0.79897203 -0.09302797 0.0102557\nBas.med.-F. open     -0.06433333 -0.41730537  0.28863870 0.9943994\nA.dest-F. open       -0.76500000 -1.11797203 -0.41202797 0.0000721\nTap water-F. open    -0.77833333 -1.17296806 -0.38369860 0.0001913\nF.+CaCO3-F.+sugar     0.27633333 -0.07663870  0.62930537 0.1727182\nBas.med.-F.+sugar     0.65800000  0.30502797  1.01097203 0.0003363\nA.dest-F.+sugar      -0.04266667 -0.39563870  0.31030537 0.9994197\nTap water-F.+sugar   -0.05600000 -0.45063473  0.33863473 0.9985686\nBas.med.-F.+CaCO3     0.38166667  0.02869463  0.73463870 0.0307459\nA.dest-F.+CaCO3      -0.31900000 -0.67197203  0.03397203 0.0879106\nTap water-F.+CaCO3   -0.33233333 -0.72696806  0.06230140 0.1247914\nA.dest-Bas.med.      -0.70066667 -1.05363870 -0.34769463 0.0001792\nTap water-Bas.med.   -0.71400000 -1.10863473 -0.31936527 0.0004507\nTap water-A.dest     -0.01333333 -0.40796806  0.38130140 0.9999997"
  },
  {
    "objectID": "slides/07-anova.html#graphical-output",
    "href": "slides/07-anova.html#graphical-output",
    "title": "07-One and two-way ANOVA",
    "section": "Graphical output",
    "text": "Graphical output\n\npar(las = 1)             # las = 1 make y annotation horizontal\npar(mar = c(4, 10, 3, 1)) # more space at the left for axis annotation\nplot(tk)"
  },
  {
    "objectID": "slides/07-anova.html#anova-assumptions-and-diagnostics",
    "href": "slides/07-anova.html#anova-assumptions-and-diagnostics",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA assumptions and diagnostics",
    "text": "ANOVA assumptions and diagnostics\n\nANOVA has same assumptions as the linear model.\n\n\nIndependence of errors\nVariance homogeneity\nApproximate normality of errors\n\nGraphical checks are preferred.\n\n\n\npar(mfrow=c(2, 2))\nplot(m)"
  },
  {
    "objectID": "slides/07-anova.html#numerical-tests",
    "href": "slides/07-anova.html#numerical-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Numerical tests",
    "text": "Numerical tests\n\n\nTest variance homogeneity\n\nF-test compares only two variances.\nSeveral tests for multiple variances available, e.g. Bartlett, Levene, Fligner-Killeen\nRecommended: Fligner-Killeen-test\n\n\nfligner.test(growth ~ treat, \n             data = algae)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  growth by treat\nFligner-Killeen:med chi-squared = 4.2095, df = 6, p-value = 0.6483\n\n\n\n\nTest of normal distribution\n\nThe Shapiro-Wilks test can be misleading.\nUse a graphical method!\n\n\nqqnorm(residuals(m))\nqqline(residuals(m))"
  },
  {
    "objectID": "slides/07-anova.html#one-way-anova-with-heterogeneous-variances",
    "href": "slides/07-anova.html#one-way-anova-with-heterogeneous-variances",
    "title": "07-One and two-way ANOVA",
    "section": "One-way ANOVA with heterogeneous variances",
    "text": "One-way ANOVA with heterogeneous variances\n\n\nextension of the Welch test for \\(\\ge 2\\) samples\nin R called oneway.test\n\n\noneway.test(growth ~ treat, data = algae)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  growth and treat\nF = 115.09, num df = 6.0000, denom df = 4.6224, p-value = 6.57e-05"
  },
  {
    "objectID": "slides/07-anova.html#two-way-anova",
    "href": "slides/07-anova.html#two-way-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Two-way ANOVA",
    "text": "Two-way ANOVA\n\n\n\n\nExample from a statistics text book (Crawley, 2002)\nEffects of diet and coat color on growth of Hamsters in gramm per time\n\n\n\n\ndiet\nlight coat\ndark coat\n\n\n\n\nA\n8.3\n6.6\n\n\nA\n8.7\n7.2\n\n\nB\n8.1\n6.9\n\n\nB\n8.5\n8.3\n\n\nC\n9.1\n7.9\n\n\nC\n9.0\n9.2\n\n\n\n\n\n\n\n\n\nfactorial experiment (with replicates): each factor combination has more than one observation.\nwithout replication:\n\nno replicates per factor combination\nthis is possible, but does not allow identification of interactions"
  },
  {
    "objectID": "slides/07-anova.html#enter-data-in-long-format",
    "href": "slides/07-anova.html#enter-data-in-long-format",
    "title": "07-One and two-way ANOVA",
    "section": "Enter data in long format",
    "text": "Enter data in long format\n\n\n\nhams <- data.frame(No = 1:12,\n                   growth = c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,\n                            8.3, 8.7, 8.1, 8.5, 9.1, 9.0),\n                   diet = rep(c(\"A\", \"B\", \"C\"), each=2),\n                   coat = rep(c(\"light\", \"dark\"), each=6)\n                   )\n\n\n\n\n\n\n\n \n  \n    No \n    growth \n    diet \n    coat \n  \n \n\n  \n    1 \n    6.6 \n    A \n    light \n  \n  \n    2 \n    7.2 \n    A \n    light \n  \n  \n    3 \n    6.9 \n    B \n    light \n  \n  \n    4 \n    8.3 \n    B \n    light \n  \n  \n    5 \n    7.9 \n    C \n    light \n  \n  \n    6 \n    9.2 \n    C \n    light \n  \n  \n    7 \n    8.3 \n    A \n    dark \n  \n  \n    8 \n    8.7 \n    A \n    dark \n  \n  \n    9 \n    8.1 \n    B \n    dark \n  \n  \n    10 \n    8.5 \n    B \n    dark \n  \n  \n    11 \n    9.1 \n    C \n    dark \n  \n  \n    12 \n    9.0 \n    C \n    dark"
  },
  {
    "objectID": "slides/07-anova.html#linear-model-and-anova",
    "href": "slides/07-anova.html#linear-model-and-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Linear model and ANOVA",
    "text": "Linear model and ANOVA\n\nANOVA\n\nm <- lm(growth ~ coat * diet, data = hams)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: growth\n          Df  Sum Sq Mean Sq F value  Pr(>F)  \ncoat       1 2.61333 2.61333  7.2258 0.03614 *\ndiet       2 2.66000 1.33000  3.6774 0.09069 .\ncoat:diet  2 0.68667 0.34333  0.9493 0.43833  \nResiduals  6 2.17000 0.36167                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-anova.html#interaction-plot",
    "href": "slides/07-anova.html#interaction-plot",
    "title": "07-One and two-way ANOVA",
    "section": "Interaction plot",
    "text": "Interaction plot\n\nwith(hams, interaction.plot(diet, coat, growth, \n                            col = c(\"brown\", \"orange\"), lty = 1, lwd = 2))"
  },
  {
    "objectID": "slides/07-anova.html#diagnostics",
    "href": "slides/07-anova.html#diagnostics",
    "title": "07-One and two-way ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\nAssumptions\n\nIndependence of measurements (within samples)\nVariance homogeneity of residuals\nNormal distribution of residuals\n\n Test of assumptions needs residuals of the fitted model. \\(\\Rightarrow\\) Fit the ANOVA model first, then check if it was correct!\n\nDiagnostic tools\n\nBox plot\nPlot of residuals vs. mean values\nQ-Q-plot of residuals\nFligner-Killeen test (alternative: some people recommend the Levene-Test)"
  },
  {
    "objectID": "slides/07-anova.html#diagnostics-ii",
    "href": "slides/07-anova.html#diagnostics-ii",
    "title": "07-One and two-way ANOVA",
    "section": "Diagnostics II",
    "text": "Diagnostics II\n\npar(mfrow=c(1, 2))\npar(cex=1.2, las=1)\nqqnorm(residuals(m))\nqqline(residuals(m))\n\nplot(residuals(m)~fitted(m))\nabline(h=0)\n\n\n\nfligner.test(growth ~ interaction(coat, diet), data=hams)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  growth by interaction(coat, diet)\nFligner-Killeen:med chi-squared = 10.788, df = 5, p-value = 0.05575\n\n\nResiduals: look ok and p-value of the Fligner test \\(< 0.05\\), \\(\\rightarrow\\) looks fine."
  },
  {
    "objectID": "slides/07-anova.html#notes",
    "href": "slides/07-anova.html#notes",
    "title": "07-One and two-way ANOVA",
    "section": "Notes",
    "text": "Notes\nLinear regression or ANOVA?\n\nessentially the same\nindependent variables are metric: linear model\nindependent variables are nominal (= factor): ANOVA\nmix of metric and nominal variables: ANCOVA\n\nUse of pre-tests\nPre-tests are in general questionable for theoretical reasons:\n\nThe null hypotheses \\(H_0\\) can only be rejected and not ultimately confirmed.\nIf the sample size is large, normality of residuals is not required\nIf \\(p\\) is close to the threshold and sample size is small, we would be left in uncertainty.\n\nAll this can only be overcome with careful thinking and some experience.\nIt is always a good idea to discuss results with colleagues and supervisors."
  },
  {
    "objectID": "slides/07-anova.html#sequential-holm-bonferroni-method",
    "href": "slides/07-anova.html#sequential-holm-bonferroni-method",
    "title": "07-One and two-way ANOVA",
    "section": "Sequential Holm-Bonferroni method",
    "text": "Sequential Holm-Bonferroni method\n\nAlso called Holm procedure (Holm, 1979)\nEasy to use\nCan be applied to any multiple test problem\nLess conservative that ordinary Bonferroni correction, but …\n… still a very conservative approach\nsee also Wikipedia\n\nAlgorithm\n\nSelect smallest \\(p\\) out of all \\(n\\) \\(p\\)-values\nIf \\(p \\cdot n < \\alpha\\) \\(\\Rightarrow\\) significant, else STOP\nSet \\(n − 1 \\rightarrow n\\), remove smallest \\(p\\) from the list and go to step 1."
  },
  {
    "objectID": "slides/07-anova.html#example-1",
    "href": "slides/07-anova.html#example-1",
    "title": "07-One and two-way ANOVA",
    "section": "Example",
    "text": "Example\nGrowth rate per day (\\(d^{-1}\\)) of blue-green algae cultures (Pseudanabaena) after adding toxic peptides from another blue-green algae (Microcystis).\nThe original hypothesis was that Microcystin LR (MCYST) or a derivative of it (Substance A) inhibits growth.\n\nmcyst <-  data.frame(treat = factor(c(rep(\"Control\", 5),\n                                       rep(\"MCYST\", 5),\n                                       rep(\"Subst A\", 5)),\n                                levels=c(\"Control\", \"MCYST\", \"Subst A\")),\n                      mu   = c(0.086, 0.101, 0.086, 0.086, 0.099,\n                               0.092, 0.088, 0.093, 0.088, 0.086,\n                               0.095, 0.102, 0.106, 0.106, 0.106)\n                     )"
  },
  {
    "objectID": "slides/07-anova.html#approach-1-one-way-anova",
    "href": "slides/07-anova.html#approach-1-one-way-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Approach 1: one-way ANOVA",
    "text": "Approach 1: one-way ANOVA\n\npar(mar=c(4, 8, 2, 1), las=1)\nm <- lm(mu ~ treat, data=mcyst)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: mu\n          Df     Sum Sq    Mean Sq F value   Pr(>F)   \ntreat      2 0.00053293 2.6647e-04   8.775 0.004485 **\nResiduals 12 0.00036440 3.0367e-05                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(TukeyHSD(aov(m)))"
  },
  {
    "objectID": "slides/07-anova.html#approach-2-multiple-t-tests-with-sequential-bonferroni-correction",
    "href": "slides/07-anova.html#approach-2-multiple-t-tests-with-sequential-bonferroni-correction",
    "title": "07-One and two-way ANOVA",
    "section": "Approach 2: multiple t-Tests with sequential Bonferroni correction",
    "text": "Approach 2: multiple t-Tests with sequential Bonferroni correction\nWe separate the data set in single subsets:\n\nControl <- mcyst$mu[mcyst$treat == \"Control\"]\nMCYST   <- mcyst$mu[mcyst$treat == \"MCYST\"]\nSubstA  <- mcyst$mu[mcyst$treat == \"Subst A\"]\n\nand perform 3 t-Tests:\n\np1 <- t.test(Control, MCYST)$p.value\np2 <- t.test(Control, SubstA)$p.value\np3 <- t.test(MCYST, SubstA)$p.value\n\nThe following shows the raw p-values without correction:\n\nc(p1, p2, p3)\n\n[1] 0.576275261 0.027378832 0.001190592\n\n\nand with Holm correction:\n\np.adjust(c(p1, p2, p3))\n\n[1] 0.576275261 0.054757664 0.003571775"
  },
  {
    "objectID": "slides/07-anova.html#conclusions",
    "href": "slides/07-anova.html#conclusions",
    "title": "07-One and two-way ANOVA",
    "section": "Conclusions",
    "text": "Conclusions\nStatistical methods\n\nIn case of Holm-corrected t-tests, only a single p-value (MCYST vs. Subst A) remains significant. This indicates that in this case, Holm’s method is more conservative than TukeyHSD (only one compared to two significant) effects.\nAn ANOVA with posthoc test is in general preferred,\nbut the sequential Holm-Bonferroni can be helpful in special cases.\nMoreover, it demonstrates clearly that massive multiple testing needs to be avoided.\n\n\\(\\Rightarrow\\) ANOVA is to be preferred, when possible.\nInterpretation\n\nRegarding our original hypothesis, we can see that MCYST and SubstA did not inhibit growth of Pseudanabaena. In fact SubstA stimulated growth.\nThis was contrary to our expectations – the biological reason was then found 10 years later.\n\nMore about this can be found in Jähnichen et al. (2001), Jähnichen et al. (2007), Jähnichen et al. (2011), Zilliges et al. (2011) or Dziallas & Grossart (2011)."
  },
  {
    "objectID": "slides/07-anova.html#ancova",
    "href": "slides/07-anova.html#ancova",
    "title": "07-One and two-way ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\nStatistical question\n\nComparison of regression lines\nSimilar to ANOVA, but contains also metric variables (covariates)\n\nExample\nAnnette Dobson’s birthweight data. A data set from a statistics textbook (Dobson, 2013), birth weight of boys and girls in dependence of the pregnancy week."
  },
  {
    "objectID": "slides/07-anova.html#the-birthweight-data-set",
    "href": "slides/07-anova.html#the-birthweight-data-set",
    "title": "07-One and two-way ANOVA",
    "section": "The birthweight data set",
    "text": "The birthweight data set\nThe data set is found at different places on the internet and in different versions.\nHere the version that is found in an R demo: demo(lm.glm)\n\n## Birth Weight Data see stats/demo/lm.glm.R\ndobson <- data.frame(\n  week = c(40, 38, 40, 35, 36, 37, 41, 40, 37, 38, 40, 38,\n     40, 36, 40, 38, 42, 39, 40, 37, 36, 38, 39, 40),\n  weight = c(2968, 2795, 3163, 2925, 2625, 2847, 3292, 3473, 2628, 3176,\n        3421, 2975, 3317, 2729, 2935, 2754, 3210, 2817, 3126, 2539,\n        2412, 2991, 2875, 3231),\n  sex = gl(2, 12, labels=c(\"M\", \"F\"))\n)"
  },
  {
    "objectID": "slides/07-anova.html#anette-dobsons-birthweight-data",
    "href": "slides/07-anova.html#anette-dobsons-birthweight-data",
    "title": "07-One and two-way ANOVA",
    "section": "Anette Dobson’s birthweight data",
    "text": "Anette Dobson’s birthweight data\nWhy not just using a t-test?\n\nboxplot(weight ~ sex,data=dobson, ylab=\"weight\")\n\nt.test(weight ~ sex, data=dobson, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by sex\nt = 0.97747, df = 22, p-value = 0.339\nalternative hypothesis: true difference in means between group M and group F is not equal to 0\n95 percent confidence interval:\n -126.3753  351.7086\nsample estimates:\nmean in group M mean in group F \n       3024.000        2911.333 \n\n\nThe box plot shows much overlap and the difference is not significant, because the t-test ignores important information: the pregnancy week."
  },
  {
    "objectID": "slides/07-anova.html#ancova-makes-use-of-covariates",
    "href": "slides/07-anova.html#ancova-makes-use-of-covariates",
    "title": "07-One and two-way ANOVA",
    "section": "ANCOVA makes use of covariates",
    "text": "ANCOVA makes use of covariates\n\nm <- lm(weight ~ week * sex, data = dobson)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df  Sum Sq Mean Sq F value    Pr(>F)    \nweek       1 1013799 1013799 31.0779 1.862e-05 ***\nsex        1  157304  157304  4.8221   0.04006 *  \nweek:sex   1    6346    6346  0.1945   0.66389    \nResiduals 20  652425   32621                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-anova.html#pitfalls-of-anova-and-ancova-described-so-far",
    "href": "slides/07-anova.html#pitfalls-of-anova-and-ancova-described-so-far",
    "title": "07-One and two-way ANOVA",
    "section": "Pitfalls of ANOVA and ANCOVA described so far",
    "text": "Pitfalls of ANOVA and ANCOVA described so far\n\n\nHeterogeneity of variance\n\np-values can be biased (i.e. misleading or wrong)\nuse of a one-way ANOVA for uneaqual variances (in R: oneway.test)\n\nUnbalanced case: unequal number of samples for each factor combination \\(\\rightarrow\\) ANOVA results depend on the order of factors in the model formula.\n\nClassical method: Type II or Type III ANOVA\nModern approach: model selection and likelihood ratio tests"
  },
  {
    "objectID": "slides/07-anova.html#type-ii-and-type-iii-anova",
    "href": "slides/07-anova.html#type-ii-and-type-iii-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Type II and type III ANOVA",
    "text": "Type II and type III ANOVA\n\n\nfunction Anova (with upper case A) in package car\nHelp file of function Anova:\n\n\n“Type-II tests are calculated according to the principle of marginality, testing each term after all others, except ignoring the term’s higher-order relatives; so-called type-III tests violate marginality, testing each term in the model after all of the others.”\n\n\nConclusion: use Type II and don’t try to interpret single terms in case of significant interactions."
  },
  {
    "objectID": "slides/07-anova.html#type-ii-anova-example",
    "href": "slides/07-anova.html#type-ii-anova-example",
    "title": "07-One and two-way ANOVA",
    "section": "Type II ANOVA: Example",
    "text": "Type II ANOVA: Example\n\n\nlibrary(\"car\")\nm <- lm(growth ~ coat * diet, data = hams)\nAnova(m, type=\"II\")\n\nAnova Table (Type II tests)\n\nResponse: growth\n           Sum Sq Df F value  Pr(>F)  \ncoat      2.61333  1  7.2258 0.03614 *\ndiet      2.66000  2  3.6774 0.09069 .\ncoat:diet 0.68667  2  0.9493 0.43833  \nResiduals 2.17000  6                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "slides/07-anova.html#model-selection-a-paradigm-change",
    "href": "slides/07-anova.html#model-selection-a-paradigm-change",
    "title": "07-One and two-way ANOVA",
    "section": "Model selection – a paradigm change",
    "text": "Model selection – a paradigm change\n\nProblem:\n\nIn complicated models, p-values depend on number (and sometimes of order) of included factors and interactions.\nThe \\(H_0\\)-based approach becomes confusing, e.g. because of contradictory p-values.\n\nAlternative approach:\nComparison of different model candidates instead of p-value based testing.\n\nModel with all potentiall effects → full model,\nOmit single factors → reduced models (several!),\nNo influence factors (ony mean value) → null model.\nWhich model is the best → minimal adequate model?"
  },
  {
    "objectID": "slides/07-anova.html#how-can-we-measure-which-model-is-the-best",
    "href": "slides/07-anova.html#how-can-we-measure-which-model-is-the-best",
    "title": "07-One and two-way ANOVA",
    "section": "How can we measure which model is the best?",
    "text": "How can we measure which model is the best?\n\nCompromise between model fit and model complexity (number of parameters, k).\n\nGoodness of fit: Likelihood L (measures how good the data match a given model).\nLog Likelihood: makes the criterion additive.\nAIC (Akaike Information Criterion):\n\n\\[AIC = −2 \\ln(L) + 2k\\]\n\nBIC (Bayesian Information Criterion), takes sample size into account (\\(n\\)):\n\n\\[BIC = −2 \\ln(L) + k · \\ln(n)\\]\nThe model with the smallest AIC (or BIC) is the minimal adequate (i.e. optimal) model."
  },
  {
    "objectID": "slides/07-anova.html#model-selection-and-likelihood-ratio-tests",
    "href": "slides/07-anova.html#model-selection-and-likelihood-ratio-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Model Selection and Likelihood Ratio Tests",
    "text": "Model Selection and Likelihood Ratio Tests\nApproach\n\nFit several models individually\nCompare the models pairwise with ANOVA (likelihood ratio test)\n\nData and example\n\nhams <- data.frame(No=1:12,\n                   growth=c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,\n                            8.3, 8.7, 8.1, 8.5, 9.1, 9.0),\n                   diet= rep(c(\"A\", \"B\", \"C\"), each=2),\n                   coat= rep(c(\"light\", \"dark\"), each=6)\n                   )\n\n\nm1 <- lm(growth ~ diet * coat, data=hams)\nm2 <- lm(growth ~ diet + coat, data=hams)\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: growth ~ diet * coat\nModel 2: growth ~ diet + coat\n  Res.Df    RSS Df Sum of Sq      F Pr(>F)\n1      6 2.1700                           \n2      8 2.8567 -2  -0.68667 0.9493 0.4383\n\n\n\nLikelihood ratio test compares two models (anova with > 1 model)\nModel with interaction (m1) not significantly better than model without interaction (m2)."
  },
  {
    "objectID": "slides/07-anova.html#aic-based-model-selection",
    "href": "slides/07-anova.html#aic-based-model-selection",
    "title": "07-One and two-way ANOVA",
    "section": "AIC-based model selection",
    "text": "AIC-based model selection\n\nPairwise model comparison is cumbersome, especially for large number of models.\nSolution\nUse Akaike Information Criterion and select the optimal (or: minimal adequate) model.\n\nAIC(m1, m2)\n\n   df      AIC\nm1  7 27.53237\nm2  5 26.83151\n\n\nConclusion: take the simpler model (m2)\nExercise: Generate all possible models and select the model with minimum AIC."
  },
  {
    "objectID": "slides/07-anova.html#automatic-model-selection",
    "href": "slides/07-anova.html#automatic-model-selection",
    "title": "07-One and two-way ANOVA",
    "section": "Automatic Model Selection",
    "text": "Automatic Model Selection\n\nThe full model is supplied to the step function.\nThe model with the smallest AIC ist the optimal model:\n\n\n\nm1 <- lm(growth ~ diet * coat, data=hams)\nopt <- step(m1)\n\nStart:  AIC=-8.52\ngrowth ~ diet * coat\n\n            Df Sum of Sq    RSS     AIC\n- diet:coat  2   0.68667 2.8567 -9.2230\n<none>                   2.1700 -8.5222\n\nStep:  AIC=-9.22\ngrowth ~ diet + coat\n\n       Df Sum of Sq    RSS     AIC\n<none>              2.8567 -9.2230\n- diet  2    2.6600 5.5167 -5.3256\n- coat  1    2.6133 5.4700 -3.4275\n\n\n\n\n\nsummary(opt)\n\n\nCall:\nlm(formula = growth ~ diet + coat, data = hams)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6333 -0.3458 -0.1000  0.2333  0.8667 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   8.1667     0.3450  23.671 1.08e-08 ***\ndietB         0.2500     0.4225   0.592   0.5704    \ndietC         1.1000     0.4225   2.603   0.0315 *  \ncoatlight    -0.9333     0.3450  -2.705   0.0269 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5976 on 8 degrees of freedom\nMultiple R-squared:  0.6486,    Adjusted R-squared:  0.5169 \nF-statistic: 4.923 on 3 and 8 DF,  p-value: 0.03178"
  },
  {
    "objectID": "slides/07-anova.html#summary",
    "href": "slides/07-anova.html#summary",
    "title": "07-One and two-way ANOVA",
    "section": "Summary",
    "text": "Summary\n\nLinear models form the basis of many statistical methods\n\nLinear regression\nANOVA, ANCOVA, GLM, GAM, GLMM, . . .\nANOVA/ANCOVA instead of multiple testing\n\nANOVA is more powerful than multiple tests:\n\none big experiment needs less n than many small experiments together\nidentification of interaction effects\nelimination of co-variates\n\nModel selection vs. p-value based testing\n\nparadigm shift in statistics: AIC instead of p-value\nmore reliable, especially for imbalanced or complex designs\nbut: p-value based tests are sometimes easier to understand"
  },
  {
    "objectID": "slides/07-anova.html#five-ways-to-fix-statistics.-comment-on-nature",
    "href": "slides/07-anova.html#five-ways-to-fix-statistics.-comment-on-nature",
    "title": "07-One and two-way ANOVA",
    "section": "Five ways to fix statistics. Comment on Nature",
    "text": "Five ways to fix statistics. Comment on Nature\n\nLeek et al. (2017) https://doi.org/10.1038/d41586-017-07522-z\n\nJeff Leek: Adjust for human cognition\nBlakeley B. McShane & Andrew Gelman: Abandon statistical significance\nDavid Colquhoun: State false-positive risk, too\nMichèle B. Nuijten: Share analysis plans and results\nSteven N. Goodman: Change norms from within\n\nAnother blog post that aims to improve understanding: http://daniellakens.blogspot.de/2017/12/understanding-common-misconceptions.html?m=1\n\n\nMy conclusion: The p-value is still useful but apply it with great care."
  },
  {
    "objectID": "slides/07-anova.html#bibliography",
    "href": "slides/07-anova.html#bibliography",
    "title": "07-One and two-way ANOVA",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\n\n\nCrawley, M. J. (2002). Statistical computing. An introduction to data analysis using S-PLUS (pp. 1–761). Wiley. datasets: http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/\n\n\nDobson, A. J. (2013). Introduction to statistical modelling. Springer.\n\n\nDziallas, C., & Grossart, H.-P. (2011). Increasing Oxygen Radicals and Water Temperature Select for Toxic Microcystis sp. PLoS ONE, 6(9), e25569. https://doi.org/10.1371/journal.pone.0025569\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 65–70. https://www.jstor.org/stable/4615733\n\n\nJähnichen, S., Ihle, T., Petzoldt, T., & Benndorf, J. (2007). Impact of Inorganic Carbon Availability on Microcystin Production by Microcystis aeruginosa PCC 7806. Applied and Environmental Microbiology, 73(21), 6994–7002. https://doi.org/10.1128/AEM.01253-07\n\n\nJähnichen, S., Long, B. M., & Petzoldt, T. (2011). Microcystin production by Microcystis aeruginosa: Direct regulation by multiple environmental factors. Harmful Algae, 12, 95–104. https://doi.org/10.1016/j.hal.2011.09.002\n\n\nJähnichen, S., Petzoldt, T., & Benndorf, J. (2001). Evidence for control of microcystin dynamics in Bautzen Reservoir (Germany) by cyanobacterial population growth rates and dissolved inorganic carbon. Fundamental and Applied Limnology, 150(2), 177–196. https://doi.org/10.1127/archiv-hydrobiol/150/2001/177\n\n\nJohnson, G., Jerald, & Omland, K. S. (2004). Model Selection in Ecology and Evolution. Trends in Ecology and Evolution, 19(2), 101–108. https://doi.org/10.1016/j.tree.2003.10.013\n\n\nZilliges, Y., Kehr, J.-C., Meissner, S., Ishida, K., Mikkat, S., Hagemann, M., Kaplan, A., Börner, T., & Dittmann, E. (2011). The Cyanobacterial Hepatotoxin Microcystin Binds to Proteins and Increases the Fitness of Microcystis under Oxidative Stress Conditions. PLoS ONE, 6(3), e17615. https://doi.org/10.1371/journal.pone.0017615"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#nonlinear-regression",
    "href": "slides/08-nonlinear-regression.html#nonlinear-regression",
    "title": "08-Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\nMany phenomena in nature are non-linear!\nLinear or non-linear?\n\nSome non-linear problems can be solved with linear methods\ne.g., polynomials or periodic (sine and cosine) functions\nothers can be fitted by using transformations\n\nExample\n\\[y = a \\cdot x^b\\] can be transformed to:\n\\[\\ln(y) = \\ln(a) + b \\cdot \\ln(x)\\]\n\nbut: linearization transforms also the residuals\ntransformation is sometimes correct and sometimes wrong\nhomogeneity of variances"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#linearization-or-direct-nonlinear-fitting",
    "href": "slides/08-nonlinear-regression.html#linearization-or-direct-nonlinear-fitting",
    "title": "08-Nonlinear Regression",
    "section": "Linearization or direct nonlinear fitting?",
    "text": "Linearization or direct nonlinear fitting?\n\nLinearising transformations\n\nlog, square root, reciprocals …\ncan be applied if the residual variance remains (or is becoming) homogenous.\nbut in some cases: transformations lead to heteroscedasticity \\(\\Rightarrow\\) biased regression parameters.\n\nNonlinear regression\n\nvery flexible, user-defined functions,\nno transformation required\nbut: requires iterative optimization methods,\nin theory only local optima can be found,\nparameters are not in all cases identifiable."
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#nonlinear-regression-1",
    "href": "slides/08-nonlinear-regression.html#nonlinear-regression-1",
    "title": "08-Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nIterative search for the minimum of the sum of squares"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#how-to-find-the-global-minimum",
    "href": "slides/08-nonlinear-regression.html#how-to-find-the-global-minimum",
    "title": "08-Nonlinear Regression",
    "section": "How to find the global minimum?",
    "text": "How to find the global minimum?\n\nGoodness of fit: Sum of least squares \\(SQ\\):\n\\[\nSQ = \\sum_{i=1}^n\\left(y_i- f(\\mathbf x_i, \\mathbf p)\\right)^2 = \\min !\n\\]\nwith \\(y\\): dependent var, \\(\\mathbf x\\): matrix of independent vars,\\(\\mathbf p\\): parameter vector, \\(n\\): sample size\nUse of an iterative solver \\(\\rightarrow\\) see next slides\nNonlinear coefficient of determination \\(R^2\\)\n\nnot related to the (linear) correlation coefficient\ncan be calculated from the residual and total variances\n\n\\[\nR^2 = 1 - \\frac{\\text{variance of the residuals}}{\\text{variance of the y-data}} = 1- \\frac{s^2_\\varepsilon}{s^2_y}\n\\]\n\nnonlinear \\(r^2\\) measures the fraction of the explained variance\n… other indices can be used in addition, e.g. MSE, RMSE, NSE, …"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#general-principle-of-optimization-algorithms",
    "href": "slides/08-nonlinear-regression.html#general-principle-of-optimization-algorithms",
    "title": "08-Nonlinear Regression",
    "section": "General principle of optimization algorithms",
    "text": "General principle of optimization algorithms\n\nThe minimum of the squared residuals (SQ) is searched by iteration:\n\n\nStart from an initial guess for the parameter set.\nTry to find a parameter set with lower SQ.\nIs the new parameter set better?\n\nNo: Reject the new parameters and goto 2\nYes: Accept the new parameters and goto 4\n\nIs the new parameter set good enough?\n\nNo: goto 2\nYes: goto 5\n\nParameter set found."
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#deterministic-methods",
    "href": "slides/08-nonlinear-regression.html#deterministic-methods",
    "title": "08-Nonlinear Regression",
    "section": "Deterministic Methods",
    "text": "Deterministic Methods\n\n\nGradient Descent\n\ngo one step into the direction of steepest descent\n\\(\\rightarrow\\) relatively simple\n\\(\\rightarrow\\) relatively robust for “more complicated’’ problems\n\nNewton’s Method\n\nquadratic approximation of the SQ function\ntry to estimate the minimum\n\\(\\rightarrow\\) takes curvature into account\n\\(\\rightarrow\\) faster for “well behaving” problems\nseveral versions: quasi-Newton, Gauss-Newton, L-BFGS, …\n\nLevenberg-Marquardt\n\ninterpolates between Gauss-Newton and gradient descent.\n\n\n\n\nNewton method (red) uses curvature information to converge faster than gradient descent (green). Source: Wikipedia."
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#stochastic-methods",
    "href": "slides/08-nonlinear-regression.html#stochastic-methods",
    "title": "08-Nonlinear Regression",
    "section": "Stochastic Methods",
    "text": "Stochastic Methods\n\nUse statistical principles (random search)\nClassical methods\n\nSimulated annealing: simulates heating and cooling of matter \\(\\rightarrow\\) “Crystallisation process”.\nControlled random search Price (1983)\n\nEvolutionary Algorithms\n\nanalogy to genetics: mutation and selection\nfollows a “population” of several parameter sets in parallel\ninformation exchange (“crossover”) between parameter sets\n\\(\\rightarrow\\) for complicated problems with large number of parameters\nnowadays builtin in Microsoft Excel and LibreOffice Calc\n\n… and many more."
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#enzyme-kinetics",
    "href": "slides/08-nonlinear-regression.html#enzyme-kinetics",
    "title": "08-Nonlinear Regression",
    "section": "Enzyme Kinetics",
    "text": "Enzyme Kinetics\n\n… can be described with the well-known Michaelis-Menten function:\n\\[\nv = v_{max} \\frac{S}{k_m + S}\n\\]"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#linearization-vs.-true-nonlinear-regression",
    "href": "slides/08-nonlinear-regression.html#linearization-vs.-true-nonlinear-regression",
    "title": "08-Nonlinear Regression",
    "section": "Linearization vs. (true) nonlinear regression",
    "text": "Linearization vs. (true) nonlinear regression\n\nLinearizing transformation\n[>] Appropriate if transformation improves homogeneity of variances  [+] Fast, simple and easy. [+] Analytical solution returns the global optimum. [-] Only a limited set of functions can be fitted. [-] Can lead to wrongly transformed error structure and biased results.\nNonlinear Regression\n[>] Appropriate if error structure is already homogeneous and/or analytical solution does not exist. [+] Can be used to fit arbitrary functions, given that the parameters are identifiable. [-] Needs start values and considerable computation time. [-] Best solution (global optimum) is not guaranteed."
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#nonlinear-regression-in-r-simple-exponential",
    "href": "slides/08-nonlinear-regression.html#nonlinear-regression-in-r-simple-exponential",
    "title": "08-Nonlinear Regression",
    "section": "Nonlinear regression in R: simple exponential",
    "text": "Nonlinear regression in R: simple exponential\n\nCode and Plot\n\n# example data\nx <- 1:10\ny <- c(1.6, 1.8, 2.1, 2.8, 3.5, 4.1, 5.1, 5.8, 7.1, 9.0)\n\n# initial parameters for the optimizer\npstart <- c(a = 1, b = 1)\n\n# nonlinear least squares\nfit <- nls(y ~ a * exp(b * x), start = pstart)\n\n# additional x-values to get a smooth curve\nx1 <- seq(1, 10, 0.1)\ny1 <- predict(fit, data.frame(x = x1))\n\nplot(x, y)\nlines(x1, y1, col = \"red\")\n\n\n\n\n\n\n\n\n\nFitted parameters\n\n\n\nFormula: y ~ a * exp(b * x)\n\nParameters:\n  Estimate Std. Error t value Pr(>|t|)    \na 1.263586   0.049902   25.32 6.34e-09 ***\nb 0.194659   0.004716   41.27 1.31e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1525 on 8 degrees of freedom\n\nNumber of iterations to convergence: 13 \nAchieved convergence tolerance: 5.956e-08\n\n\n\n“Estimate”: the fitted parameters\nStr. error: \\(s_{\\bar{x}}\\), indicates reliability of the estimate\nt- and p-values: can be misleading in the nonlinear case because “non-significant” parameters can be structurally necessary\n\nCoefficient of determination \\(r^2 = 1-\\frac{s_\\varepsilon}{s_y}\\)\n\n(Rsquared <- 1 - var(residuals(fit))/var(y))\n\n[1] 0.9965644"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics",
    "href": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics",
    "title": "08-Nonlinear Regression",
    "section": "Michaelis-Menten-Kinetics",
    "text": "Michaelis-Menten-Kinetics\n\n\nCode\n\nS <-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)\nV <-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,\n      0.0575, 0.0399, 0.0381, 0.017, 0.0253)\n\n## Michaelis-Menten kinetics\nf <- function(S, Vm, K) { Vm * S/(K + S) }\n\npstart <- c(Vm = 0.1, K = 5)\nmodel_fit <- nls(V ~ f(S, Vm, K), start = pstart)\nsummary(model_fit)\n\nplot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))\nx1 <-seq(0, 25, length = 100)\nlines(x1, predict(model_fit, data.frame(S = x1)), col = \"red\")\n\n Coefficient of determination\n\n(1 - var(residuals(model_fit))/var(V)) # nonlinear r2\n\n[1] 0.9895147\n\n\n\n\n\n\n\nResults\n\n\n\nFormula: V ~ f(S, Vm, K)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nVm  0.11713    0.00381   30.74 1.36e-09 ***\nK   5.38277    0.46780   11.51 2.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.003053 on 8 degrees of freedom\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 6.678e-06"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics-with-selfstart",
    "href": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics-with-selfstart",
    "title": "08-Nonlinear Regression",
    "section": "Michaelis-Menten-Kinetics with selfstart",
    "text": "Michaelis-Menten-Kinetics with selfstart\n\nFunction SSmicmen determines start parameters automatically.\nOnly few selfstart functions available in R\n\n\nCode\n\n\nS <- c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)\nV <- c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,\n       0.0575, 0.0399, 0.0381, 0.017, 0.0253)\n\nmodel_fit <- nls(V ~ SSmicmen(S, Vm, K))\n\nplot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))\nx1 <-seq(0, 25, length = 100)\nlines(x1, predict(model_fit, data.frame(S = x1)), col=\"red\")\n\n\nResults\n\n\nsummary(model_fit, correlation=TRUE)\n\n\nFormula: V ~ f(S, Vm, K)\n\nParameters:\n   Estimate Std. Error t value Pr(>|t|)    \nVm  0.11713    0.00381   30.74 1.36e-09 ***\nK   5.38277    0.46780   11.51 2.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.003053 on 8 degrees of freedom\n\nCorrelation of Parameter Estimates:\n  Vm  \nK 0.88\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 6.678e-06\n\n\n\n\n\n\n\n\nPlot\n\n\n\n\n\n\nNote: Correlation of parameters\n\nhigh absolute values of correlation indicate non-identifiability of parameters\ncritical value depends on the data\nsometimes, better start values or another optimization algorithm can help"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#practical-hints",
    "href": "slides/08-nonlinear-regression.html#practical-hints",
    "title": "08-Nonlinear Regression",
    "section": "Practical hints",
    "text": "Practical hints\n\n\nplot data\nfind good starting values by thinking about it or by trial and error\navoid very small and/or very large numbers \\(\\longrightarrow\\) rescale the problem to values between approx 0.001 to 1000\nstart with a simple function and add terms and parameters sequentially\nDon’t take significance of parameters too seriously. A non-significant parameter may be necessary for the structure of the model, removal of it will invalidate the whole model."
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#further-reading",
    "href": "slides/08-nonlinear-regression.html#further-reading",
    "title": "08-Nonlinear Regression",
    "section": "Further reading",
    "text": "Further reading\n\n\nPackage growthrates for growth curves: https://cran.r-project.org/package=growthrates\nPackage FME for more complex model fitting tasks (identifiability analysis, constrained optimization, multiple dependent variables and MCMC): (Soetaert & Petzoldt, 2010), https://cran.r-project.org/package=FME\nMore about optimization in R: https://cran.r-project.org/web/views/Optimization.html"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#lineweaver-burk-transformation-vs.-nonlinear-fit",
    "href": "slides/08-nonlinear-regression.html#lineweaver-burk-transformation-vs.-nonlinear-fit",
    "title": "08-Nonlinear Regression",
    "section": "Lineweaver-Burk transformation vs. nonlinear fit",
    "text": "Lineweaver-Burk transformation vs. nonlinear fit\n\nCode for the comparison slide\n\nS <-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)\nV <-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557, \n      0.0575, 0.0399, 0.0381, 0.017, 0.0253)\nmodel_fit<-nls(V ~ SSmicmen(S, Vm, K))\n\npar(mfrow=c(1,2), las=1, lwd=2)\n## Lineweaver-Burk\nx <- 1/S; y <- 1/V\n\nplot(x, y, xlab=\"1/S\", ylab=\"1/V\", xlim=c(-0.2,1), ylim=c(0, 80), pch=16,\n  main=\"Linearisation\\n(Lineweaver-Burk)\")\nabline(h=0, lwd=1, col=\"grey\")\nabline(v=0, lwd=1, col=\"grey\")\nm <- lm(y ~ x)\nabline(m, col = \"red\")\nVm_l <- 1/coef(m)[1]\nKm_l <- coef(m)[2] * Vm_l\n#legend(\"topleft\", c(\"vmax = 1/intercept\", \"km = slope * vmax\"), \n#        box.lwd=1, bg=\"white\")\ntext(0.35, 75, \"1/V = 1/vmax + km / vmax * S\")\n\n## retransformed, both together\nplot(S, V, xlim = c(0, max(S)),ylim=c(0, max(V)), pch=16, main=\"No Transformation\")\nx1 <-seq(0, 25, length=100)\nlines(x1, Vm_l * x1 / (Km_l + x1), col=\"red\")\nlines(x1, predict(model_fit, data.frame(S=x1)), col=\"blue\")\nlegend(\"bottomright\", legend=c(\"linearisation\", \"nonlinear\"), \n       box.lwd=1, lwd=2, col=c(\"red\", \"blue\"))\ntext(15, 0.04, \"V = S * vmax / (km + S)\")\n\nSee: https://en.wikipedia.org/wiki/Lineweaver-Burk_plot"
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#references",
    "href": "slides/08-nonlinear-regression.html#references",
    "title": "08-Nonlinear Regression",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nPrice, W. L. (1977). A controlled random search procedure for global optimization. The Computer Journal, 20(4), 367–370.\n\n\nPrice, W. L. (1983). Global optimization by controlled random search. Journal of Optimization Theory and Applications, 40(3), 333–348.\n\n\nSoetaert, K., & Petzoldt, T. (2010). Inverse modelling, sensitivity and monte carlo analysis in R using package FME. Journal of Statistical Software, 33(3), 1–28. https://doi.org/10.18637/jss.v033.i03"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#time-series-analysis",
    "href": "slides/09-timeseries-basics.html#time-series-analysis",
    "title": "09-Time Series Basics",
    "section": "Time series analysis",
    "text": "Time series analysis\n\n\nDeals with development of processes in time: \\(x(t)\\),\nHuge number of specific approaches, only a few examples can be given here.\n\nAims of time series analysis\n\nDoes a trend exist? (significance)\nHow strong is a trend? (effect size)\nIdentification of covariates. (influencing factors)\nTrend, seasonality and random error. (component model)\nStatistical modeling and forecasting.\nBreakpoint analysis, intervention analysis, …, and more."
  },
  {
    "objectID": "slides/09-timeseries-basics.html#stationarity",
    "href": "slides/09-timeseries-basics.html#stationarity",
    "title": "09-Time Series Basics",
    "section": "Stationarity",
    "text": "Stationarity\n\n\nTime series methods have certain assumptions.\nOne of the most common assumptions is stationarity.\n\n\n\nNote: we are often not primarily interested in stationarity itself!\n\\(\\rightarrow\\) It is often just “the ticket” to go further.\n\n\nExample 1: trend analysis\nWe test stationarity to check if a trend test would be appropriate.\nExample 2: linear models\nEstimation of a linear trend or a breakpoint model. We check residuals for stationarity to get in formation about reliability of the fitted model."
  },
  {
    "objectID": "slides/09-timeseries-basics.html#stationarity-1",
    "href": "slides/09-timeseries-basics.html#stationarity-1",
    "title": "09-Time Series Basics",
    "section": "Stationarity",
    "text": "Stationarity\n\n… is a central concept in time series analysis.\n\nStrictly (or strong) stationary process: distribution of \\((x_{s+t})\\) independent from index \\(s\\).\nWeakly or wide-sense stationary processes: requires only that 1st and 2nd moments (mean, variance and covariance) do not vary with time."
  },
  {
    "objectID": "slides/09-timeseries-basics.html#two-different-basic-types-time-series",
    "href": "slides/09-timeseries-basics.html#two-different-basic-types-time-series",
    "title": "09-Time Series Basics",
    "section": "Two different basic types time series",
    "text": "Two different basic types time series\n\n\\[\\begin{align}\nx_t &= \\beta_0 + \\beta_1 t + u_t \\label{ts:trendstat} \\\\\nx_t &= x_{t-1} + c + u_t          \\label{ts:diffstat}\n\\end{align}\\]\n\n\n\n\n\nTSP: trend stationary process, linear regression model\n\n\ntime <- 1:100\nTSP <- 2 + 0.2 * time + rnorm(time)\n\n\nDSP: difference stationary process (random walk)\n\n\nDSP <- numeric(length(time))\nDSP[1] <- rnorm(1)\nfor (tt in time[-1])\n  DSP[tt] <- DSP[tt-1] + 0.2 + rnorm(1)"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#time-series-classes-in-r",
    "href": "slides/09-timeseries-basics.html#time-series-classes-in-r",
    "title": "09-Time Series Basics",
    "section": "Time series classes in R",
    "text": "Time series classes in R\n\nts for regularly spaced time series,\nzoo irregularly spaced time series.\n\n\npar(mfrow=c(1, 2), las=1)\nTSP <- ts(TSP)\nDSP <- ts(DSP)\n\nplot(TSP, main=\"trend stationary process\")\nplot(DSP, main=\"difference stationary process\")"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#a-simulation-experiment",
    "href": "slides/09-timeseries-basics.html#a-simulation-experiment",
    "title": "09-Time Series Basics",
    "section": "A Simulation Experiment",
    "text": "A Simulation Experiment\n\n\nDefine two functions to generate time series with specific properties:\n\n\ngenTSP <- function(time, beta0, beta1)\n  as.ts(beta0 + beta1 * time + rnorm(time))\n\ngenDSP <- function(time, c) {\n  DSP <- numeric(length(time))\n  DSP[1] <- rnorm(1)\n  for (tt in time[-1]) DSP[tt] <- DSP[tt-1] + c + rnorm(1)\n  as.ts(DSP)\n}"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#run-the-experiment",
    "href": "slides/09-timeseries-basics.html#run-the-experiment",
    "title": "09-Time Series Basics",
    "section": "Run the experiment",
    "text": "Run the experiment\n\nset the trend to zero, so count.signif counts a false positive results.\n\n\ncount.signif <- function(N, time, FUN, ...) {\n  a <- 0\n  for (i in 1:N) {\n    x <- FUN(time, ...)\n    m <- summary(lm(x  ~ time(x)))\n    f <- m$fstatistic\n    p.value <- pf(f[1], f[2], f[3], lower=FALSE)\n    # cat(\"p.value\", p.value, \"\\n\")\n    if (p.value < 0.05) a <- a + 1\n  }\n  a\n}\n\n\ntest for the number of significant F-values,\nrun loop 100 … 1000 times for both types of time series,\n\n\nNruns <- 100 # better 1000 !!!\ncount.signif(N=Nruns, time=time, FUN=genTSP, beta0=0, beta1=0) / Nruns\n\n[1] 0.05\n\ncount.signif(N=Nruns, time=time, FUN=genDSP, c=0) / Nruns\n\n[1] 0.91"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#autocorrelation-function-acf",
    "href": "slides/09-timeseries-basics.html#autocorrelation-function-acf",
    "title": "09-Time Series Basics",
    "section": "Autocorrelation function (ACF)",
    "text": "Autocorrelation function (ACF)\n\n… correlation between time series and its time-shifted (= lagged) versions,\nthe time shift (lag) is being varied \\(\\rightarrow\\) correlogram.\nindirect dependency, e.g. \\(x_{t}\\) on \\(x_{t-2}\\) because \\(x_{t-1}\\) depends on \\(x_{t-2}\\).\nwithout indirect effects it is called partial autocorrelation, PACF.\nCross-correlation (CCF): two variables; lag is positive or negative.\nExample: CCF between solar radiation and water temperature."
  },
  {
    "objectID": "slides/09-timeseries-basics.html#typical-patterns-of-acf-and-pacf",
    "href": "slides/09-timeseries-basics.html#typical-patterns-of-acf-and-pacf",
    "title": "09-Time Series Basics",
    "section": "Typical patterns of ACF and PACF",
    "text": "Typical patterns of ACF and PACF\n\n\nACF plots of TSP and DSP (left) very similar,\nDifferentiated time series (middle),\nseries with substracted trend (right)."
  },
  {
    "objectID": "slides/09-timeseries-basics.html#typical-patterns-of-acf-and-pacf-code",
    "href": "slides/09-timeseries-basics.html#typical-patterns-of-acf-and-pacf-code",
    "title": "09-Time Series Basics",
    "section": "Typical patterns of ACF and PACF (Code)",
    "text": "Typical patterns of ACF and PACF (Code)\n\n\npar(mfrow=c(2,3), cex=1)\nacf(TSP)\nacf(diff(TSP))\nacf(residuals(lm(TSP ~ time(TSP))))\nacf(DSP)\nacf(diff(DSP))\nacf(residuals(lm(DSP ~ time(DSP))))"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#unit-root-test",
    "href": "slides/09-timeseries-basics.html#unit-root-test",
    "title": "09-Time Series Basics",
    "section": "Unit root test",
    "text": "Unit root test\n\n\ndetermine whether a time series is of type “DSP”.\nADF-test (augmented Dickey-Fuller test), contained in R-package tseries:\nRemember: A “TSP” can be made stationary by subtracting a trend.\n\n\\(\\rightarrow\\) This is done automatically by the test.\n\nlibrary(tseries)\nadf.test(TSP)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  TSP\nDickey-Fuller = -4.0145, Lag order = 4, p-value = 0.01134\nalternative hypothesis: stationary\n\nadf.test(DSP)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  DSP\nDickey-Fuller = -2.4355, Lag order = 4, p-value = 0.3962\nalternative hypothesis: stationary"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#dsp-presence-of-unit-root",
    "href": "slides/09-timeseries-basics.html#dsp-presence-of-unit-root",
    "title": "09-Time Series Basics",
    "section": "DSP: presence of unit root",
    "text": "DSP: presence of unit root\n\n\nDSP series: presence of an unit root cannot be rejected,\n\\(\\rightarrow\\) non-stationary.\nBut, after differencing, there are no objections against stationarity:\n\n\nadf.test(diff(DSP))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(DSP)\nDickey-Fuller = -3.9902, Lag order = 4, p-value = 0.01261\nalternative hypothesis: stationary"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#kpss-test-kwiatkowski-phillips-schmidt-shin-test",
    "href": "slides/09-timeseries-basics.html#kpss-test-kwiatkowski-phillips-schmidt-shin-test",
    "title": "09-Time Series Basics",
    "section": "KPSS test: Kwiatkowski-Phillips-Schmidt-Shin test",
    "text": "KPSS test: Kwiatkowski-Phillips-Schmidt-Shin test\n\n… tests directly for stationarity or trend stationarity:\n\nkpss.test(TSP)                # non-stationary\n\n\n    KPSS Test for Level Stationarity\n\ndata:  TSP\nKPSS Level = 2.0842, Truncation lag parameter = 4, p-value = 0.01\n\nkpss.test(TSP, null=\"Trend\")  # stationary after trend removal\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  TSP\nKPSS Trend = 0.068349, Truncation lag parameter = 4, p-value = 0.1\n\n\n\n\ndetrending made the series stationary"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#kpss-test-kwiatkowski-phillips-schmidt-shin-test-ii",
    "href": "slides/09-timeseries-basics.html#kpss-test-kwiatkowski-phillips-schmidt-shin-test-ii",
    "title": "09-Time Series Basics",
    "section": "KPSS test: Kwiatkowski-Phillips-Schmidt-Shin test II",
    "text": "KPSS test: Kwiatkowski-Phillips-Schmidt-Shin test II\n\n\nkpss.test(DSP)                # non-stationary\n\n\n    KPSS Test for Level Stationarity\n\ndata:  DSP\nKPSS Level = 1.6951, Truncation lag parameter = 4, p-value = 0.01\n\nkpss.test(DSP, null=\"Trend\")  # still non-stationary\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  DSP\nKPSS Trend = 0.22627, Truncation lag parameter = 4, p-value = 0.01\n\n\n\n\ndetrending does not cure non-stationarity"
  },
  {
    "objectID": "slides/09-timeseries-basics.html#trend-tests",
    "href": "slides/09-timeseries-basics.html#trend-tests",
    "title": "09-Time Series Basics",
    "section": "Trend Tests",
    "text": "Trend Tests\n\n\nMann-Kendall trend test, popular in environmental sciences\nstrictly suitable only for trend-stationary time series, but robust in cases of weak autocorrelation\nnot for “pure” difference-stationary time series, because residuals are autocorrelated\n\\(\\Rightarrow\\) Test stationarity (or autocorrelation) before testing trend !!!\n\n\nlibrary(\"Kendall\")\nMannKendall(TSP)\n\ntau = 0.894, 2-sided pvalue =< 2.22e-16\n\n\nSignificant trend. \n\nMannKendall(DSP)\n\ntau = 0.755, 2-sided pvalue =< 2.22e-16\n\n\nIndicates significant trend, but as process is DSP, interpretation is wrong."
  },
  {
    "objectID": "slides/x1-r-basics.html#r-is-more-convenient-with-rstudio",
    "href": "slides/x1-r-basics.html#r-is-more-convenient-with-rstudio",
    "title": "x1-R Basics",
    "section": "R is more convenient with RStudio",
    "text": "R is more convenient with RStudio"
  },
  {
    "objectID": "slides/x1-r-basics.html#r-and-rstudio",
    "href": "slides/x1-r-basics.html#r-and-rstudio",
    "title": "x1-R Basics",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nEngine and Control\n\nR The main engine for computations and graphics.\nRstudio the IDE (integrated development environment) that embeds and controls R and provides additional facilities.\nR can also be used without RStudio.\n\n\nCitation\nCite R and optionally RStudio.\nR Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/\nRStudio Team (2022). RStudio: Integrated Development Environment for R. RStudio, PBC, Boston, MA URL http://www.rstudio.com/"
  },
  {
    "objectID": "slides/x1-r-basics.html#expressions-and-assignments",
    "href": "slides/x1-r-basics.html#expressions-and-assignments",
    "title": "x1-R Basics",
    "section": "Expressions and Assignments",
    "text": "Expressions and Assignments\n\n\nExpression\n\n\n1 - pi + exp(1.7)\n\n[1] 3.332355\n\n\n\n\nresult is printed to the screen\nthe [1] indicates that the value shown at the beginning of the line is the first (and here the only) element\n\n\n\nAssignment\n\n\na <- 1 - pi + exp(1.7)\n\n\n\nThe expression on the left hand side is assigned to the variable on the right.\nThe arrow is spelled as “a gets …”\nTo avoid confusion: use <- for assignment and let = for parameter matching"
  },
  {
    "objectID": "slides/x1-r-basics.html#constants-variables-and-assignments",
    "href": "slides/x1-r-basics.html#constants-variables-and-assignments",
    "title": "x1-R Basics",
    "section": "Constants, variables and assignments",
    "text": "Constants, variables and assignments\nAssignment of constants and variables to a variable\n\n\nx <- 1.3      # numeric constant\ny <- \"hello\"  # character constant\na <- x        # a and x both variables\n\nAssignment in opposite direction (rarely used)\n\nx -> b\n\nMultiple assignment\n\nx <- a <- b\n\n\nDo not use the following constructs\n\n# Equal sign has two meanings: parameter matching and assignment\n# - Don't use it for assignment!\nx = a\n\n# Super assignment, useful for programmers in special cases\nx <<- 2"
  },
  {
    "objectID": "slides/x1-r-basics.html#objects-constants-variables",
    "href": "slides/x1-r-basics.html#objects-constants-variables",
    "title": "x1-R Basics",
    "section": "Objects, constants, variables",
    "text": "Objects, constants, variables\n\nEverything stored in R’s memory is an object:\n\ncan be simple or complex\ncan be constants or variables\nconstants: 1, 123, 5.6, 5e7, “hello”\nvariables: can change their value, are referenced by variable names\n\n\n\nx <- 2.0 # x is a variable, 2.0 is a constant\n\nA syntactically valid variable name consists of:\n\nletters, numbers, underline (_), dot (.)\nstarts with a letter or the dot\nif starting with the dot, not followed by a number\n\nSpecial characters, except _ and . (underscore and dot) are not allowed.\nInternational characters (e.g German umlauts ä, ö, ü, …) are possible, but not recommended."
  },
  {
    "objectID": "slides/x1-r-basics.html#allowed-and-disallowed-identifiers",
    "href": "slides/x1-r-basics.html#allowed-and-disallowed-identifiers",
    "title": "x1-R Basics",
    "section": "Allowed and disallowed identifiers",
    "text": "Allowed and disallowed identifiers\n\ncorrect:\n\nx, y, X, x1, i, j, k\nvalue, test, myVariableName, do_something\n`.hidden, .x1``\n\nforbidden:\n\n1x, .1x (starts with a number)\n!, @, \\$, #, space, comma, semicolon and other special characters\n\nreserved words cannot be used as variable names:\n\nif, else, repeat, while, function, for, in, next, break\nTRUE, FALSE, NULL, Inf, NaN, NA, NA_integer_, NA_real_, NA_complex_, NA_character\\_\n..., ..1, ..2\n\nNote: R is case sensitive, x and X, value and Value are different."
  },
  {
    "objectID": "slides/x1-r-basics.html#operators",
    "href": "slides/x1-r-basics.html#operators",
    "title": "x1-R Basics",
    "section": "Operators",
    "text": "Operators\n\n\n\n\n\n\noperator\nsymbol\n\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nNegation\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nModulo\n%%\n\n\nInteger Divison\n%/%\n\n\nPower\n^\n\n\nMatrix product\n%*%\n\n\nOuter product\n%o%\n\n\n\n\n\n\n\n\n\n\n\n\noperator\nsymbol\n\n\n\n\nNegation\n!\n\n\nAnd\n&\n\n\nOr\n|\n\n\nEqual\n==\n\n\nUnequal\n!=\n\n\nLess than\n<\n\n\nGreater than\n>\n\n\nLess or equal\n<=\n\n\nGreater or equal\n>=\n\n\nAssignment\n<-\n\n\nElement of a list\n$\n\n\nPipeline\n|>\n\n\n\n\n\n\n… and more"
  },
  {
    "objectID": "slides/x1-r-basics.html#functions",
    "href": "slides/x1-r-basics.html#functions",
    "title": "x1-R Basics",
    "section": "Functions",
    "text": "Functions\nPre-defined functions:\n\nwith return value: sin(x), log(x)\nwith side effect: plot(x), print(x)\nwith both return value and side efect: hist(x)\n\nArguments: mandatory or optional, un-named or named\n\nplot(1:4, c(3, 4, 3, 6), type = \"l\", col = \"red\")\nif named arguments are used (with the “=” sign), argument order does not matter\n\nUser-defined functions:\n\ncan be used to extend R\nwill be discussed later\n\n\\(\\rightarrow\\) Functions have always a name followed by arguments in round parentheses."
  },
  {
    "objectID": "slides/x1-r-basics.html#parentheses",
    "href": "slides/x1-r-basics.html#parentheses",
    "title": "x1-R Basics",
    "section": "Parentheses",
    "text": "Parentheses"
  },
  {
    "objectID": "slides/x1-r-basics.html#vectors-matrices-and-arrays",
    "href": "slides/x1-r-basics.html#vectors-matrices-and-arrays",
    "title": "x1-R Basics",
    "section": "Vectors, matrices and arrays",
    "text": "Vectors, matrices and arrays\n\nvectors = 1D, matrices = 2D and arrays = n-dimensional\ndata are arranged into rows, columns, layers, …\ndata filled in column-wise, can be changed\ncreate vector\n\n\nx <- 1:20\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\nconvert it to matrix\n\n\ny <- matrix(x, nrow = 5, ncol = 4)\ny\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n\n\nback-convert (flatten) to vector\n\n\nas.vector(y) # flattens the matrix to a vector\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20"
  },
  {
    "objectID": "slides/x1-r-basics.html#vectors-matrices-and-arrays-ii",
    "href": "slides/x1-r-basics.html#vectors-matrices-and-arrays-ii",
    "title": "x1-R Basics",
    "section": "Vectors, matrices and arrays II",
    "text": "Vectors, matrices and arrays II\n\nrecycling rule if the number of elements is too small\n\n\nx <- matrix(0, nrow=5, ncol=4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n[5,]    0    0    0    0\n\nx <- matrix(1:4, nrow=5, ncol=4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    2    3    4    1\n[3,]    3    4    1    2\n[4,]    4    1    2    3\n[5,]    1    2    3    4"
  },
  {
    "objectID": "slides/x1-r-basics.html#transpose-rows-and-columns",
    "href": "slides/x1-r-basics.html#transpose-rows-and-columns",
    "title": "x1-R Basics",
    "section": "Transpose rows and columns",
    "text": "Transpose rows and columns\n\nrow-wise creation of a matrix\n\n\nx <- matrix(1:20, nrow = 5, ncol = 4, byrow = TRUE)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n[5,]   17   18   19   20\n\n\n\ntranspose of a matrix\n\n\nx <- t(x)\nx\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20"
  },
  {
    "objectID": "slides/x1-r-basics.html#access-array-elements",
    "href": "slides/x1-r-basics.html#access-array-elements",
    "title": "x1-R Basics",
    "section": "Access array elements",
    "text": "Access array elements\n\n\na three dimensional array\nrow, column, layer/page\nsub-matrices (slices)\n\n\nx <- array(1:24, dim=c(3, 4, 2))\nx\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\n\n\n\n\n\n\n\nelements of a matrix or array\n\n\nx[1, 3, 1] # single element\n\n[1] 7\n\nx[ , 3, 1] # 3rd column of 1st layer\n\n[1] 7 8 9\n\nx[ ,  , 2] # second layer\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\nx[1,  ,  ] # another slice\n\n     [,1] [,2]\n[1,]    1   13\n[2,]    4   16\n[3,]    7   19\n[4,]   10   22"
  },
  {
    "objectID": "slides/x1-r-basics.html#reordering-and-indirect-indexing",
    "href": "slides/x1-r-basics.html#reordering-and-indirect-indexing",
    "title": "x1-R Basics",
    "section": "Reordering and indirect indexing",
    "text": "Reordering and indirect indexing\n\nOriginal matrix\n\n(x <- matrix(1:20, nrow = 4))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\n\nInverted row order\n\nx[4:1, ]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    4    8   12   16   20\n[2,]    3    7   11   15   19\n[3,]    2    6   10   14   18\n[4,]    1    5    9   13   17\n\n\n\n\nIndirect index\n\nx[c(1, 2, 1, 2), c(1, 3, 2, 5, 4)]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    9    5   17   13\n[2,]    2   10    6   18   14\n[3,]    1    9    5   17   13\n[4,]    2   10    6   18   14\n\n\nLogical selection\n\nx[c(FALSE, TRUE, FALSE, TRUE), ]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    6   10   14   18\n[2,]    4    8   12   16   20\n\n\nSurprise?\n\nx[c(0, 1, 0, 1), ]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    1    5    9   13   17"
  },
  {
    "objectID": "slides/x1-r-basics.html#matrix-multiplication-explained",
    "href": "slides/x1-r-basics.html#matrix-multiplication-explained",
    "title": "x1-R Basics",
    "section": "Matrix multiplication explained",
    "text": "Matrix multiplication explained\n\n\nTwo matrices: A and B\n\nA <- matrix(c(1, 2, 3,\n              5, 4, 2), \n            nrow = 2, byrow = TRUE)\n\nB <- matrix(c(1, 2, 3, 4,\n              6, 8, 4, 2,\n              3, 1, 3, 2), \n            nrow = 3, byrow = TRUE)\n\nMultiplication: \\(A \\cdot B\\)\n\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]   22   21   20   14\n[2,]   35   44   37   32"
  },
  {
    "objectID": "slides/x1-r-basics.html#transpose-and-inverse",
    "href": "slides/x1-r-basics.html#transpose-and-inverse",
    "title": "x1-R Basics",
    "section": "Transpose and inverse",
    "text": "Transpose and inverse\nMatrix\n\nX <- matrix(c(1, 2, 3, \n              4, 3, 2, \n              5, 4, 6),\n            nrow = 3)\nX\n\n     [,1] [,2] [,3]\n[1,]    1    4    5\n[2,]    2    3    4\n[3,]    3    2    6\n\n\nTranspose\n\nt(X)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    3    2\n[3,]    5    4    6\n\n\nInverse (\\(X^{-1}\\))\n\nsolve(X)\n\n\n\n        [,1]    [,2]    [,3]\n[1,] -0.6667  0.9333 -0.0667\n[2,]  0.0000  0.6000 -0.4000\n[3,]  0.3333 -0.6667  0.3333"
  },
  {
    "objectID": "slides/x1-r-basics.html#multiplication-of-a-matrix-with-its-inverse",
    "href": "slides/x1-r-basics.html#multiplication-of-a-matrix-with-its-inverse",
    "title": "x1-R Basics",
    "section": "Multiplication of a matrix with its inverse",
    "text": "Multiplication of a matrix with its inverse\n\n\n\\[X \\cdot X^{-1} = I\\]\n\nX %*% solve(X)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n\n\n\\(I\\): identity matrix"
  },
  {
    "objectID": "slides/x1-r-basics.html#linear-system-of-equations",
    "href": "slides/x1-r-basics.html#linear-system-of-equations",
    "title": "x1-R Basics",
    "section": "Linear system of equations",
    "text": "Linear system of equations\n\n\\[\\begin{align}\n3x && +  && 2y   && -  && z  && =  && 1 \\\\\n2x && -  && 2y   && +  && 4z && =  && -2 \\\\\n-x && +  && 1/2y && -  && z  && =  && 0\n\\end{align}\\]\n\n\nA <- matrix(c(3,  2,   -1,\n             2,  -2,    4,\n            -1,   0.5, -1), nrow=3, byrow=TRUE)\nb <- c(1, -2, 0)\n\n\n\\[\\begin{align}\nAx &= b\\\\\nx  &= A^{-1}b\n\\end{align}\\]\n\n\nsolve(A) %*% b\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]   -2"
  },
  {
    "objectID": "slides/x1-r-basics.html#data-frames",
    "href": "slides/x1-r-basics.html#data-frames",
    "title": "x1-R Basics",
    "section": "Data frames",
    "text": "Data frames\n\nrepresent tabular data\nsimilar to matrices, but different types of data in columns possible\ntypically imported from a file with read.table or read.csv\n\n\n\ncities <- read.csv(\"cities.csv\")\ncities\n\n\n\n\n               Name    Country Population Latitude Longitude IsCapital\n1  Fürstenfeldbruck    Germany      34033  48.1690   11.2340     FALSE\n2             Dhaka Bangladesh   13000000  23.7500   90.3700      TRUE\n3       Ulaanbaatar   Mongolia    3010000  47.9170  106.8830      TRUE\n4           Shantou      China    5320000  23.3500  116.6700     FALSE\n5           Kampala     Uganda    1659000   0.3310   32.5830      TRUE\n6           Cottbus    Germany     100000  51.7650   14.3280     FALSE\n7           Nairobi      Kenya    3100000   1.2833   36.8167      TRUE\n8             Hanoi    Vietnam    1452055  21.0300  105.8400      TRUE\n9          Bacgiang    Vietnam      53739  21.2800  106.1900     FALSE\n10       Addis Abba   Ethiopia    2823167   9.0300   38.7400      TRUE\n11        Hyderabad      India    3632094  17.4000   78.4800     FALSE\n\n\n\\(\\rightarrow\\) download data set"
  },
  {
    "objectID": "slides/x1-r-basics.html#what-is-a-csv-file",
    "href": "slides/x1-r-basics.html#what-is-a-csv-file",
    "title": "x1-R Basics",
    "section": "What is a CSV file?",
    "text": "What is a CSV file?\n\ncomma separated values.\nfirst line contains column names\ndecimal is dec=\".\", column separator is sep=\",\"\n\nExample CSV file (Data from Wikipedia, 2023)\n\nName,Country,Population,Latitude,Longitude\nDhaka,Bangladesh,10278882,23.75,90.37\nUlaanbaatar,Mongolia,1672627,47.917,106.883\nShantou,China,5502031,23.35,116.67\nKampala,Uganda,1680600,0.331,32.583\nBerlin,Germany,3850809,52.52,13.405\nNairobi,Kenya,4672000,1.2833,36.8167\nHanoi,Vietnam,8435700,21.03,105.84\nAddis Abba,Ethiopia,3945000,9.03,38.74\nHyderabad,India,9482000,17.4,78.48\n\nHints\n\nsome countries use dec = \",\" and sep = \";\"\nExcel may export mixed style with dec = \".\" and sep = \";\"\ncomments above the header line can be skipped"
  },
  {
    "objectID": "slides/x1-r-basics.html#different-read-funktions",
    "href": "slides/x1-r-basics.html#different-read-funktions",
    "title": "x1-R Basics",
    "section": "Different read-Funktions",
    "text": "Different read-Funktions\n\nR contains several read-functions for different file types.\nSome are more flexible, some more automatic, some faster, some more robust …\n\nTo avoid confusion, we use only the following:\nBase R\n\nread.table(): this is the most flexible standard function, see help file for details\nread.csv(): default options for standard csv files (with dec=\".\" and sep=,)\n\nTidyverse readr-package\n\nread_delim(): similar to read.table() but more modern, automatic and faster\nread_csv(): similar to read.csv() with more automatism, e.g. date detection"
  },
  {
    "objectID": "slides/x1-r-basics.html#the-most-versatile-read.table",
    "href": "slides/x1-r-basics.html#the-most-versatile-read.table",
    "title": "x1-R Basics",
    "section": "The most versatile: read.table()",
    "text": "The most versatile: read.table()\n\nread.table(file, header = FALSE, sep = \"\", quote = \"\\\"'\",\n           dec = \".\", numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"),\n           row.names, col.names, as.is = !stringsAsFactors, tryLogical = TRUE,\n           na.strings = \"NA\", colClasses = NA, nrows = -1,\n           skip = 0, check.names = TRUE, fill = !blank.lines.skip,\n           strip.white = FALSE, blank.lines.skip = TRUE,\n           comment.char = \"#\",\n           allowEscapes = FALSE, flush = FALSE,\n           stringsAsFactors = FALSE,\n           fileEncoding = \"\", encoding = \"unknown\", text, skipNul = FALSE)\n\nExamples\n\nread.table(\"cities.csv\", sep = \",\",  dec = \".\")  # same as read.csv\nread.table(\"cities.txt\", sep = \"\\t\", dec = \".\")  # tab delimited\nread.table(\"cities.csv\", sep = \";\",  dec = \",\")  # German csv\n\nread.table(\"cities.csv\", sep = \",\", dec = \".\", skip = 5) # skip first 5 lines"
  },
  {
    "objectID": "slides/x1-r-basics.html#recommendation",
    "href": "slides/x1-r-basics.html#recommendation",
    "title": "x1-R Basics",
    "section": "Recommendation",
    "text": "Recommendation\n\nMost of our course examples are plain CSV files, so we can use read.csv() or read_csv().\n\n\nlibrary(\"readr\")\ncities <- read_csv(\"cities.csv\")\ncities\n\n\n\n\n# A tibble: 11 × 6\n   Name             Country    Population Latitude Longitude IsCapital\n   <chr>            <chr>           <dbl>    <dbl>     <dbl> <lgl>    \n 1 Fürstenfeldbruck Germany         34033   48.2        11.2 FALSE    \n 2 Dhaka            Bangladesh   13000000   23.8        90.4 TRUE     \n 3 Ulaanbaatar      Mongolia      3010000   47.9       107.  TRUE     \n 4 Shantou          China         5320000   23.4       117.  FALSE    \n 5 Kampala          Uganda        1659000    0.331      32.6 TRUE     \n 6 Cottbus          Germany        100000   51.8        14.3 FALSE    \n 7 Nairobi          Kenya         3100000    1.28       36.8 TRUE     \n 8 Hanoi            Vietnam       1452055   21.0       106.  TRUE     \n 9 Bacgiang         Vietnam         53739   21.3       106.  FALSE    \n10 Addis Abba       Ethiopia      2823167    9.03       38.7 TRUE     \n11 Hyderabad        India         3632094   17.4        78.5 FALSE"
  },
  {
    "objectID": "slides/x1-r-basics.html#data-import-assistant-of-rstudio",
    "href": "slides/x1-r-basics.html#data-import-assistant-of-rstudio",
    "title": "x1-R Basics",
    "section": "Data import assistant of RStudio",
    "text": "Data import assistant of RStudio\n\nFile –> Import Dataset\nSeveral options are available:\n\n“From text (base)” uses the classical R functions\n“From text (readr)” is more modern and uses an add-on package\n“From Excel” can read Excel files if (and only if) they have a clear tabular structure"
  },
  {
    "objectID": "slides/x1-r-basics.html#from-text-base",
    "href": "slides/x1-r-basics.html#from-text-base",
    "title": "x1-R Basics",
    "section": "From text (base)",
    "text": "From text (base)"
  },
  {
    "objectID": "slides/x1-r-basics.html#from-text-readr",
    "href": "slides/x1-r-basics.html#from-text-readr",
    "title": "x1-R Basics",
    "section": "From text (readr)",
    "text": "From text (readr)"
  },
  {
    "objectID": "slides/x1-r-basics.html#save-data-to-excel-compatible-format",
    "href": "slides/x1-r-basics.html#save-data-to-excel-compatible-format",
    "title": "x1-R Basics",
    "section": "Save data to Excel-compatible format",
    "text": "Save data to Excel-compatible format\nEnglish number format (“.” as decimal):\n\nwrite.table(cities, \"output.csv\", row.names = FALSE, sep=\",\")\n\nGerman number format (“,” as decimal):\n\nwrite.table(cities, \"output.csv\", row.names = FALSE, sep=\";\", dec=\",\")\n\n ## Creation of data frames\n\n\ntypical: read data from external file, e.g. csv-files.\nsmall data frames can be created inline in a script\n\nInline creation of a data frame\n\nclem <- data.frame(\n  brand = c(\"EP\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \n            \"EB\", \"EB\", \"EB\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EB\", \"EP\"),\n  weight = c(88, 96, 100, 96, 90, 100, 92, 92, 102, 99, 86, 89, 99, 89, 75, 80, \n             81, 96, 82, 98, 80, 107, 88)\n)"
  },
  {
    "objectID": "slides/x1-r-basics.html#conversion-between-matrices-and-data-frames",
    "href": "slides/x1-r-basics.html#conversion-between-matrices-and-data-frames",
    "title": "x1-R Basics",
    "section": "Conversion between matrices and data frames",
    "text": "Conversion between matrices and data frames\n\nMatrix to data frame\n\nx <- matrix(1:16, nrow=4)\ndf <- as.data.frame(x)\ndf\n\n  V1 V2 V3 V4\n1  1  5  9 13\n2  2  6 10 14\n3  3  7 11 15\n4  4  8 12 16\n\n\n\nData frame to matrix\n\nas.matrix(df)\n\n     V1 V2 V3 V4\n[1,]  1  5  9 13\n[2,]  2  6 10 14\n[3,]  3  7 11 15\n[4,]  4  8 12 16\n\n\n\n\nAppend column\n\ndf2 <- cbind(df,\n         id = c(\"first\", \"second\", \"third\", \"fourth\")\n       )\n\nOr simply\n\ndf2$id <- c(\"first\", \"second\", \"third\", \"fourth\")\n\n\nData frame with character column\n\nas.matrix(df2)\n\n     V1  V2  V3   V4   id      \n[1,] \"1\" \"5\" \" 9\" \"13\" \"first\" \n[2,] \"2\" \"6\" \"10\" \"14\" \"second\"\n[3,] \"3\" \"7\" \"11\" \"15\" \"third\" \n[4,] \"4\" \"8\" \"12\" \"16\" \"fourth\"\n\n\n\nall columns are now character\nmatrix does not support mixed data"
  },
  {
    "objectID": "slides/x1-r-basics.html#selection-of-data-frame-columns",
    "href": "slides/x1-r-basics.html#selection-of-data-frame-columns",
    "title": "x1-R Basics",
    "section": "Selection of data frame columns",
    "text": "Selection of data frame columns\nCreate a data frame from a matrix\n\nx <- matrix(1:16, nrow=4)\ndf <- as.data.frame(x)\ndf\n\n  V1 V2 V3 V4\n1  1  5  9 13\n2  2  6 10 14\n3  3  7 11 15\n4  4  8 12 16\n\n\nAdd names to the columns\n\nnames(df) <- c(\"N\", \"P\", \"O2\", \"C\")\ndf\n\n  N P O2  C\n1 1 5  9 13\n2 2 6 10 14\n3 3 7 11 15\n4 4 8 12 16\n\n\nSelect 3 columns and change order\n\ndf2 <- df[c(\"C\", \"N\", \"P\")]\ndf2\n\n   C N P\n1 13 1 5\n2 14 2 6\n3 15 3 7\n4 16 4 8"
  },
  {
    "objectID": "slides/x1-r-basics.html#data-frame-indexing-like-a-matrix",
    "href": "slides/x1-r-basics.html#data-frame-indexing-like-a-matrix",
    "title": "x1-R Basics",
    "section": "Data frame indexing like a matrix",
    "text": "Data frame indexing like a matrix\n\nA data frame\n\ndf\n\n  N P O2  C\n1 1 5  9 13\n2 2 6 10 14\n3 3 7 11 15\n4 4 8 12 16\n\n\nA single value\n\ndf[2, 3]\n\n[1] 10\n\n\nComplete column\n\ndf[,1]\n\n[1] 1 2 3 4\n\n\nComplete row\n\ndf[2,]\n\n  N P O2  C\n2 2 6 10 14\n\n\n\n\nConditional selection of rows\n\ndf[df$P > 6, ]\n\n  N P O2  C\n3 3 7 11 15\n4 4 8 12 16\n\n\n\nDifferences between [], [[]] and $\n\ndf[\"P\"]     # a single column data frame\n\n  P\n1 5\n2 6\n3 7\n4 8\n\ndf[[\"P\"]]   # a vector\n\n[1] 5 6 7 8\n\ndf$P        # a vector\n\n[1] 5 6 7 8"
  },
  {
    "objectID": "slides/x1-r-basics.html#lists-1",
    "href": "slides/x1-r-basics.html#lists-1",
    "title": "x1-R Basics",
    "section": "Lists",
    "text": "Lists\n\nmost flexible data type in R\ncan contain arbitrary data objects as elements of the list\nallows tree-like structure\n\nExamples\n\nOutput of many R functions, e.g. return value of hist:\n\n\nL <- hist(rnorm(100), plot=FALSE)\nstr(L)\n\nList of 6\n $ breaks  : num [1:8] -3 -2 -1 0 1 2 3 4\n $ counts  : int [1:7] 4 13 42 25 14 0 2\n $ density : num [1:7] 0.04 0.13 0.42 0.25 0.14 0 0.02\n $ mids    : num [1:7] -2.5 -1.5 -0.5 0.5 1.5 2.5 3.5\n $ xname   : chr \"rnorm(100)\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\""
  },
  {
    "objectID": "slides/x1-r-basics.html#creation-of-lists",
    "href": "slides/x1-r-basics.html#creation-of-lists",
    "title": "x1-R Basics",
    "section": "Creation of lists",
    "text": "Creation of lists\n\nL1 <- list(a=1:10, b=c(1,2,3), x=\"hello\")\n\nNested list (lists within a list)\n\nL2 <- list(a=5:7, b=L1)\n\nstr shows tree-like structure\n\nstr(L2)\n\nList of 2\n $ a: int [1:3] 5 6 7\n $ b:List of 3\n  ..$ a: int [1:10] 1 2 3 4 5 6 7 8 9 10\n  ..$ b: num [1:3] 1 2 3\n  ..$ x: chr \"hello\"\n\n\n\nAccess to list elements by names\n\nL2$a\n\n[1] 5 6 7\n\nL2$b$a\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\n\n\nor with indices\n\nL2[1]   # a list with 1 element\n\n$a\n[1] 5 6 7\n\nL2[[1]] # content of 1st element\n\n[1] 5 6 7"
  },
  {
    "objectID": "slides/x1-r-basics.html#lists-ii",
    "href": "slides/x1-r-basics.html#lists-ii",
    "title": "x1-R Basics",
    "section": "Lists II",
    "text": "Lists II\n\nConvert list to vector\n\n\nL <- unlist(L2)\nstr(L)\n\n Named chr [1:17] \"5\" \"6\" \"7\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \"10\" \"1\" ...\n - attr(*, \"names\")= chr [1:17] \"a1\" \"a2\" \"a3\" \"b.a1\" ...\n\n\n\n\nFlatten list (remove only top level of list)\n\n\nL <- unlist(L2, recursive = FALSE)\nstr(L)\n\nList of 6\n $ a1 : int 5\n $ a2 : int 6\n $ a3 : int 7\n $ b.a: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ b.b: num [1:3] 1 2 3\n $ b.x: chr \"hello\""
  },
  {
    "objectID": "slides/x1-r-basics.html#naming-of-list-elements",
    "href": "slides/x1-r-basics.html#naming-of-list-elements",
    "title": "x1-R Basics",
    "section": "Naming of list elements",
    "text": "Naming of list elements\nDuring creation\n\nx <- c(a=1.2, b=2.3, c=6)\nL <- list(a=1:3, b=\"hello\")\n\nWith names-function\nOriginal names:\n\nnames(L)\n\n[1] \"a\" \"b\"\n\n\nRename list elements:\n\nnames(L) <- c(\"numbers\", \"text\")\nnames(L)\n\n[1] \"numbers\" \"text\"   \n\n\nThe names-functions works also with vectors. The pre-defined vectors letters contains lower case and LETTERS uppercase letters:\n\nx <- 1:5\nnames(x) <- letters[1:5]\nx\n\na b c d e \n1 2 3 4 5"
  },
  {
    "objectID": "slides/x1-r-basics.html#apply-a-function-to-multiple-rows-and-columns",
    "href": "slides/x1-r-basics.html#apply-a-function-to-multiple-rows-and-columns",
    "title": "x1-R Basics",
    "section": "Apply a function to multiple rows and columns",
    "text": "Apply a function to multiple rows and columns\n\nExample data frame\n\ndf  # data frame of previous slide\n\n  N P O2  C\n1 1 5  9 13\n2 2 6 10 14\n3 3 7 11 15\n4 4 8 12 16\n\n\nApply a function to all elements of a list\n\nlapply(df, mean)  # returns list\n\n$N\n[1] 2.5\n\n$P\n[1] 6.5\n\n$O2\n[1] 10.5\n\n$C\n[1] 14.5\n\nsapply(df, mean)  # returns vector\n\n   N    P   O2    C \n 2.5  6.5 10.5 14.5 \n\n\n\n\n\n\n\nRow wise apply\n\napply(df, MARGIN = 1, sum)\n\n[1] 28 32 36 40\n\n\nColumn wise apply\n\napply(df, MARGIN = 2, sum)\n\n N  P O2  C \n10 26 42 58 \n\n\nApply user defined function\n\nse <- function(x)\n  sd(x)/sqrt(length(x))\n\nsapply(df, se)\n\n\n\n     N      P     O2      C \n0.6455 0.6455 0.6455 0.6455"
  },
  {
    "objectID": "slides/x1-r-basics.html#for-loop",
    "href": "slides/x1-r-basics.html#for-loop",
    "title": "x1-R Basics",
    "section": "for-loop",
    "text": "for-loop\nA simple for-loop\n\n\nfor (i in 1:4) {\n  cat(i, 2*i, \"\\n\")\n}\n\n1 2 \n2 4 \n3 6 \n4 8 \n\n\n\nNested for-loops\n\n\nfor (i in 1:3) {\n  for (j in c(1,3,5)) {\n    cat(i, i*j, \"\\n\")\n  }\n}\n\n1 1 \n1 3 \n1 5 \n2 2 \n2 6 \n2 10 \n3 3 \n3 9 \n3 15"
  },
  {
    "objectID": "slides/x1-r-basics.html#repeat-and-while-loops",
    "href": "slides/x1-r-basics.html#repeat-and-while-loops",
    "title": "x1-R Basics",
    "section": "repeat and while-loops",
    "text": "repeat and while-loops\nRepeat until a break condition occurs\n\n\nx <- 1\nrepeat {\n x <- 0.1*x\n cat(x, \"\\n\")\n if (x < 1e-4) break\n}\n\n0.1 \n0.01 \n0.001 \n1e-04 \n1e-05 \n\n\nLoop as long as a whilecondition is TRUE:\n\nj <- 1; x <- 0\nwhile (j > 1e-3) {\n  j <- 0.1 * j\n  x <- x + j\n  cat(j, x, \"\\n\")\n}\n\n0.1 0.1 \n0.01 0.11 \n0.001 0.111 \n1e-04 0.1111 \n\n\n\nIn many cases, loops can be avoided by using vectors and matrices or apply."
  },
  {
    "objectID": "slides/x1-r-basics.html#avoidable-loops",
    "href": "slides/x1-r-basics.html#avoidable-loops",
    "title": "x1-R Basics",
    "section": "Avoidable loops",
    "text": "Avoidable loops\n\nColumn means of a data frame\n\n## a data frame\ndf <- data.frame(\n  N=1:4, P=5:8, O2=9:12, C=13:16\n)\n\n## loop\nm <- numeric(4)\nfor(i in 1:4) {\n m[i] <- mean(df[,i])\n}\nm\n\n[1]  2.5  6.5 10.5 14.5\n\n\n\\(\\rightarrow\\) easier without loop\n\nsapply(df, mean)\n\n   N    P   O2    C \n 2.5  6.5 10.5 14.5 \n\n\n… also possible colMeans\n\n\n\n\n\nAn infinite series:\n\\[\n\\sum_{k=1}^{\\infty}\\frac{(-1)^{k-1}}{2k-1} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7}\n\\]\n\nx <- 0\nfor (k in seq(1, 1e5)) {\n  enum  <- (-1)^(k-1)\n  denom <- 2*k-1\n  x <- x + enum/denom\n}\n4 * x\n\n[1] 3.141583\n\n\n\\(\\Rightarrow\\) Can you vectorize this?"
  },
  {
    "objectID": "slides/x1-r-basics.html#unavoidable-loop",
    "href": "slides/x1-r-basics.html#unavoidable-loop",
    "title": "x1-R Basics",
    "section": "Unavoidable loop",
    "text": "Unavoidable loop\nThe same series:\n\\[\n\\sum_{k=1}^{\\infty}\\frac{(-1)^{k-1}}{2k-1} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7}\n\\]\n\nx <- 0\nk <- 0\nrepeat {\n  k <- k + 1\n  enum  <- (-1)^(k-1)\n  denom <- 2*k-1\n  delta <- enum/denom\n  x <- x + delta\n  if (abs(delta) < 1e-6) break\n}\n4 * x\n\n[1] 3.141595\n\n\n\nnumber of iterations not known in advance\nconvergence criterium, stop when required precision is reached\nno allocation of long vectors –> less memory than for loop\n\n\n\nNote: there are more efficient methods to calculate \\(\\pi\\)."
  },
  {
    "objectID": "slides/x1-r-basics.html#if-clause",
    "href": "slides/x1-r-basics.html#if-clause",
    "title": "x1-R Basics",
    "section": "if-clause",
    "text": "if-clause\n\nThe example before showed already an if-clause. The syntax is as follows:\n\nif (<condition>)\n  <statement>\nelse if (<condition>)\n  <statement>\nelse\n  <statement>\n\n\nProper indentation improves readability.\nRecommended: 2 characters\nProfessionals indent always.\nPlease do!\n\n\n\n\n\n\nUse of {} to group statements\n\nstatement can of be a compound statement with curly brackets {}\nto avoid common mistakes and be on the safe side, use always {}:\n\nExample:\n\nif (x == 0) {\n  print(\"x is Null\")\n} else if (x < 0) {\n  print(\"x is negative\")\n} else {\n  print(\"x is positive\")\n}"
  },
  {
    "objectID": "slides/x1-r-basics.html#vectorized-if",
    "href": "slides/x1-r-basics.html#vectorized-if",
    "title": "x1-R Basics",
    "section": "Vectorized if",
    "text": "Vectorized if\nOften, a vectorized ifelse is more appropropriate than an if-function.\nLet’s assume we have a data set of chemical measurements x with missing NA values, and “nondetects” that are encoded with -99. First we want to replace the nontetects with half of the detection limit (e.g. 0.5):\n\nx <- c(3, 6, NA, 5, 4, -99, 7, NA,  8, -99, -99, 9)\nx2 <- ifelse(x == -99, 0.5, x)\nx2\n\n [1] 3.0 6.0  NA 5.0 4.0 0.5 7.0  NA 8.0 0.5 0.5 9.0\n\n\nNow let’s remove the NAs:\n\nx3 <- na.omit(x2)\nx3\n\n [1] 3.0 6.0 5.0 4.0 0.5 7.0 8.0 0.5 0.5 9.0\nattr(,\"na.action\")\n[1] 3 8\nattr(,\"class\")\n[1] \"omit\""
  },
  {
    "objectID": "slides/x1-r-basics.html#further-reading",
    "href": "slides/x1-r-basics.html#further-reading",
    "title": "x1-R Basics",
    "section": "Further reading",
    "text": "Further reading\n\nFollow-up presentations:\n\nFunctions everywhere\nGraphics in R\n\nMore details in the official R manuals, especially in “An Introduction to R”\nMany videos can be found on Youtube, at the Posit webpage and somewhere else.\nThis tutorial was made with Quarto\nAuthor: tpetzoldt +++ Homepage +++ Github page"
  },
  {
    "objectID": "slides/x2-r-graphics.html#table-of-contents",
    "href": "slides/x2-r-graphics.html#table-of-contents",
    "title": "x2-Graphics with R",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n\nThe easy way\nCustomizing graphics\nMultiple grapics on one page\nSaving and exporting graphics\nLattice\ngrid and gridbase\nggplot2"
  },
  {
    "objectID": "slides/x2-r-graphics.html#the-easy-way",
    "href": "slides/x2-r-graphics.html#the-easy-way",
    "title": "x2-Graphics with R",
    "section": "The Easy Way",
    "text": "The Easy Way\n\n\nplot(iris)\n\n\n\nR contains many graphics functions with convenient defaults.\niris is a built-in data set in R (see next slide)\nplot is a so-called generic function that automatically decides how to plot."
  },
  {
    "objectID": "slides/x2-r-graphics.html#the-iris-data-set",
    "href": "slides/x2-r-graphics.html#the-iris-data-set",
    "title": "x2-Graphics with R",
    "section": "The iris data set",
    "text": "The iris data set\n\nThe famous (Fisher’s or Anderson’s) iris data set contains measurements (in centimeter) of the variables sepal length, sepal width, petal length and petal width of 50 flowers from each of 3 species of iris, Iris setosa, I. versicolor, and I. virginica.\n\nsee ?iris in R’s online help.\nor: https://en.wikipedia.org/wiki/Iris_flower_data_set"
  },
  {
    "objectID": "slides/x2-r-graphics.html#plotting-colums-of-a-data-frame",
    "href": "slides/x2-r-graphics.html#plotting-colums-of-a-data-frame",
    "title": "x2-Graphics with R",
    "section": "Plotting colums of a data frame",
    "text": "Plotting colums of a data frame\n\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\n\n\n\n\n\n\n\nplot(iris$Sepal.Length, iris$Petal.Length,\n     col=iris$Species)\n\n\n\n\n\n\n\n\n\nA column of a data.frame is accessed with $."
  },
  {
    "objectID": "slides/x2-r-graphics.html#the-use-of-with-saves-dollars",
    "href": "slides/x2-r-graphics.html#the-use-of-with-saves-dollars",
    "title": "x2-Graphics with R",
    "section": "The use of with() saves dollars",
    "text": "The use of with() saves dollars\n\n\n\nplot(iris$Sepal.Length, iris$Petal.Length, \n     col=iris$Species)\n\n\n\n\n\n\n\n\n\n\n\nwith(iris, plot(Sepal.Length, Petal.Length, \n                col=Species))"
  },
  {
    "objectID": "slides/x2-r-graphics.html#colors-and-plotting-symbols-in-r",
    "href": "slides/x2-r-graphics.html#colors-and-plotting-symbols-in-r",
    "title": "x2-Graphics with R",
    "section": "Colors and plotting symbols in R",
    "text": "Colors and plotting symbols in R\nR allows to change style and color of plotting symbols:\n\ncol: color, can be one of 8 default colors or a user-defined color\npch: plotting character, can be one of 25 symbols or a quoted letter\ncex: character extension: size of a plotting character\n\n\nplot(1:25, col=1:25, pch=1:15, cex=2)"
  },
  {
    "objectID": "slides/x2-r-graphics.html#special-plotting-symbols",
    "href": "slides/x2-r-graphics.html#special-plotting-symbols",
    "title": "x2-Graphics with R",
    "section": "Special plotting symbols",
    "text": "Special plotting symbols\n\nsymbols 21..25 have an optional background color\nlwd: border width of the symbol\n\n\nplot(21:25, col=\"darkred\", pch=21:25, cex=2, bg=\"green\", lwd=2)"
  },
  {
    "objectID": "slides/x2-r-graphics.html#r-as-function-plotter",
    "href": "slides/x2-r-graphics.html#r-as-function-plotter",
    "title": "x2-Graphics with R",
    "section": "R as function plotter",
    "text": "R as function plotter\n\nx <- seq(0, 20, length.out=100)\ny1 <- sin(x)\ny2 <- cos(x)\nplot(x, y1, type=\"l\", col=\"red\")\nlines(x, y2, col=\"blue\")\n\n\n\ntype: “p”: points, “l”: lines, “b”: both, points and lines, “c”: empty points joined by lines, “o”: overplotted points and lines, “s” and “S”: stair steps, “h” histogram-like vertical lines, “n”: no points or lines."
  },
  {
    "objectID": "slides/x2-r-graphics.html#line-styles",
    "href": "slides/x2-r-graphics.html#line-styles",
    "title": "x2-Graphics with R",
    "section": "Line styles",
    "text": "Line styles\n\nx <- seq(0, 20, length.out=100)\nplot(x, sin(x), type=\"l\", col=\"red\", lwd=3, lty=\"dotted\")\nlines(x, cos(x), col=\"blue\", lwd=2, lty=\"dashed\")\n\n\n\nlty: line type (“blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, “twodash”) or a number from 1…7, or a string with up to 8 numbers for drawing and skipping (e.g. “4224”).\nlwd: line width (a number, defaults to 1)"
  },
  {
    "objectID": "slides/x2-r-graphics.html#coordinate-axes-and-annotations",
    "href": "slides/x2-r-graphics.html#coordinate-axes-and-annotations",
    "title": "x2-Graphics with R",
    "section": "Coordinate axes and annotations",
    "text": "Coordinate axes and annotations\n\nplot(iris$Sepal.Length, iris$Petal.Length, xlim=c(0, 8), ylim=c(2,8),\n     col=iris$Species, pch=16,\n     xlab=\"Sepal Length (cm)\", ylab=\"Petal Length (cm)\", main=\"Iris Data\",\n     las = 1)\n\n\n\ncol=iris$Species: works because Species is a factor\nlas=1: numbers on y-axis upright (try: 0, 1, 2 or 3)\nlog: may be used to transform axes (e.g. log=“x”, log=“y”, log=“xy”)"
  },
  {
    "objectID": "slides/x2-r-graphics.html#adding-a-legend",
    "href": "slides/x2-r-graphics.html#adding-a-legend",
    "title": "x2-Graphics with R",
    "section": "Adding a legend",
    "text": "Adding a legend\n\nmycolors <- c(\"blue\", \"red\", \"cyan\")\nplot(iris$Sepal.Length, iris$Petal.Length, xlim=c(0, 8), ylim=c(2,8),\n     col=mycolors[iris$Species], pch = 16,\n     xlab=\"Sepal Length (cm)\", ylab=\"Petal Length (cm)\", main=\"Iris Data\",\n     las = 1)\nlegend(\"topleft\", legend=c(\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"),\n  col=mycolors, pch=16)\n\n\n\nsee ?legend for more options (e.g. line styles, position of the legend)"
  },
  {
    "objectID": "slides/x2-r-graphics.html#global-parameters-font-size-margins",
    "href": "slides/x2-r-graphics.html#global-parameters-font-size-margins",
    "title": "x2-Graphics with R",
    "section": "Global parameters, font size, margins, …",
    "text": "Global parameters, font size, margins, …\n\n\nMany figure options can be specified globally with par()\npar(lwd=2) all lines have double width\npar(mfrow=c(2,2)) subdivides the graphics area in 2 x 2 fields\npar(las=1) numbers at y axis upright\npar(mar=c(5, 5, 0.5, 0.5)) changes figure margins (bottom, left, top, right)\npar(cex=2) increase font size\n\\(\\rightarrow\\) sometimes it is better to leave font size as is and change size of the figure instead\n\n\n\nRead the ?par help page!"
  },
  {
    "objectID": "slides/x2-r-graphics.html#example",
    "href": "slides/x2-r-graphics.html#example",
    "title": "x2-Graphics with R",
    "section": "Example",
    "text": "Example\n\n\n#\nplot(iris$Sepal.Length, iris$Petal.Length, \n     col=iris$Species)\n#\n\n\n\n\n\n\n\n\n\nopar <- par(cex=2, mar=c(4,4,1,1), las=1)\nplot(iris$Sepal.Length, iris$Petal.Length, \n     col=iris$Species)\npar(opar)\n\n\n\n\n\n\n\n\nchange font size, margins and axis label orientation\nopar stores previuos parameter and allows resetting"
  },
  {
    "objectID": "slides/x2-r-graphics.html#saving-and-exporting-figures",
    "href": "slides/x2-r-graphics.html#saving-and-exporting-figures",
    "title": "x2-Graphics with R",
    "section": "Saving and exporting figures",
    "text": "Saving and exporting figures\n\nEasiest way ist to use the RStudio’s Export –> Save as Image (or copy to clipboard)\nImportant: Select correct image format and image size!\n\n\n\n\n\n\n\n\n\n\nFormat\nType\nUsage\nNotes\n\n\n\n\nPNG\nbitmap\ngeneral purpose\nfixed size, use at least 300 pixels per inch\n\n\nJPEG\nbitmap\nphotographs\nnot good for R images\n\n\nTIFF\nbitmap\nPNG is easier\noutdated, required by some journals\n\n\nBMP\nbitmap\nnot recommended\noutdated, needs huge memory\n\n\nMetafile\nvector\nWindows standard format\neasy to use, quality varies\n\n\nSVG\nvector\ncan be edited\nallows editing graphics with Inkscape\n\n\nEPS\nvector\nPDF is easier\nrequired by some journals\n\n\nPDF\nvector\nbest quality\nperfect for LaTex, RMarkdown and Quarto, MS Office requires conversion"
  },
  {
    "objectID": "slides/x2-r-graphics.html#writing-figures-directly-to-pdf",
    "href": "slides/x2-r-graphics.html#writing-figures-directly-to-pdf",
    "title": "x2-Graphics with R",
    "section": "Writing figures directly to PDF",
    "text": "Writing figures directly to PDF\n\n\n\npdf(\"myfile.pdf\", width=8, height=6)\npar(las=1)\nplot(iris$Sepal.Length, iris$Petal.Length, col=iris$Species)\ndev.off()\n\n\n\nwidth and height in inch (1 inch = 2.54cm)\nprofessional quality, size can be changed without quality loss\nconversion to PNG can be done later with free programs\n\n\\(\\rightarrow\\) Inkscape, SumatraPDF, ImageMagick"
  },
  {
    "objectID": "slides/x2-r-graphics.html#writing-figures-directly-to-png",
    "href": "slides/x2-r-graphics.html#writing-figures-directly-to-png",
    "title": "x2-Graphics with R",
    "section": "Writing figures directly to PNG",
    "text": "Writing figures directly to PNG\n\n\npng(\"myfile.png\", width=1600, height=1200, res=300)  # good for Word\n#png(\"myfile.png\", width=800, height=600, res=150)   # good for Powerpoint\npar(las=1)\npar(mar=c(5, 5, 1, 1))\nplot(iris$Sepal.Length, iris$Petal.Length)\ndev.off()\n\n\nwidth and height given in pixels\nHint: play with res to change nominal resolution and font size\nuse at least 300 dpi (dots per inch, i.e. number of pixels = 300/2.54 * width in cm)\nprofessionals use 600 or even 1200 pixels per inch, but then .docx and .pptx files will dramatically increase\n1600 x 1200px is good for 13.3 x 10 cm size in the printed document"
  },
  {
    "objectID": "slides/x2-r-graphics.html#example-solar-radiation-data-in-dresden",
    "href": "slides/x2-r-graphics.html#example-solar-radiation-data-in-dresden",
    "title": "x2-Graphics with R",
    "section": "Example: Solar Radiation Data in Dresden",
    "text": "Example: Solar Radiation Data in Dresden\n\nradiation <- read.csv(\"../data/radiation.csv\")\nradiation$Date <- as.Date(radiation$date)\n\n\nplot(radiation$Date, radiation$rad)\n\n\nNote: The data set contains derived data from the German Weather Service (http://www.dwd.de), station Dresden. Missing data were interpolated."
  },
  {
    "objectID": "slides/x2-r-graphics.html#date-and-time-classes-in-r",
    "href": "slides/x2-r-graphics.html#date-and-time-classes-in-r",
    "title": "x2-Graphics with R",
    "section": "Date and time classes in R",
    "text": "Date and time classes in R\n\n\nMost important classes\n\nas.Date (dates only)\nas.POSIXct (date and time)\n\nformat and strptime\nextract day, month, year, Julian day\ntime series objects tseriesand zoo"
  },
  {
    "objectID": "slides/x2-r-graphics.html#format-and-strptime",
    "href": "slides/x2-r-graphics.html#format-and-strptime",
    "title": "x2-Graphics with R",
    "section": "format and strptime",
    "text": "format and strptime\n\nformat(x, format = \"\", tz = \"\", usetz = FALSE, ...)\n\n\n\n\n%Y\nyear with century\n\n\n%m\nmonth as decimal number\n\n\n%d\nday of the month\n\n\n%H\nhours as decimal number (00-23)\n\n\n%M\nminute as decimal number (00-59)\n\n\n%S\nsecond as decimal number (00-59)\n\n\n%j\nday of year (001-366)\n\n\n%u\nweekday, Monday is 1\n\n\n\n\nas.Date(\"11.03.2015\", format=\"%d.%m.%Y\")\n\n[1] \"2015-03-11\""
  },
  {
    "objectID": "slides/x2-r-graphics.html#date-conversion-for-the-solar-radiation-data-set",
    "href": "slides/x2-r-graphics.html#date-conversion-for-the-solar-radiation-data-set",
    "title": "x2-Graphics with R",
    "section": "Date conversion for the solar radiation data set",
    "text": "Date conversion for the solar radiation data set\n\nradiation$year <- format(radiation$Date, \"%Y\")\nradiation$month <- format(radiation$Date, \"%m\")\nradiation$doy <- format(radiation$Date, \"%j\")\nradiation$weekday <- format(radiation$Date, \"%u\")\n\nhead(radiation)\n\n        date rad interpolated       Date year month doy weekday\n1 1981-01-01 197            0 1981-01-01 1981    01 001       4\n2 1981-01-02  89            0 1981-01-02 1981    01 002       5\n3 1981-01-03  49            0 1981-01-03 1981    01 003       6\n4 1981-01-04 111            0 1981-01-04 1981    01 004       7\n5 1981-01-05 161            0 1981-01-05 1981    01 005       1\n6 1981-01-06  55            0 1981-01-06 1981    01 006       2\n\n\n\nThe lubridate package has date and time functions that are easier to use."
  },
  {
    "objectID": "slides/x2-r-graphics.html#summarize-data-with-aggregate",
    "href": "slides/x2-r-graphics.html#summarize-data-with-aggregate",
    "title": "x2-Graphics with R",
    "section": "Summarize data with aggregate",
    "text": "Summarize data with aggregate\nSyntax\n\naggregate(x, by, FUN, ..., simplify = TRUE)\n\nExample\n\nyearmax <- aggregate(\n  list(rad = radiation$rad),\n  list(year = radiation$year),\n  max)\n\nmonmean <- aggregate(\n  list(radiation = radiation$rad),\n  list(year = radiation$year, month = radiation$month),\n  mean)\n\n\naggregate is essentially a wrapper to apply"
  },
  {
    "objectID": "slides/x2-r-graphics.html#plot-aggregated-radiation-data",
    "href": "slides/x2-r-graphics.html#plot-aggregated-radiation-data",
    "title": "x2-Graphics with R",
    "section": "Plot aggregated radiation data",
    "text": "Plot aggregated radiation data\n\npar(mfrow=c(1,2), las=1)\nboxplot(rad ~ year, data = radiation)\nboxplot(rad ~ month, data = radiation)\n\n\nMost functions that support a formula argument (containing ~) allow to specify the data frame with a data argument."
  },
  {
    "objectID": "slides/x2-r-graphics.html#different-plotting-packages-with-different-philosophies",
    "href": "slides/x2-r-graphics.html#different-plotting-packages-with-different-philosophies",
    "title": "x2-Graphics with R",
    "section": "Different plotting packages with different philosophies",
    "text": "Different plotting packages with different philosophies\n\n\nbase graphics\npackage lattice\npackage ggplot2\nManipulation of plots\n\nset size and fonts; save plots to disk\nuse pdf, svg or png – not jpg - except for photographs\n\nRelated software\n\nedit/convert svg (and pdf) with Inkscape\nconvert images with ImageMagick"
  },
  {
    "objectID": "slides/x2-r-graphics.html#base-graphics-1",
    "href": "slides/x2-r-graphics.html#base-graphics-1",
    "title": "x2-Graphics with R",
    "section": "Base Graphics",
    "text": "Base Graphics\n\nx <- rnorm(100)\npar(mfrow=c(2,2))\nplot(x)\nhist(x)\nqqnorm(x)\nboxplot(x)"
  },
  {
    "objectID": "slides/x2-r-graphics.html#grid-and-gridbase",
    "href": "slides/x2-r-graphics.html#grid-and-gridbase",
    "title": "x2-Graphics with R",
    "section": "grid and gridBase",
    "text": "grid and gridBase\n\ncomplete freedom to organise plotting area\ninterface relatively raw\nbasis of other plotting packages"
  },
  {
    "objectID": "slides/x2-r-graphics.html#lattice-graphics",
    "href": "slides/x2-r-graphics.html#lattice-graphics",
    "title": "x2-Graphics with R",
    "section": "Lattice Graphics",
    "text": "Lattice Graphics\n\nImplements “trellis graphics” (i.e. gridded graphics) in R\nSarkar, D. (2008). Lattice: multivariate data visualization with R. Springer Science & Business Media.\n\n\nrequire(lattice)\ndata(iris)\nxyplot(Sepal.Length ~ Sepal.Width|Species, data=iris, layout=c(3,1))"
  },
  {
    "objectID": "slides/x2-r-graphics.html#ggplot2",
    "href": "slides/x2-r-graphics.html#ggplot2",
    "title": "x2-Graphics with R",
    "section": "ggplot2",
    "text": "ggplot2\n\n\nImplements the “Grammar of Graphics”\n\nLeland Wilkinson (2005) The Grammar of Graphics. 2nd edn. Springer\nHadley Wickham (2009, 2016) ggplot2: Elegant Graphics for Data Analysis. Springer.\n\nvery popular, part of the tidyverse family of packages\nhttps://ggplot2.tidyverse.org/"
  },
  {
    "objectID": "slides/x2-r-graphics.html#ggplot-example",
    "href": "slides/x2-r-graphics.html#ggplot-example",
    "title": "x2-Graphics with R",
    "section": "ggplot-Example",
    "text": "ggplot-Example\n\nlibrary(ggplot2)\ndata(iris)\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = factor(Species))) + \n  geom_point() + \n  stat_smooth(method = \"lm\")"
  },
  {
    "objectID": "slides/x2-r-graphics.html#pipelines-and-faceting-in-ggplot2",
    "href": "slides/x2-r-graphics.html#pipelines-and-faceting-in-ggplot2",
    "title": "x2-Graphics with R",
    "section": "Pipelines and faceting in ggplot2",
    "text": "Pipelines and faceting in ggplot2\n\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"ggplot2\")\nread.csv(\"../data/radiation.csv\") |>\n  mutate(year=year(date), doy=yday(date)) |>\n  ggplot(aes(doy, rad)) + geom_line() + facet_wrap(. ~ year)"
  },
  {
    "objectID": "slides/x2-r-graphics.html#save-images-to-disk",
    "href": "slides/x2-r-graphics.html#save-images-to-disk",
    "title": "x2-Graphics with R",
    "section": "Save Images to Disk",
    "text": "Save Images to Disk\n\nSimple: use of RStudio’s graphics export\nBetter:\n\n\npdf(file=\"figure.pdf\",width=8,height=6) # open\nplot(rnorm(100))                        # plotting\ndev.off()                               # close\n\n\nother graphics formats: png(), svg(), …\npdf allows several plots in one file (until dev.off)\nspecial formats possible, e.g. width=100, height=10 (useful for hclust)\nin case of error: repeated dev.off() to close all files."
  },
  {
    "objectID": "slides/x2-r-graphics.html#vector-vs.-bitmap-graphics",
    "href": "slides/x2-r-graphics.html#vector-vs.-bitmap-graphics",
    "title": "x2-Graphics with R",
    "section": "Vector vs. Bitmap Graphics",
    "text": "Vector vs. Bitmap Graphics\nBitmap formats\n\njpg, png, tiff\nfixed resolution, cannot be magnified without loss\nretouching possible, but not editing\nwell suited for pictures or plots with huge number of data (color maps)\ncannot be converted to vector without complications and quality loss\n\nVector formats\n\nsvg, pdf, [wmf, emf]\ncan be up- and downscaled and edited\nwell suited drawings and diagrams (except if huge amount of data)\ncan always be converted to bitmap"
  },
  {
    "objectID": "slides/x2-r-graphics.html#further-reading",
    "href": "slides/x2-r-graphics.html#further-reading",
    "title": "x2-Graphics with R",
    "section": "Further Reading",
    "text": "Further Reading\n\nMore presentations\n\nR Basics\nFunctions everywhere\n\nManuals\nMore details in the official R manuals, especially in An Introduction to R\nVideos\nMany videos can be found on Youtube, at the Posit webpage and somewhere else.\nThis tutorial was made with Quarto\nContact\nAuthor: tpetzoldt +++ Homepage +++ Github page"
  },
  {
    "objectID": "slides/x3-r-functions.html#functions-bring-life-to-the-r-language",
    "href": "slides/x3-r-functions.html#functions-bring-life-to-the-r-language",
    "title": "x3-Functions Everywhere",
    "section": "Functions bring life to the R language",
    "text": "Functions bring life to the R language\n\nsin(x), log(x), plot(x, y), summary(x), anova(lm.object), mean(x), monod(S, vmax, ks), simulate_phytoplankton(N, P, T, Zoo, ...)\n\nFunctions in R\n\nhave a name, followed by parenthesis ()\ncan have 1, 2 or more arguments (or no argument)\nusually return something (an object)\ncan have side-effects (e.g. plotting)"
  },
  {
    "objectID": "slides/x3-r-functions.html#what-are-functions",
    "href": "slides/x3-r-functions.html#what-are-functions",
    "title": "x3-Functions Everywhere",
    "section": "What are functions",
    "text": "What are functions\n\nParentheses and arguments\n\nall functions are followed by parentheses and arguments\nfunctions: log(x) par()\npar <- c(a=5, b=3)\n\n\\(\\rightarrow\\) here, par is a variable, c() a function\nReturn value and/or side effect\n\nsin(x), log(x), mean(x) are functions with return value\nprint(x), plot(x, y) are functions with side effect\nhist(x) is a function with both, side effect and return value\n\nPredefined and user-defined functions\n\npredefined: available in R\nuser defined: users become programmers"
  },
  {
    "objectID": "slides/x3-r-functions.html#arguments-of-functions",
    "href": "slides/x3-r-functions.html#arguments-of-functions",
    "title": "x3-Functions Everywhere",
    "section": "Arguments of functions",
    "text": "Arguments of functions\nUsage\n\ndnorm(x, mean = 0, sd = 1, log = FALSE)\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nqnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nrnorm(n, mean = 0, sd = 1)\n\n\n\n\n\n\n\n\nx, q\nvector of quantiles.\n\n\np\nvector of probabilities.\n\n\nn\nnumber of observations. If length(n) > 1, the length is taken to be …\n\n\nlog.p\nif TRUE, probabilities p are given as log(p).\n\n\nlower.tail\nif TRUE (default), …\n\n\n\nArguments\n\nrequired arguments: have no default\noptional arguments: have default values\nnamed arguments: argument mathing with = allows to specify arguments in arbitrary order\nargument order: arguments can occur without names when in defined order\n“…”: dots-arguments are passed down to other called functions"
  },
  {
    "objectID": "slides/x3-r-functions.html#examples",
    "href": "slides/x3-r-functions.html#examples",
    "title": "x3-Functions Everywhere",
    "section": "Examples",
    "text": "Examples\n\n\n\nrnorm(10)                       # x given, other arguments = defaults\nrnorm(10, 0, 1)                 # order matters\nrnorm(n = 10, mean = 0, sd = 1) # use argument names\nrnorm(10, sd = 1, mean = 0)     # named arguments in arbitrary order\nrnorm(10, m = 5, s = 1)         # abbreviated arguments: = bad style\nargs(rnorm)                     # all arguments from rnorm"
  },
  {
    "objectID": "slides/x3-r-functions.html#the-ellipsis-argument",
    "href": "slides/x3-r-functions.html#the-ellipsis-argument",
    "title": "x3-Functions Everywhere",
    "section": "The ellipsis argument",
    "text": "The ellipsis argument\n\nplot(x, y, ...)\n\n\nSome functions have a … argument, called “ellipsis”.\nThis means that additional arguments are passed to other functions.\nMakes R flexible and extensible, but is sometimes tricky.\n\n\npar(mfrow=c(1, 3))\nx <- 1:10; y <- rnorm(10)\nplot(x, y)\nplot(x, y, type = \"h\")\nplot(x, y, type = \"s\", col=\"red\")"
  },
  {
    "objectID": "slides/x3-r-functions.html#plot.default",
    "href": "slides/x3-r-functions.html#plot.default",
    "title": "x3-Functions Everywhere",
    "section": "plot.default",
    "text": "plot.default\n\nplot(x, y = NULL, type = \"p\",  xlim = NULL, ylim = NULL,\n     log = \"\", main = NULL, sub = NULL, xlab = NULL, ylab = NULL,\n     ann = par(\"ann\"), axes = TRUE, frame.plot = axes,\n     panel.first = NULL, panel.last = NULL, asp = NA, ...)\n\nObject orientation\n\nplot is a generic function\nworks automagic differently for different classes of objects\nplot.default is the basic function\n... see ?par for additional graphical parameters, e.g.:\n\n\n\n\ncol\ncolor\n\n\nbg\nbackground color for two-color symbols\n\n\npch\nsymbol (plotting character)\n\n\ncex\nsize of symbol (character extension)\n\n\nlty\nline type\n\n\nlwd\nline width"
  },
  {
    "objectID": "slides/x3-r-functions.html#a-user-defined-monod-function",
    "href": "slides/x3-r-functions.html#a-user-defined-monod-function",
    "title": "x3-Functions Everywhere",
    "section": "A user-defined Monod function",
    "text": "A user-defined Monod function\n\n\ndescribes substrate dependence of biochemical turnover\nwidely used in biochemistry and in models\ne.g. organic matter turnover in wastewater treatment\n\n\\[\nv = \\frac{v_{max} \\cdot S}{k_S + S}\n\\]\n\npar(mar=c(4,4,1,1))\npar(mfrow=c(3, 1))\nmonod <- function(S, vmax, ks) {\n  vmax * S / (ks + S)\n}\n\n\nS <- 1:10\nP <- seq(0, 20, 0.1)\nkP <- 5; mumax <- 1.2;\n\n## different ways to call the function\nplot(S, monod(S, 2, 2))                # simple call\nplot(P, monod(S=P, vmax=mumax, ks=kP)) # named arguments\nplot(P, monod(P, mumax, kP))           # argument position\n\n\nnames of caller and function can be different"
  },
  {
    "objectID": "slides/x3-r-functions.html#seasonal-light-intensity-in-dresden",
    "href": "slides/x3-r-functions.html#seasonal-light-intensity-in-dresden",
    "title": "x3-Functions Everywhere",
    "section": "Seasonal Light Intensity in Dresden",
    "text": "Seasonal Light Intensity in Dresden\n\n\\[\nI_t = 997 - 816 \\cos(2 \\pi t / 365) + 126 \\sin(2 \\pi t / 365)\n\\]\n\nFunctions as a knowledge base\n\nput knowledge in function and use it\nforget what is inside\n\n\n\nrad <- function(t) {\n  ## fill equation in\n}\n\nt <- 1:365\nplot(t, rad(t), type = \"l\")"
  },
  {
    "objectID": "slides/x3-r-functions.html#oxygen-saturation-in-fresh-and-sea-water",
    "href": "slides/x3-r-functions.html#oxygen-saturation-in-fresh-and-sea-water",
    "title": "x3-Functions Everywhere",
    "section": "Oxygen saturation in fresh and sea water",
    "text": "Oxygen saturation in fresh and sea water\n\\[\nc_{O_2, 100\\%} = ... ?\n\\]\n\no2sat <- function(t) {\n  K <- t + 273.15 # Celsius to Kelvin\n  exp(-139.34411 + (157570.1/K) - (66423080/K^2) +\n   (1.2438e+10/K^3) - (862194900000/K^4))\n}\n\no2sat(20)\n\n[1] 9.092426\n\n\n A more precise formula is found in package marelac\n\nlibrary(marelac)\ngas_O2sat(t = 20, S = 0, method = \"APHA\")\n\n[1] 9.092426\n\n\nconsult ?gas_O2sat for citations."
  },
  {
    "objectID": "slides/x3-r-functions.html#local-and-global-variables",
    "href": "slides/x3-r-functions.html#local-and-global-variables",
    "title": "x3-Functions Everywhere",
    "section": "Local and global variables",
    "text": "Local and global variables\n\nVariables in a function are local:\n\nnot visible from outside.\nno collisions with existing variables in the calling environment\n\nLexical Scoping\n\nfunctions can see variables of the calling function\nuseful for interactive work\ndangerous for (exported) functions in packages\nexcept in special cases, e.g. for functions within functions"
  },
  {
    "objectID": "slides/x3-r-functions.html#local-and-global-variables-ii",
    "href": "slides/x3-r-functions.html#local-and-global-variables-ii",
    "title": "x3-Functions Everywhere",
    "section": "Local and global variables II",
    "text": "Local and global variables II\n\n\nrm(list = ls()) # remove all objects\no2sat <- function(t) {\n  K <- t + 273.15 # Celsius to Kelvin\n  exp(-139.34411 + (157570.1/K) - (66423080/K^2) +\n   (1.2438e+10/K^3) - (862194900000/K^4))\n}\n\no2sat(20)\nK\n\nK <- 0\no2sat(20)\n\nNow outcomment:\n\n# K <- t + 273.15\n\nand try again."
  },
  {
    "objectID": "slides/x3-r-functions.html#logistic-growth",
    "href": "slides/x3-r-functions.html#logistic-growth",
    "title": "x3-Functions Everywhere",
    "section": "Logistic growth",
    "text": "Logistic growth\n\nThe logistic growth function describes saturated growth of a population abundance \\(N_t\\), dependent of an initial value \\(N_0\\), growth rate \\(r\\) and carrying capacity \\(K\\).\n\\[\nN_t = \\frac{K N_0 e^{rt}}{K + N_0 (e^{rt}-1)}\n\\]\n\nlogistic <- function(t, r, K, N0) {\n  K*N0*exp(r*t)/(K+N0*(exp(r*t)-1))\n}\n\n\nmu <- 0.1; K = 10; N0 = 0.1\ntimes <- 1:100"
  },
  {
    "objectID": "slides/x3-r-functions.html#functional-response-types-in-ecology",
    "href": "slides/x3-r-functions.html#functional-response-types-in-ecology",
    "title": "x3-Functions Everywhere",
    "section": "Functional response types in Ecology",
    "text": "Functional response types in Ecology\n\nHolling type I \\(P = \\min(k \\cdot N, P_{max})\\)\nHolling type II \\(P = \\frac{\\alpha N}{1 + \\alpha H N}\\)\nHolling type III \\(P = \\frac{\\alpha N^b}{1 + \\alpha H N^b}\\)\n\nwith\n\n\n\n\\(P\\)\npredation rate\n\n\n\\(N\\)\nabundance of prey\n\n\n\\(P_{max}\\)\nmaximum predation rate\n\n\n\\(k\\)\na constant\n\n\n\\(\\alpha\\)\nattack rate\n\n\n\\(H\\)\nhandling time\n\n\n\\(b\\)\nexponent \\(>1\\)\n\n\n\n\nWrite a function for each functional reponse type and plot it.\nWrite a universal function for all types."
  },
  {
    "objectID": "slides/x3-r-functions.html#further-reading",
    "href": "slides/x3-r-functions.html#further-reading",
    "title": "x3-Functions Everywhere",
    "section": "Further Reading",
    "text": "Further Reading\n\nMore presentations\n\nR Basics\nGraphics in R\n\nManuals\nMore details in the official R manuals, especially in An Introduction to R\nVideos\nMany videos can be found on Youtube, at the Posit webpage and somewhere else."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#prerequisites",
    "href": "slides/x4-pipes-intro.html#prerequisites",
    "title": "x4-Pipelines in R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nThe examples in slis slide require the following R packages: packages, that must be installed and loaded.\n\nInstallation\n\ninstall.packages(c(\"dplyr\", \"tiryr\", \"lubridate\", \"readxl\", \"ggplot2\"))\n\n\nLoading\n\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"readxl\")\nlibrary(\"ggplot2\")\n\n\nThe examples were tested with R 4.3.1 and RStudio 2023.06.1"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#an-introductory-example",
    "href": "slides/x4-pipes-intro.html#an-introductory-example",
    "title": "x4-Pipelines in R",
    "section": "An introductory example",
    "text": "An introductory example\n\nIn a statistics course, two samples of Maple (Acer platanoides) leaves were collected by two groups of students:\ngroup HSE: had the freedom to collect leaves individually from trees close to the institute\ngroup HYB: got a random sample from the supervisor\n\nHypothesis: sampling bias may affect statistical parameters, especially mean and variance.\n\nDownload the file leaves.csv, save it to a working directory and then read it with read.csv:\n\nleaves <- read.csv(\"leaves.csv\") \n\n\nHave a look at the data:\n\nhead(leaves)\n\n  group no length width stalk\n1   HSE  1     83    87    74\n2   HSE  2    130   153   105\n3   HSE  3    140   148   135\n4   HSE  4    102   110    94\n5   HSE  5    190   151    89\n6   HSE  6    225   139    91"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#a-boxplot",
    "href": "slides/x4-pipes-intro.html#a-boxplot",
    "title": "x4-Pipelines in R",
    "section": "A boxplot",
    "text": "A boxplot\n\n\n\nboxplot(width ~ group, data=leaves)"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics",
    "href": "slides/x4-pipes-intro.html#summary-statistics",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics",
    "text": "Summary statistics\n\n\nsummary(leaves)\n\n    group                 no             length           width      \n Length:126         Min.   :  1.00   Min.   : 37.00   Min.   : 44.0  \n Class :character   1st Qu.: 32.25   1st Qu.: 72.00   1st Qu.: 96.0  \n Mode  :character   Median : 63.50   Median : 90.00   Median :118.5  \n                    Mean   : 63.50   Mean   : 95.83   Mean   :117.4  \n                    3rd Qu.: 94.75   3rd Qu.:102.00   3rd Qu.:138.5  \n                    Max.   :126.00   Max.   :250.00   Max.   :199.0  \n     stalk       \n Min.   : 29.00  \n 1st Qu.: 62.00  \n Median : 82.00  \n Mean   : 81.89  \n 3rd Qu.:101.00  \n Max.   :175.00"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-per-group",
    "href": "slides/x4-pipes-intro.html#summary-statistics-per-group",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics per group",
    "text": "Summary statistics per group\n\n\ndifferent ways to calculate summary statistics\nclassical method with aggregate\n\nA few examples\n\naggregate(cbind(length, width, stalk) ~ group, mean, data=leaves)\n\n  group    length    width     stalk\n1   HSE 137.82857 139.0286 102.02857\n2   HYB  79.67033 109.1319  74.14286\n\naggregate(cbind(length, width, stalk) ~ group, sd, data=leaves)\n\n  group   length    width    stalk\n1   HSE 47.99356 26.07285 26.47917\n2   HYB 19.93771 30.90423 25.24087\n\naggregate(cbind(length, width, stalk) ~ group, min, data=leaves)\n\n  group length width stalk\n1   HSE     83    87    52\n2   HYB     37    44    29\n\n\n\naggregate is very powerful, but the modern “tidyverse” approach is easier to understand. This is explained in the following."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-with-the-dplyr-package",
    "href": "slides/x4-pipes-intro.html#summary-statistics-with-the-dplyr-package",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics with the dplyr-package",
    "text": "Summary statistics with the dplyr-package\nPackage dplyr contains two handy functions:\n\ngroup_by\nsummarize\n\nThe functions can be combined in different ways:\nA) Two separate code lines\n\nleaves_grouped <- group_by(leaves, group)\nsummarize(leaves_grouped, mean = mean(width), sd = sd(width), min = min(width), max = max(width))\n\n\\(\\ominus\\) needs a temporary variable: leaves_grouped\n B) One line, group_by enclosed in parentheses\n\nsummarize(group_by(leaves, group), mean=mean(width), sd=sd(width), min=min(width), max=max(width))\n\n\\(\\oplus\\) no temporary variables necessary  \\(\\ominus\\) nested parentheses"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#more-streamlined-pipelines",
    "href": "slides/x4-pipes-intro.html#more-streamlined-pipelines",
    "title": "x4-Pipelines in R",
    "section": "More streamlined: pipelines",
    "text": "More streamlined: pipelines\n\nAim to make code easier to understand:\n\navoids nested parentheses\navoids temporary variables\n\nRecent versions of R (since 4.1) have built-in support for pipelines:\n\nNative pipeline operator |>\n\nA predecessor was the so-called “magrittr” pipeline operator %>%\n\nMost examples from these slides will work with both pipeline operators.\nThere are a few differences regarding use of so-called placeholders.\nI recommended to prefer native pipes |>\n\n\n\nThe `%>%-pipeline was introduced by the user-contributed magrittr package (Bache & Wickham, 2022) and became very popular. It is automatically loaded by the dplyr package (Wickham, François, et al., 2023). In his new book, Hadley Wickham recommends native pipes (Wickham, Çetinkaya-Rundel, et al., 2023)."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#application-of",
    "href": "slides/x4-pipes-intro.html#application-of",
    "title": "x4-Pipelines in R",
    "section": "Application of |>",
    "text": "Application of |>\n\nThe output from the first function is piped to the next\n\ngroup_by(leaves, group) |>\nsummarize(mean = mean(width), sd = sd(width), \n          min = min(width), max = max(width))\n\n\nOr, even more streamlined: start pipeline with the data frame\n\nleaves |>\n  group_by(group) |>\n  summarize(mean = mean(width), sd = sd(width), \n            min = min(width), max = max(width))\n\n# A tibble: 2 × 5\n  group  mean    sd   min   max\n  <chr> <dbl> <dbl> <int> <int>\n1 HSE    139.  26.1    87   195\n2 HYB    109.  30.9    44   199"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#how-it-works",
    "href": "slides/x4-pipes-intro.html#how-it-works",
    "title": "x4-Pipelines in R",
    "section": "How it works",
    "text": "How it works\nThe pipe operator |> inserts the output from one function into the first argument of the next function.\n\n\nClassical functional style\n\ngroup_by(leaves, group)\n\n\n\n\n\n\n\n\n\nPipeline style\n\nleaves |> group_by(group)"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables",
    "href": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics for all variables",
    "text": "Summary statistics for all variables\n\nThe leaves dataset contains different variables length, width and stalk length.\nWe can now, in principle, extend summarize:\n\nAdd more code rows\n\nleaves |>\n  group_by(group) |>\n  summarize(mean_l=mean(width),  sd_l=sd(width),  min_l=min(width),  max_l=max(width),\n            mean_w=mean(length), sd_w=sd(length), min_w=min(length), max_w=max(length),\n            mean_s=mean(stalk),  sd_s=sd(stalk),  min_s=min(stalk),  max_s=max(stalk)\n  )\n\n\nIs copy and paste a good idea?\nNo, at least not in excess.\n\nCopy and paste can lead to errors.\n… and there are more compact and elegant ways."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#tidy-your-data-and-use-long-data-formats",
    "href": "slides/x4-pipes-intro.html#tidy-your-data-and-use-long-data-formats",
    "title": "x4-Pipelines in R",
    "section": "Tidy your data and use “long” data formats!",
    "text": "Tidy your data and use “long” data formats!\n\n\nLong data formats are more database like and more flexible.\n\nIf you are used to working with LibeOffiice or Excel, you will probably prefer “wide” tables that fit well on the computer screen. However, this is not such a good idea for data bases and scripted data science.\nModern data analysis packages like dplyr and ggplot2 mandatorily require the long format."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#long-data-format-tidy-format",
    "href": "slides/x4-pipes-intro.html#long-data-format-tidy-format",
    "title": "x4-Pipelines in R",
    "section": "Long data format (= tidy format)",
    "text": "Long data format (= tidy format)\n\nPut data from all 3 variables in one column: length, width, stalk \\(\\rightarrow\\) value\nIdentifier column for the variables: name\n\n\nWide format\n\n\n\n\n\ngroup\nno\nlength\nwidth\nstalk\n\n\n\n\nHSE\n1\n83\n87\n74\n\n\nHSE\n2\n130\n153\n105\n\n\nHSE\n3\n140\n148\n135\n\n\nHSE\n4\n102\n110\n94\n\n\nHSE\n5\n190\n151\n89\n\n\nHSE\n6\n225\n139\n91\n\n\nHSE\n7\n195\n165\n76\n\n\nHSE\n8\n216\n135\n113\n\n\nHSE\n9\n250\n195\n119\n\n\nHSE\n10\n152\n168\n158\n\n\n\n\n\n\n\nLong format\n\n\n\n\n\ngroup\nno\nname\nvalue\n\n\n\n\nHSE\n1\nlength\n83\n\n\nHSE\n1\nwidth\n87\n\n\nHSE\n1\nstalk\n74\n\n\nHSE\n2\nlength\n130\n\n\nHSE\n2\nwidth\n153\n\n\nHSE\n2\nstalk\n105\n\n\nHSE\n3\nlength\n140\n\n\nHSE\n3\nwidth\n148\n\n\nHSE\n3\nstalk\n135\n\n\nHSE\n4\nlength\n102\n\n\n\n\n\n… … …"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#long-data-format-with-pivot_longer",
    "href": "slides/x4-pipes-intro.html#long-data-format-with-pivot_longer",
    "title": "x4-Pipelines in R",
    "section": "Long data format with pivot_longer",
    "text": "Long data format with pivot_longer\n\n\n\nleaves |> \n  pivot_longer(c(\"length\", \"width\", \"stalk\"))\n\n# A tibble: 378 × 4\n   group    no name   value\n   <chr> <int> <chr>  <int>\n 1 HSE       1 length    83\n 2 HSE       1 width     87\n 3 HSE       1 stalk     74\n 4 HSE       2 length   130\n 5 HSE       2 width    153\n 6 HSE       2 stalk    105\n 7 HSE       3 length   140\n 8 HSE       3 width    148\n 9 HSE       3 stalk    135\n10 HSE       4 length   102\n# ℹ 368 more rows"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables-groupwise",
    "href": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables-groupwise",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics for all variables groupwise",
    "text": "Summary statistics for all variables groupwise\n\n\n\nleaves |> \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |>\n  group_by(group, name) |>\n  summarize(mean = mean(value), \n            sd   = sd(value), \n            min  = min(value),\n            max  = max(value))\n\n# A tibble: 6 × 6\n# Groups:   group [2]\n  group name    mean    sd   min   max\n  <chr> <chr>  <dbl> <dbl> <int> <int>\n1 HSE   length 138.   48.0    83   250\n2 HSE   stalk  102.   26.5    52   175\n3 HSE   width  139.   26.1    87   195\n4 HYB   length  79.7  19.9    37   124\n5 HYB   stalk   74.1  25.2    29   144\n6 HYB   width  109.   30.9    44   199"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#pipes-and-the-assignment-operator--",
    "href": "slides/x4-pipes-intro.html#pipes-and-the-assignment-operator--",
    "title": "x4-Pipelines in R",
    "section": "Pipes and the assignment operator <-",
    "text": "Pipes and the assignment operator <-\n\n\nIn the examples before, the pipe-output was directly printed to the screen\nIf we need the result in a subsequent operation, we assign it to a variable as usual with <-\n\n\n\ntotals <- \n  leaves |> \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |>\n  group_by(group, name) |>\n  summarize(mean=mean(value), sd=sd(value), min=min(value), max=max(value))\n\n\nDon’t get confused!\n\nthe pipe starts with leaves in the second code line\nthe direction of the pipeline is from left \\(\\rightarrow\\) right\nthen the of the complete pipeline is assigned to totals\n\nIt follows the convention, that the result of an equation is assigned from right to the left."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#reverse-assignment",
    "href": "slides/x4-pipes-intro.html#reverse-assignment",
    "title": "x4-Pipelines in R",
    "section": "Reverse assignment?",
    "text": "Reverse assignment?\n\n“More logical”, but less common would be a consequent left to the right notation with ->\n\n\nleaves |> \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |>\n  group_by(group, name) |>\n  summarize(mean=mean(value), sd=sd(value), min=min(value), max=max(value)) ->\n  totals\n\n\nAmelia McNamara used this style in her keynote talk at the 2020 use!R conference about Speaking R on youtube.\nBut, Headley Wickham discouraged this style, because the -> breaks with mathematical convention and is difficult to spot in the code."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#indentation",
    "href": "slides/x4-pipes-intro.html#indentation",
    "title": "x4-Pipelines in R",
    "section": "Indentation",
    "text": "Indentation\n\n\ntotals <- \n  leaves |> \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |>\n  group_by(group, name) |>\n  summarize(mean=mean(value), sd=sd(value), min=min(value), max=max(value))\n\n\nThe pipeline above shows essentially one single line of code.\n\nTo improve readability, code lines should not be longer than 80 characters.\nRemember: Line breaks can be at any position, as long as a code line is not complete.\nCommon style: make a newline after <- and |> and use 2 characters for indentation."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#the-clementine-orange-data-set",
    "href": "slides/x4-pipes-intro.html#the-clementine-orange-data-set",
    "title": "x4-Pipelines in R",
    "section": "The Clementine orange data set",
    "text": "The Clementine orange data set"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#clementine-orange-data-set",
    "href": "slides/x4-pipes-intro.html#clementine-orange-data-set",
    "title": "x4-Pipelines in R",
    "section": "Clementine orange data set",
    "text": "Clementine orange data set\n\n\nSamples of clementine oranges, measured, weighed and consumed in a statistic course.\nExcel file with two tables:\n\nlong table with the fruits\nshorter table brands with meta data\n\nData can be downloaded from here.\n\n\nRead data directly from Excel file\n\nbrands  <- read_excel(\"clementines_2019.xlsx\", \"Brands\")\nfruits  <- read_excel(\"clementines_2019.xlsx\", \"Fruits\")"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#clementine-orange-data-set-1",
    "href": "slides/x4-pipes-intro.html#clementine-orange-data-set-1",
    "title": "x4-Pipelines in R",
    "section": "Clementine orange data set",
    "text": "Clementine orange data set\n\n\nTable “fruits”\n\n\n\n\n\nyear\nbrand\nweight\nwidth\nheight\n\n\n\n\n2019\nEB\n107\n61.0\n54\n\n\n2019\nEB\n100\n65.0\n55\n\n\n2019\nEB\n89\n58.0\n49\n\n\n2019\nEB\n99\n62.0\n52\n\n\n2019\nEB\n99\n64.0\n58\n\n\n2019\nEB\n96\n63.0\n59\n\n\n2019\nEB\n100\n54.0\n54\n\n\n2019\nEB\n89\n58.5\n47\n\n\n2019\nEB\n86\n66.0\n48\n\n\n2019\nEB\n92\n43.0\n33\n\n\n2019\nEB\n102\n48.0\n33\n\n\n2019\nEB\n92\n43.0\n32\n\n\n\n\n\n\n\nTable “brands”\n\n\n\n\n\nyear\nbrand\ntype\nkilogram\nprice\n\n\n\n\n2019\nEB\nBasic\n1.50\n2.99\n\n\n2019\nEP\nGold\n0.75\n1.49\n\n\n2019\nLB\nBasic\n1.00\n1.49\n\n\n2019\nLO\nBio\n0.50\n1.49\n\n\n2019\nNX\nBox\n2.30\n2.99\n\n\n2019\nNP\nPremium\n1.00\n2.29\n\n\n2019\nNB\nBasic\n1.00\n1.49\n\n\n2019\nNC\nBasic\n1.00\n1.49"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#database-join",
    "href": "slides/x4-pipes-intro.html#database-join",
    "title": "x4-Pipelines in R",
    "section": "Database join",
    "text": "Database join\n\njoin two tables to bring the information together\nIn case of left_join, the (larger) main table is at the left.\nThe tables have two key fields in common: year and brand.\nThe key fields can be automatically detected or explicitly specified or renamed \\(\\rightarrow\\) help page of left_join\n\n\n\nfruits2 <- left_join(fruits, brands)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nno\nyear\nbrand\nweight\nwidth\nheight\ntaste\nshop\ntype\nquality\nkilogram\nprice\n\n\n\n\n1\n2019\nEB\n107\n61\n54\n6\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n2\n2019\nEB\n100\n65\n55\n7\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n3\n2019\nEB\n89\n58\n49\n10\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n4\n2019\nEB\n99\n62\n52\n7\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n5\n2019\nEB\n99\n64\n58\n8\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n6\n2019\nEB\n96\n63\n59\n8\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n7\n2019\nEB\n100\n54\n54\n7\nEdeka\nBasic\nBasic\n1.5\n2.99"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#selection-of-columns-and-rows-select-and-filter",
    "href": "slides/x4-pipes-intro.html#selection-of-columns-and-rows-select-and-filter",
    "title": "x4-Pipelines in R",
    "section": "Selection of columns and rows: select and filter",
    "text": "Selection of columns and rows: select and filter\n\n\nselect: select columns\nfilter: filters rows\n\n\n\n\nfruits2 |>\n  select(brand, shop, type, weight, width) |>\n  filter(shop %in% c(\"Edeka\", \"Lidl\"))\n\n# A tibble: 44 × 5\n   brand shop  type  weight width\n   <chr> <chr> <chr>  <dbl> <dbl>\n 1 EB    Edeka Basic    107  61  \n 2 EB    Edeka Basic    100  65  \n 3 EB    Edeka Basic     89  58  \n 4 EB    Edeka Basic     99  62  \n 5 EB    Edeka Basic     99  64  \n 6 EB    Edeka Basic     96  63  \n 7 EB    Edeka Basic    100  54  \n 8 EB    Edeka Basic     89  58.5\n 9 EB    Edeka Basic     86  66  \n10 EB    Edeka Basic     92  43  \n# ℹ 34 more rows"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#create-or-transform-columns-mutate",
    "href": "slides/x4-pipes-intro.html#create-or-transform-columns-mutate",
    "title": "x4-Pipelines in R",
    "section": "Create or transform columns: mutate",
    "text": "Create or transform columns: mutate\n\nExample\nTransform a variable, e.g. weight by \\(x^{1/3}\\) into a theoretical mean diameter\n\nfruits2 <- \n  fruits2 |>\n  mutate(L_mean = weight^(1/3))\n\n\n\nShow results\nClassical boxplot\n\n\nboxplot(L_mean ~ brand, data=fruits2)"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#plot-in-tidyverse-style-with-pipes-and-ggplot",
    "href": "slides/x4-pipes-intro.html#plot-in-tidyverse-style-with-pipes-and-ggplot",
    "title": "x4-Pipelines in R",
    "section": "Plot in “tidyverse”-style with pipes and ggplot",
    "text": "Plot in “tidyverse”-style with pipes and ggplot\n\nfruits2 |>\n  mutate(L_mean = weight^(1/3)) |>\n  ggplot(aes(brand, L_mean)) + geom_boxplot()"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#another-mutate-example",
    "href": "slides/x4-pipes-intro.html#another-mutate-example",
    "title": "x4-Pipelines in R",
    "section": "Another mutate example",
    "text": "Another mutate example\nLet’s compare the measured weight of our fruits with a “theoretical volume” calculated from length and height using the formula of an ellipsoid. This is of course an approximation:\n\\[\nV = 4/3 \\pi \\cdot \\rm (length/2)^2 \\cdot height/2\n\\]\n\nfruits <-\n  fruits |>\n  mutate(V = 0.001 * 4/3 * pi * (width/2)^2 * height/2, index = weight / V)\n\n\n\n\nlibrary(ggplot2)\nfruits |>\n  ggplot(aes(weight, index)) + \n  geom_point()\n\nThe “+” operator in ggplot looks like a pipeline, but works differently.\nIt adds elements to a plot."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#color-coded-points",
    "href": "slides/x4-pipes-intro.html#color-coded-points",
    "title": "x4-Pipelines in R",
    "section": "Color coded points",
    "text": "Color coded points\n\nlibrary(ggplot2)\nfruits |> ggplot(aes(weight, index, color=brand)) + geom_point()"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#categorial-split-faceting-and-regression-line",
    "href": "slides/x4-pipes-intro.html#categorial-split-faceting-and-regression-line",
    "title": "x4-Pipelines in R",
    "section": "Categorial split (faceting) and regression line",
    "text": "Categorial split (faceting) and regression line\n\nfruits |> \n  ggplot(aes(weight, V)) + \n  geom_point() + \n  geom_smooth(method=lm, se=FALSE) + \n  facet_wrap( ~ brand)"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#modify-font-size",
    "href": "slides/x4-pipes-intro.html#modify-font-size",
    "title": "x4-Pipelines in R",
    "section": "Modify font size",
    "text": "Modify font size\n\nfruits |> \n  ggplot(aes(weight, V)) + \n  geom_point() + \n  geom_smooth(method=lm, se=FALSE) + \n  facet_wrap( ~ brand) +\n  theme(text = element_text(size=24))\n\n\n\\(\\rightarrow\\) themes allow to configure “almost everything” …"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#discharge-of-the-elbe-river",
    "href": "slides/x4-pipes-intro.html#discharge-of-the-elbe-river",
    "title": "x4-Pipelines in R",
    "section": "Discharge of the Elbe River",
    "text": "Discharge of the Elbe River\nElbe River in Dresden 2006-04-01"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#read-data-to-r",
    "href": "slides/x4-pipes-intro.html#read-data-to-r",
    "title": "x4-Pipelines in R",
    "section": "Read data to R",
    "text": "Read data to R\n\nThe example file elbe.csv contains daily discharge of the Elbe River in \\(\\mathrm{m^3 s^{-1}}\\) from gauging station Dresden, river km 55.6. The data are from the Federal Waterways and Shipping Administration (WSV) and where provided by the Federal Institute for Hydrology (BfG).\n\nWe can skip downloading and read the file directly from its internet location:\n\n\nelbe <- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/elbe.csv\")\n\n\n\n\n\nThe third column “validated” indicate whether the values were finally approved by WSV and BfG. Data of the 19th century are particularly uncertain. Please consult the file elbe_info.txt for details."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#date-and-time-conversion",
    "href": "slides/x4-pipes-intro.html#date-and-time-conversion",
    "title": "x4-Pipelines in R",
    "section": "Date and time conversion",
    "text": "Date and time conversion\n\nNow, let’s extend the elbe data frame by adding information about the day, month, year and day of year. Here function mutate adds additional columns.\nNote also that the day of year function in the date and time package lubridate is named yday.\nDetails about date and time conversion can be found in the lubridate cheatsheet.\n\n\nlibrary(lubridate) # a tidyverse package for dates\nelbe <- mutate(elbe,\n               date  = as.POSIXct(date),\n               day   = day(date), \n               month = month(date), \n               year  = year(date), \n               doy   = yday(date))"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#inspect-data-structure",
    "href": "slides/x4-pipes-intro.html#inspect-data-structure",
    "title": "x4-Pipelines in R",
    "section": "Inspect data structure",
    "text": "Inspect data structure\nIf we work with RStudio, may have a look at the “Global Environment” pane and inspect the data structure of the elbe data frame.\n\n\n\n\n\ndate\ndischarge\nvalidated\nday\nmonth\nyear\ndoy\n\n\n\n\n1989-01-01\n765\nTRUE\n1\n1\n1989\n1\n\n\n1989-01-02\n713\nTRUE\n2\n1\n1989\n2\n\n\n1989-01-03\n684\nTRUE\n3\n1\n1989\n3\n\n\n1989-01-04\n612\nTRUE\n4\n1\n1989\n4\n\n\n1989-01-05\n565\nTRUE\n5\n1\n1989\n5\n\n\n1989-01-06\n519\nTRUE\n6\n1\n1989\n6\n\n\n1989-01-07\n522\nTRUE\n7\n1\n1989\n7\n\n\n1989-01-08\n524\nTRUE\n8\n1\n1989\n8\n\n\n1989-01-09\n544\nTRUE\n9\n1\n1989\n9\n\n\n1989-01-10\n539\nTRUE\n10\n1\n1989\n10\n\n\n1989-01-11\n606\nTRUE\n11\n1\n1989\n11\n\n\n1989-01-12\n606\nTRUE\n12\n1\n1989\n12"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#annual-summary-statistics",
    "href": "slides/x4-pipes-intro.html#annual-summary-statistics",
    "title": "x4-Pipelines in R",
    "section": "Annual summary statistics",
    "text": "Annual summary statistics\n\nSummarize data\n\n## calculate annual mean, minimum, maximum\ntotals <- elbe |>\n  group_by(year) |>\n  summarize(mean = mean(discharge), \n            min = min(discharge), \n            max = max(discharge))\n\n\nShow table of summary statistics\n\nhead(totals)\n\n# A tibble: 6 × 4\n   year  mean   min   max\n  <dbl> <dbl> <dbl> <dbl>\n1  1989  268.   122   765\n2  1990  217.    89   885\n3  1991  189.    97   634\n4  1992  267.    89  1090\n5  1993  256.    92  1610\n6  1994  317.    92  1030\n\n\n Exercise: Compute monthly discharge mean values and monthly sums."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#more-about-pivot-tables",
    "href": "slides/x4-pipes-intro.html#more-about-pivot-tables",
    "title": "x4-Pipelines in R",
    "section": "More about pivot tables",
    "text": "More about pivot tables\n\n\nIn a section before, we already used pivot_longer to reorganize data. Now we do the opposite and convert a data base table (long data format) into a cross-table (wide data format) and vice versa.\nR provides several function pairs for this, so you may see functions like melt and cast or gather and spread.\nRecently the two functions pivot_wider and pivot_longer were recommended for this purpose.\n\nThe first argument is a data base table, the other arguments define the structure of the desired crosstable.\nid_cols is the name of a column in a long table that will become the rows\nnames_from indicates where the names of the columns are taken from\nvalues_from is the column with the values for the cross table.\n\n\n\\(\\rightarrow\\) If more than one value exists for a row x column combination, an optional aggregation function values_fn can be given."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#crosstable-with-one-column-per-year",
    "href": "slides/x4-pipes-intro.html#crosstable-with-one-column-per-year",
    "title": "x4-Pipelines in R",
    "section": "Crosstable with one column per year",
    "text": "Crosstable with one column per year\n\n\nelbe_wide <-  elbe |>\n  pivot_wider(id_cols = doy, \n              names_from = year, \n              values_from = discharge, \n              values_fn = mean)\nelbe_wide\n\n\nExercise\n\nCreate a suitable crosstable elbe_wide.\nThen create a crosstable for monthly maximum discharge over all years."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "href": "slides/x4-pipes-intro.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "title": "x4-Pipelines in R",
    "section": "Back-conversion of a crosstable into a data base table",
    "text": "Back-conversion of a crosstable into a data base table\n\nThe inverse case is also possible, e.g. the conversion of a cross table into a data base table. It can be done with the function pivot_longer. The column of the id.vars variable(s) will become identifier(s) downwards.\n\n\npivot_longer(elbe_wide, names_to=\"year\", cols=as.character(1989:2019))\n\n\nExercise\nTry it yourself."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "href": "slides/x4-pipes-intro.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "title": "x4-Pipelines in R",
    "section": "Minimum-Maximum plot with summarize and ggplot2",
    "text": "Minimum-Maximum plot with summarize and ggplot2\n\nelbe |> \n  mutate(doy = yday(date)) |>\n  group_by(doy) |>\n  summarize(max = max(discharge), \n            mean = mean(discharge), \n            min = min(discharge)) |>\n  pivot_longer(cols = c(\"min\", \"mean\", \"max\"), \n               names_to = \"statistic\", \n               values_to = \"discharge\") |>\n  ggplot(aes(doy, discharge, color = statistic)) + geom_line()\n\n\nExercise\n\nRead the code and try to understand it. Then add a dry and a wet year."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#cumulative-sums",
    "href": "slides/x4-pipes-intro.html#cumulative-sums",
    "title": "x4-Pipelines in R",
    "section": "Cumulative sums",
    "text": "Cumulative sums\nAnnual cumulative sum plots are a hydrological standard tool used by reservoir managers. We can use the R function cumsum, that by successive cumulation converts a sequence of:\n\\(x_1, x_2, x_3, x_4, \\dots\\)\ninto\n\\((x_1), (x_1+x_2), (x_1+x_2+x_3), (x_1+x_2+x_3+x_4), \\dots\\)\n\nExample\n\nx <- c(1, 3, 2, 6, 4, 2, 3)\ncumsum(x)\n\n[1]  1  4  6 12 16 18 21"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#cumulative-sums-of-the-elbe-river",
    "href": "slides/x4-pipes-intro.html#cumulative-sums-of-the-elbe-river",
    "title": "x4-Pipelines in R",
    "section": "Cumulative sums of the Elbe River",
    "text": "Cumulative sums of the Elbe River\nCummulative sums allow to detect dry and wet years, or periods within years.\nIf we just use cumsum, we get a cumulative sum for all years:\n\nelbe |> \n  mutate(doy = yday(date), year = year(date)) |>\n  filter(year %in% 2000:2010) |>\n  group_by(year = factor(year)) |>\n  mutate(cum_discharge = cumsum(discharge) * 60*60*24) |>\n  ggplot(aes(doy, cum_discharge, color = year)) + geom_line()\n\n\nThe multiplication with \\(60 \\cdot 60 \\cdot 24\\) converts \\(\\rm m^3 s^{-1}\\) in \\(\\rm m^3 d^{-1}\\)."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#exercises",
    "href": "slides/x4-pipes-intro.html#exercises",
    "title": "x4-Pipelines in R",
    "section": "Exercises",
    "text": "Exercises\n\n\nRepeat the same for other time periods (years).\nWhich year was the wettest, which one the driest year in total? Find a year with dry spring and wet summer.\nIdentify some (e.g. 3 or 5) large floods in the historical time series and plot it together.\nModify the commands so that the hydrological year is shown. Note that the German hydrological year goes from 1st November to 31st October of the following year. Other countries have different regulations."
  },
  {
    "objectID": "slides/x4-pipes-intro.html#further-reading",
    "href": "slides/x4-pipes-intro.html#further-reading",
    "title": "x4-Pipelines in R",
    "section": "Further reading",
    "text": "Further reading\n\nOnline material\n\n“Welcome to tidyverse: https://dplyr.tidyverse.org/.\n“ggplot Elegant Graphics for Data Analysis: https://ggplot2-book.org/\n“R for Data Science”: https://r4ds.had.co.nz/\nHadley Wickham’s homepage: https://hadley.nz/\n\n\nPrinted books\n\n“R for Data Science” (Wickham, Çetinkaya-Rundel, et al., 2023)\n“ggplot Elegant Graphics for Data Analysis” (Wickham, 2016)"
  },
  {
    "objectID": "slides/x4-pipes-intro.html#references",
    "href": "slides/x4-pipes-intro.html#references",
    "title": "x4-Pipelines in R",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBache, S. M., & Wickham, H. (2022). Magrittr: A forward-pipe operator for r. https://CRAN.R-project.org/package=magrittr\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science (2nd ed.). O’Reiley. https://r4ds.hadley.nz/\n\n\nWickham, H., François, R., Henry, L., Müller, K., & Vaughan, D. (2023). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html",
    "href": "tutorials/s1-introductory-r-session.html",
    "title": "An Introductory R Session",
    "section": "",
    "text": "This tutorial is available in HTML format for screen reading and PDF for printing."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#program-start-and-help-system",
    "href": "tutorials/s1-introductory-r-session.html#program-start-and-help-system",
    "title": "An Introductory R Session",
    "section": "2.1 Program start and help system",
    "text": "2.1 Program start and help system\nThe easiest way to learn R is the creative understanding and modification of given examples, the usage of R for solving practical problems and the diagnosis of the frequently occurring problems and error messages. Don’t worry: error messages are a normal phenomenon in scientific computing and not an indication of a dysfunction of the computer or the human brain. The opposite is true, a certain amount of stress hormones helps to acquire permanent learning effects. Then, after a certain level of experience reading the official R-Documentation “An Introduction to R” (Venables et al., 2021). or any good R-book is strongly recommended.\nThe first sections of this “crash course” are intended to give an overview over some of the most important elements of R and an insight into a typical work flow, that may be useful for the first statistical analyses and as a starting point for self-education.\nWe begin our first session by starting RStudio, a platform independent interface that makes working with R easier. RStudio divides the screen into 3 (resp. 4) windows (called panes), where some of them have additional tabs to switch between different views.\n\nFigure 1: R Studio with 4 panes. Use File – New R Script to open the the source code pane (shown top left). Then enter some code and don’t forget to explore the help files.\nIn a fresh RStudio session, one “Pane” should be the main help page of R. It is a good idea to browse a little bit around to get an impression about the amount and the typical style of the available help topics. The most important sections are “An Introduction to R”, “Search Engine & Keywords”, “Packages”, the “Frequently Asked Questions” and possibly “R Data Import/Export”.\nWe start now to explore the R-System itself."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#r-as-a-pocket-calculator",
    "href": "tutorials/s1-introductory-r-session.html#r-as-a-pocket-calculator",
    "title": "An Introductory R Session",
    "section": "2.2 R as a pocket calculator",
    "text": "2.2 R as a pocket calculator\nEntering an arithmetic expression like this:\n\n2 + 4\n\nshows that R can be used as a pocket calculator, that immediately outputs the result:\n\n\n[1] 6\n\n\nInstead of printing the result to the screen, it is also possible to save the result into a named variable using the assignment operator “&lt;-”.\n\na &lt;- 2 + 4\n\nIt seems that nothing happens, but the result is now saved in the variable a that can be recalled at any time by entering the variable name alone:\n\na\n\nVariable names in R start always with a character (or for special purposes a dot), followed by further characters, numerals, dots or underscores, where a distinction is made between small and capital letters, i.e. the variables value, Value and VALUE can contain different data. A few character combinations are reserved words and cannot be used as variables:\nbreak, for, function, if, in, next, repeat, while and “...” (three dots).\nOther identifiers like plot can be re-defined, but this should be done with care to avoid unwanted confusion and side effects."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#vectors",
    "href": "tutorials/s1-introductory-r-session.html#vectors",
    "title": "An Introductory R Session",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nYou may have noticed, that the output of the example above had a leading [1], which means that the line begins with the first element of a. This brings us to a very important feature of R that variables can contain more than single values: vectors, matrices, lists, data frames (tables) and so on.\nThe most basic data type is the vector, that can be filled with data using the c (combine) function:\n\nvalues &lt;- c(2, 3, 5, 7, 8.3, 10)\nvalues\n\n[1]  2.0  3.0  5.0  7.0  8.3 10.0\n\n\nTo create a sequence of values, one can use the : (colon):\n\nx &lt;- 1:10\nx\n\nor, even more flexibly the seq function:\n\nx &lt;- seq(2, 4, 0.25)\nx\n\nSequences of repeated equal values can be obtained with rep:\n\nx &lt;- rep(2, 4)\nx"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercise",
    "href": "tutorials/s1-introductory-r-session.html#exercise",
    "title": "An Introductory R Session",
    "section": "2.4 Exercise",
    "text": "2.4 Exercise\nThere are many ways to use these functions, try for example:\n\nseq(0, 10)\nseq(0, 10, by = 2)\nseq(0, pi, length = 12)\nrep(c(0, 1, 2, 4, 9), times = 5)\nrep(c(0, 1, 2, 4, 9), each = 2)\nrep(c(0, 1, 2, 4, 9), each = 2, times = 5)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#access-to-vector-elements",
    "href": "tutorials/s1-introductory-r-session.html#access-to-vector-elements",
    "title": "An Introductory R Session",
    "section": "2.5 Access to vector elements",
    "text": "2.5 Access to vector elements\nInstead of accessing vectors as a whole, it is also possible to extract single elements, where the index of the requested data is itself a vector:\n\nvalues[5]\nvalues[2:4]\nvalues[c(1, 3, 5)]\n\nSometimes, elements of a vector may have individual names, which makes it easy to access them:\n\nnamed &lt;- c(a = 1, b = 2.3, c = 4.5)\nnamed\nnamed[\"a\"]\n\nIn R (and in contrast to other languages like C/C++) vector indices start with 1. Negative indices are also possible, but they have the special purpose to delete one or several elements:\n\nvalues[-3]\n\nIt is also possible to extend a given vector by preceding or appending values with the combine function (c):\n\nc(1, 1, values, 0, 0)\n\nThe length of a vector can be determined with:\n\nlength(values)\n\nand it is also possible to have empty vectors, i.e. vectors that exist, but do not contain any values. Here the keyword NULL means “nothing” in contrast to “0” (zero) that has length 1:\n\nvalues &lt;- NULL\nvalues\nlength(values)\n\nSuch empty vectors are sometimes used as “containers” for appending data step by step:\n\nvalues &lt;- NULL\nvalues\nlength(values)\nvalues &lt;- c(values, 1)\nvalues\nvalues &lt;- c(values, 1.34)\nvalues\n\nIf a data element should be removed completely, this can be done using the remove function:\nrm(values)\nvalues\nError: Object \"values\" not found\nThe complete workspace can be deleted from the menu of R or RStudio (Session – Clear workspace) or from the command line with rm (remove):\n\nrm(list = ls(all = TRUE))\n\nThe R session can be closed by using the menu as usual or by entering:\n\nq()\n\nSometimes and depending of the configuration, R asks whether the “R workspace” should be saved to the disk. This may be useful for continuing work at a later time, but has the risk to clutter the workspace and to get irreproducible results at a later session, so it is recommended to say “No” for now, except if you exactly know why.\nLater we will learn how to save only the data (and commands) that are needed."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#r-as-function-plotter",
    "href": "tutorials/s1-introductory-r-session.html#r-as-function-plotter",
    "title": "An Introductory R Session",
    "section": "3.1 R as function plotter",
    "text": "3.1 R as function plotter\nNow, we will see how to use R as a function plotter by drawing sine or cosine functions within an interval between 0 to 10. First, we create two vectors with x and y. To obtain a smooth curve, it is reasonable to choose a small step size. As a rule of thumb I always recommend to use about 100…400 small steps as a good compromise between smoothness and memory requirements, so let’s set the step size to 0.1:\n\nx &lt;- seq(0, 10, 0.1)\ny &lt;- sin(x)\nplot(x, y)\n\n\n\n\nInstead of plotting points, we can also draw continuous lines. This is indicated by supplying an optional argument type = \"l\".\nNote: the symbol used here for type is the small letter “L” for “line” and not the – in printing very similar – numeral “1” (one)!\nWe see also, that optional arguments like type can be given as “keyword = value” pair. This has the advantage that the order of arguments does not matter, because arguments are referenced by their name:\n\nplot(x, y, type = \"l\")\n\nNow we want to add a cosine function with another color. This can be done with one of the function lines or points, for adding lines or points to an existing figure:\n\ny1 &lt;- cos(x)\nlines(x, y1, col = \"red\")\n\nWith the help of text it is also possible to add arbitrary text, by specifying first the x and y coordinates and then the text:\n\nx1 &lt;- 1:10\ntext(x1, sin(x1), x1, col = \"green\")\n\nMany options exist to modify the behavior of most graphics functions so the following specifies user-defined coordinate limits (xlim, ylim), axis labels and a heading (xlab, ylab, main).\n\nplot(x, y, xlim = c(-10, 10), ylim = c(-2, 2),\n    xlab = \"x-Values\", ylab = \"y-Values\", main = \"Example Graphics\")\n\nCode formatting and line breaks\nThe above example shows a rather long command that may not fit on a single line. In such cases, R displays a + (plus sign) to indicate that a command must be continued, e.g. because a closing parenthesis or a closing quote is still missing. Such a + at the beginning of a line is an automatic “prompt” similar to the ordinary &gt; prompt and must never be typed in manually. If, however, the + continuation prompt occurs by accident, press “ESC” to cancel this mode.\nIn contrast to the long line continuation prompt, it is also possible to write several commands on one line, separated by a semi-colon “;”. This is unseful in some cases, but as a general rule it is much better to use the script editor and then to:\n\nwrite each command to a separate line\navoid long lines with more than about 80 characters\nuse proper indentation, e.g. 2 characters per indentation level\nuse spacing to improve readability of the code, e.g. before and after the assignment operator &lt;-.\n\nFinally, a number symbol (or hash) # means that a complete line or the part of the line that follows # is a comment and should be ignored by R."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#additional-plotting-options",
    "href": "tutorials/s1-introductory-r-session.html#additional-plotting-options",
    "title": "An Introductory R Session",
    "section": "3.2 Additional plotting options",
    "text": "3.2 Additional plotting options\nIn order to explore the wealth of graphical functions, you may now have a more extensive look into the online help, especially regarding ?plot or ?plot.default, and you should experiment a little bit with different plotting parameters, like lty, pch, lwd, type, log etc. R contains uncountable possibilities to get full control over the style and content of your graphics, e.g. with user-specified axes (axis), legends (legend) or user-defined lines and areas (abline, rect, polygon). The general style of figures like (font size, margins, line width) can be influenced with the par function."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#high-level-plotting-functions",
    "href": "tutorials/s1-introductory-r-session.html#high-level-plotting-functions",
    "title": "An Introductory R Session",
    "section": "3.3 High level plotting functions",
    "text": "3.3 High level plotting functions\nIn addition, R and its packages contain numerous “high level”-graphics functions for specific purposes. To demonstrate a few, we first generate a data set with normally distributed random numbers (mean = 0, standard deviation sd = 1), then we plot them and create a histogram. Here, the function par(mfrow = c(2, 2)) divides the plotting area into 2 rows and 2 columns to show 4 separate figures:\n\npar(mfrow = c(2, 2))\nx &lt;- rnorm(100)\nplot(x)\nhist(x)\n\nNow, we add a so-called normal probability plot and a second histogram with relative frequencies together with the bell-shaped density curve of the standard normal distribution. The optional argument probability = TRUE makes sure that the histogram has the same scaling as the density function, so that both can be overlayed:\n\nqqnorm(x)\nqqline(x, col = \"red\")\nhist(x, probability = TRUE)\nxx &lt;- seq(-3, 3, 0.1)\nlines(xx, dnorm(xx, 0, 1), col = \"red\")\n\n\n\n\n\n\n\nHere it may also be a good chance to do a little bit summary statistics like: z.B. mean(x), var(x), sd(x), range(x), summary(x), min(x), max(x), …\nOr we may consider to test if the generated random numbers x are approximately normal distributed using the Shapiro-Wilks-W-Test:\n\nx &lt;- rnorm(100)\nshapiro.test(x)\n\nA p-value bigger than 0.05 tells us that the test has no objections against normal distribution of the data. The concrete results may differ, because x contains random numbers, so it makes sense to repeat this several times. It can be also useful compare these normally distributed random numbers generated with rnorm with uniformly distributed random numbers generated with runif:\n\npar(mfrow=c(2,2))\ny &lt;- runif(100)\nplot(y)\nhist(y)\nqqnorm(y)\nqqline(y, col=\"red\")\nmean(y)\nvar(y)\nmin(y)\nmax(y)\nhist(y, probability=TRUE)\nyy &lt;- seq(min(y), max(y), length = 50)\nlines(yy, dnorm(yy, mean(y), sd(y)), col = \"red\")\nshapiro.test(y)\n\nAt the end, we compare the pattern of both data sets with box-and-whisker plots:\n\npar(mfrow=c(1, 1))\nboxplot(x, y)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercises",
    "href": "tutorials/s1-introductory-r-session.html#exercises",
    "title": "An Introductory R Session",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nRepeat this example with new random numbers and vary sample size (n), mean value (mean) and standard deviation (sd) for random numbers created with rnorm, and use different min and max for runif. Consult the help pages for an explanation of the functions and its arguments, and create boxplots with different data sets."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#numeric-and-character-vectors",
    "href": "tutorials/s1-introductory-r-session.html#numeric-and-character-vectors",
    "title": "An Introductory R Session",
    "section": "4.1 Numeric and character vectors",
    "text": "4.1 Numeric and character vectors\nAll data objects have the two built-in attributes mode (data type) and length (number of data in the object).\nModes can be “numeric” for calculations or “character” for text elements.\n\nx &lt;- c(1, 3, 4, 5)       # numeric\na &lt;- c(\"hello\", \"world\") # character\n\nThe following is also a character variable, because the numbers are given in quotes. It is then not possible to do calculations:\n\nx &lt;- c(\"1\", \"3\", \"4\", \"5\")  # character\nsum(x)\n\nError in sum(x) : invalid 'type' (character) of argument\n\nHere it is necessary to convert the character to numeric first:\n\ny &lt;- as.numeric(x)\nsum(y)\n\n[1] 9.040591"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#factors",
    "href": "tutorials/s1-introductory-r-session.html#factors",
    "title": "An Introductory R Session",
    "section": "4.2 Factors",
    "text": "4.2 Factors\nA special kind of mode is factor. This is, statistically speaking, a nominal variable that appears like characters, e.g. “control”, “treatment A”, “treatment B” …, but its levels are internally encoded as integer.\nHere a typical example with three factor levels. In a first step,let’s create a character variable:\n\ntext &lt;- rep(c(\"control\", \"treatment A\", \"treatment B\"), each=5)\ntext\n\n [1] \"control\"     \"control\"     \"control\"     \"control\"     \"control\"    \n [6] \"treatment A\" \"treatment A\" \"treatment A\" \"treatment A\" \"treatment A\"\n[11] \"treatment B\" \"treatment B\" \"treatment B\" \"treatment B\" \"treatment B\"\n\n\nand then convert it to a factor:\n\nf &lt;- factor(text)\nf\n\n [1] control     control     control     control     control     treatment A\n [7] treatment A treatment A treatment A treatment A treatment B treatment B\n[13] treatment B treatment B treatment B\nLevels: control treatment A treatment B\n\n\nWe se that the character variable is printed with quotes and the factor without quotes, but with an additional information about the Levels. The reason for this is, that the factor is internally encoded as integer values with assigned levels as a translation table:\n\nlevels(f)\n\n[1] \"control\"     \"treatment A\" \"treatment B\"\n\n\nTo show encoding, we can convert the factor into an integer\n\nas.integer(f)\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3\n\n\nThe encoding is done in alphabetical order by default. It can be changed by using an additional levels argument:\n\nf2 &lt;- factor(text, levels=c(\"treatment A\", \"treatment B\", \"control\"))\nf2\n\n [1] control     control     control     control     control     treatment A\n [7] treatment A treatment A treatment A treatment A treatment B treatment B\n[13] treatment B treatment B treatment B\nLevels: treatment A treatment B control\n\nas.numeric(f2)\n\n [1] 3 3 3 3 3 1 1 1 1 1 2 2 2 2 2\n\n\nFactors are useful for statistical analyses like ANOVA and statistical tests, and also as categories for plotting:\n\nx &lt;- c(7.44, 6.45, 6.04, 5.58, 4.5, 8.13, 5.54, 7.34, 8.91, 5.16, 8.7, 7.74, 6.8, 6.49, 6.2)\nplot(x ~ f)\n\n\n\n\nWe see that a boxplot is created and no x-y-plot, because the explanation variable is a factor.\nNumeric variables (especially ordinal) can also be converted to factors. This is useful, if we want to make clear, that numbers are to be treated as name without order.\nHowever, conversion of such factors back into numeric variables types should be done with care, because a character “123” may be encoded with another value (e.g. 1) and not 123, see the following demonstration of a correct and wrong factor conversions:\n\nx &lt;- c(2, 4, 6, 5, 8)\nf &lt;- as.factor(x)\nas.numeric(f)               # wrong !!!\nas.numeric(as.character(f)) # correct\nas.numeric(levels(f))[f]    # even better\n\nSuch a factor coding is not specific to R and appears also in other statistics packages. Then they are sometimes called “dummy variables”."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#matrices-and-arrays",
    "href": "tutorials/s1-introductory-r-session.html#matrices-and-arrays",
    "title": "An Introductory R Session",
    "section": "4.3 Matrices and arrays",
    "text": "4.3 Matrices and arrays\nA matrix is a two-dimensional data structure that can be used for matrix algebra. To create a matrix, we can first create a one-dimensional vector and then reformat it as two-dimensional matrix with nrow rows and ncolcolumns:\n\nx &lt;- 1:20\nx\n\ny &lt;- matrix(x, nrow = 5, ncol = 4)\ny\n\nWe see that the matrix is filled rowwise. We can also convert it back to a vector:\n\nas.vector(y) # flattens the matrix to a vector\n\nAn array extends the matrix concept to more than two dimensions:\n\nx &lt;- array(1:24, dim=c(3, 4, 2))\n\nVectors, matrices and arrays have an important limitation: they can only contain one data type (mode), either numeric or character. So, if a single element is of type character, the whole matrix will be of mode character and appears in quotes:\n\nx &lt;- c(1, 2, 5, 2, \"a\")\nmode(x)\nx"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#lists",
    "href": "tutorials/s1-introductory-r-session.html#lists",
    "title": "An Introductory R Session",
    "section": "4.4 Lists",
    "text": "4.4 Lists\nThe most flexible data type of R is the list. It can contain arbitrary data of different modes. Lists can be nested to form a tree-like structure:\n\nl &lt;- list(x = 1:3, y = c(1,2,3,4), a = \"hello\", L = list(x = 1, y = 2))\n\nLists are extremely powerful and flexible and may be discussed later. The impatient may have a look at the tutorial of w3schools.com."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#data-frames",
    "href": "tutorials/s1-introductory-r-session.html#data-frames",
    "title": "An Introductory R Session",
    "section": "4.5 Data frames",
    "text": "4.5 Data frames\nThe typical data structure for data analysis in R is the so-called data.frame. It can contain both, columns with numeric data and columns of mode character. Some packages use ando extended versions of data frames, a so called tibbles.\nA data frame can be constructed from scratch directly in the R code or read from a file or the internet. As an example, students were asked in different years for their favorite number from one to 9. The results can be put in a data frame like follows:\n\nfavnum &lt;- data.frame(\n  favorite = 1:9,\n  obs2019  = c(1, 1, 6, 2, 2,  5,  8, 6, 3),\n  obs2020  = c(1, 2, 8, 1, 2,  2, 20, 2, 4),\n  obs2021  = c(2, 6, 8, 1, 6,  4, 13, 2, 4),\n  obs2022  = c(2, 3, 7, 8, 2, 10, 12, 6, 1)\n)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#direct-input",
    "href": "tutorials/s1-introductory-r-session.html#direct-input",
    "title": "An Introductory R Session",
    "section": "5.1 Direct input",
    "text": "5.1 Direct input\nWe used this method already when creating vectors with the c (combine)-Function:\n\nx &lt;- c(1, 2, 5, 7, 3, 4, 5, 8)\nx\n\nIn the same way it is possible to create other data types like data frames:\n\ndat &lt;- data.frame(f = c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"),\n                  x = c(1,   4,   3,   3,   5,   7)\n       )\ndat\n\nor matrices:\n\nA &lt;- matrix(c(1:9), nrow=3)\nA\n\nWe see that a matrix is not much different from a vector, formatted into rows and columns."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#read-data-from-a-text-file",
    "href": "tutorials/s1-introductory-r-session.html#read-data-from-a-text-file",
    "title": "An Introductory R Session",
    "section": "5.2 Read data from a text file",
    "text": "5.2 Read data from a text file\nR has very flexible functions to read data from text files. Let’s for example use a table that contains some data from a lake area in north-eastern Germany (Table 1).\nTable 1: Morphometrical and chemical properties of selected lakes (S=Stechlinsee, NN=Nehmitzsee Nord, NS=Nehmitzsee Süd, BL=Breiter Luzin, SL = Schmaler Luzin, DA = Dagowsee, HS = Feldberger Haussee; z=mean depth (m), t=theoretical retention time (a), P=phosphorus concentration (\\(\\mathrm{\\mu g L^{-1}}\\)), N=nitrogen concentration (\\(\\mathrm{mg L{^-1}}\\)), Chl=chlorophyll concentration (\\(\\mathrm{\\mu g L^{-1}}\\)), PP=annual primary production (\\(\\mathrm{g C m^{-2} a^{-1}}\\)), SD = secchi depth (m)). The data are an adapted and simplified “toy version” taken from Casper (1985) and Koschel & Scheffler (1985).\n\n\n\n\n\nLake\nz\nt\nP\nN\nChl\nPP\nSD\n\n\n\n\nS\n23.7\n40\n2.5\n0.20\n0.7\n95\n8.4\n\n\nNN\n5.9\n10\n2.0\n0.20\n1.1\n140\n7.4\n\n\nNS\n7.1\n10\n2.5\n0.10\n0.9\n145\n6.5\n\n\nBL\n25.2\n17\n50.0\n0.10\n6.1\n210\n3.8\n\n\nSL\n7.8\n2\n30.0\n0.10\n4.7\n200\n3.7\n\n\nDA\n5.0\n4\n100.0\n0.50\n14.9\n250\n1.9\n\n\nHS\n6.3\n4\n1150.0\n0.75\n17.5\n420\n1.6\n\n\n\n\n\nThe data can be downloaded from https://github.com/tpetzoldt/datasets/tree/main/data\n\n5.2.1 Set working directory\nR needs to know where to find the data on your computer. One way is to provide the full path to the data set, e.g. if it is c:/users/&lt;username&gt;/documents, then\n\nlakes &lt;- read.csv(\"c:/users/julia/documents/lakes.csv\")\n\nThis can be cumbersome and error-prone, so the preferred method is to set the working directory of R to the data directory. This can be done in RStudio like follows:\n\nLocate the folder with the data in the “Files” pane\nSelect “More”\nSelect “Set As Working Directory”\n\n Figure2: Setting the working directory in RStudio\nAfter this, data can be retrieved directly from the working directory :\n\nlakes &lt;- read.csv(\"lakes.csv\", header=TRUE)\n\nWe may also consider to create a sub-folder data of the working direktory and put the data in. Then we could use for example:\n\nlakes &lt;- read.csv(\"../data/lakes.csv\", header=TRUE)\n\nNote also that we use always the ordinary slash “/” and not the backslash “\\”, even on Windows.\nIn some countries that have the comma and not the dot as a decimal separator and then for example a semicolon as column separator, additional arguments dec = \",\", sep=\";\" may be required. The details are found on the read.table help page."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#read-data-from-the-internet",
    "href": "tutorials/s1-introductory-r-session.html#read-data-from-the-internet",
    "title": "An Introductory R Session",
    "section": "5.3 Read data from the internet",
    "text": "5.3 Read data from the internet\nIf the data are available on an internet server, it can be read directly from there:\n\nlakes &lt;- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/lakes.csv\")\n\nNow, as the data are saved in the data frame lakes it is possible to access them as usual:\n\nlakes\nsummary(lakes)\nboxplot(lakes[-1])\n\nHere summary shows a quick overview and boxplot creates a boxplot for all columns except the first, that contains no numbers.\nNow, we are ready to inspect the content of this new variable lakes. If we use RStudio a View can be invoked by clicking to lakes in the environment window."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#import-dataset-in-rstudio",
    "href": "tutorials/s1-introductory-r-session.html#import-dataset-in-rstudio",
    "title": "An Introductory R Session",
    "section": "5.4 “Import Dataset” in RStudio",
    "text": "5.4 “Import Dataset” in RStudio\nRStudio contains a handy feature that makes importing of data more convenient. Essentially, this “Import Dataset” wizard helps us to construct the correct read.table, read.csv or read_delim function interactively. It is possible to try different options until a satisfying result is obtained. Current versions of RStudio contain several different ways to import data. Here we demonstrate the “Import Dataset From Text (readr)” assistant:\n\nFrom the menu select: File – Import DataSet – From CSV.\nSelect the requested file and select suitable options like the name of the variable the data are to be assigned to, the delimiter character (comma or Tab) and whether the first row of the file contains variable names.\n\n\nImport Dataset From Text (readr) assistant of RStudio.\nHint: In the exmple above, the name of the data frame is identical to the file name, i.e. “lakes”. If we want to name it differently (e.g.: dat), we must not forget to change this setting.\nNote also that the Code Preview contains the commands that the wizard created. If we copy these commands to the script pane, you can re-read the data several times without going back to the menu system:\n\nlibrary(readr)\nlakes &lt;- read_csv(\"D:/DATA/lakes.csv\")\nView(lakes)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#display-the-content-of-the-data-frame",
    "href": "tutorials/s1-introductory-r-session.html#display-the-content-of-the-data-frame",
    "title": "An Introductory R Session",
    "section": "6.1 Display the content of the data frame",
    "text": "6.1 Display the content of the data frame\nThe easiest way is to just enter the name of the data frame to the R console, e.g.:\n\nlakes\n\nor to click to the name of the data frame in the “Environment” explorer of RStudio. This executes then View(lakes), so that the data are shown.\nFor large tables it is often not very useful to display the full content with View, so it may be better to use the function str (structure) that gives a compact overview over type, size and content of a variable:\n\nstr(mydata)\n\nThe str function is universal and also suitable for complicated object types like lists. Of course, there are many more possibilities for inspecting the content of a variable:\n\nnames(lakes)\nmode(lakes)\nlength(lakes)\n\nand sometimes even:\n\nplot(lakes)"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#access-single-columns-with",
    "href": "tutorials/s1-introductory-r-session.html#access-single-columns-with",
    "title": "An Introductory R Session",
    "section": "6.2 Access single columns with $",
    "text": "6.2 Access single columns with $\nSingle columns of a data frame can be accessed by using indices (with []) similar to a vector or a matrix or by using the column name and the $) operator:\n\nmean(lakes[,2])\nmean(lakes$z)\nmean(lakes[,\"z\"])\nmean(lakes[[\"z\"]])\nplot(lakes$z, lakes$t)\n\nwhere z is the mean depth of the lakes and t the so-called mean residence time.\nWe should also nitice the subtle difference of the output of the [] and the [[]]- version. The difference is as follows: single brackets return a data frame with one column, but double square brackets return the content of the column as a vector without the caption.\nWarning: In some older books, the $-style is sometimes abbreviated using the attach and detach-functions. This “prehistoric relict” is strongly discouraged, as it can lead to data inconsistency and strange errors. If you find it somewhere where it is still used, then it is a good idea to use detach repeatedly until an error message confirms us that there is nothing else that can be detached. Finally: never use attach/detach in a package.\nInstead, it is much better to use another function width, that opens the data frame only temporarily:\n\nwith(lakes, plot(z, t))"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#subsets-and-logical-indices",
    "href": "tutorials/s1-introductory-r-session.html#subsets-and-logical-indices",
    "title": "An Introductory R Session",
    "section": "6.3 Subsets and logical indices",
    "text": "6.3 Subsets and logical indices\nIn the following, we use another data set that we read directly from the internet:\n\nfruits &lt;- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/clementines.csv\")\n\nIt contains weight, width and height measurements of two brands of Clementine fruits. After loading the data, we look at it with View(fruits) or the Environment explorer of Rstudio\nA very powerful feature of R is the use of logical vectors as “indices”, with similar results like data base queries. As an example, we can show the weight of all fruits of brand “A” with\n\nfruits[fruits$brand == \"A\", c(\"brand\", \"weight\")]\n\n   brand weight\n6      A     81\n7      A    113\n8      A     94\n9      A     86\n10     A    108\n11     A     91\n12     A     94\n13     A     86\n14     A     91\n15     A     88\n\n\nHere the first indext in the square brackets indicates the subset of rows that we want and the second argument the columns. If we want to see all columns, we leave the argument after the comma empty:\n\nfruits[fruits$brand == \"A\", ]\n\n   id no brand weight width height\n6   6  7     A     81    53     54\n7   7  8     A    113    61     60\n8   8  9     A     94    62     49\n9   9  3     A     86    58     53\n10 10  6     A    108    64     50\n11 11  1     A     91    59     51\n12 12 10     A     94    59     51\n13 13  4     A     86    55     55\n14 14  2     A     91    61     51\n15 15  5     A     88    54     55\n\n\nA logical comparison requires always a double “==”. Logical operations like & (and) and | (or) are also possible. Note that “and” has always precedence before “or”, except this is changed with parenthesis.\nA subset of a data frame can also be extracted with the subset function:\n\nbrand_B &lt;- subset(fruits, brand == \"B\")\nbrand_B\n\nLike in the example before, the condition argument allows also logical expressions with & (and) and | (or).\nWe can also access single elements in matrix-like manner.\nThe element from the 2nd row and the 4th column can be selected with:\n\nfruits[2, 4]\n\nthe complete 5th row with:\n\nfruits[5, ]\n\nand rows 5:10 of the 4th column (weight) with:\n\nfruits[5:10, 4]\n\nAdditional methods for working with matrices, data frames and lists can be found in R textbooks or in the official R documentation."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#pipelines-and-summaries",
    "href": "tutorials/s1-introductory-r-session.html#pipelines-and-summaries",
    "title": "An Introductory R Session",
    "section": "7.1 Pipelines and summaries",
    "text": "7.1 Pipelines and summaries\nThe last examples are intended to demonstrate how powerful a single line can be in R. How to analyse data sets evolved over the history of R, so there is for example a function aggregate to compute statistics (e.g. mean values) depending on given criteria.\nThis works well and is still used, but the modern methods are more compact and easier to understand. Here let’s introduce a modern concept first, that is called “pipelining”. The idea is, that the result of a function is directly pipelined to another function, so instead of writing:\n\nbrand_B &lt;- subset(fruits, brand == \"B\")\ncolumns_B &lt;- brand_B[c(\"weight\", \"width\", \"height\")]\nmeans_B &lt;- colMeans(columns_B)\nmeans_B\n\nwe can directly write:\n\nlibrary(\"dplyr\")\nfruits |&gt; filter(brand == \"B\") |&gt; select(weight, width, height) |&gt; colMeans()\n\nHere we load an add-on package dplyr first, that contains a lot of helpful functions for data management, for example filter that selects rows and select that selects columns. The pipeline operator |&gt; pipes then the data set fruitsto the filter- function and subsequently to the next. The “native pipeline operator” |&gt; was introduced with R 4.1. As an alternative we can also use the %&gt;% pipeline operator, that is loaded by the dplyr package. Its function would be identical in this case.\nTwo other extremely useful dplyr functions are group_by and summary, that allow to calculate arbitrary summary statistics in dependence of grouping variables. If we use the brand for grouping, we can summarize all groups simultaneously:\n\nfruits |&gt; \n  group_by(brand) |&gt;\n  summarise(mean(weight), mean(width), mean(height))\n\nThe summarize line above can still be made better, and there plenty of other stunning possibilities, so we will come back to this later."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#output-of-results",
    "href": "tutorials/s1-introductory-r-session.html#output-of-results",
    "title": "An Introductory R Session",
    "section": "7.2 Output of Results",
    "text": "7.2 Output of Results\nThe most simple method to save outputs from R is to copy it directly from the R console to any other program (e.g. LibreOffice, Microsoft Word or Powerpoint) via the Clipboard. This is convenient, but cannot be automated. Therefore, it is better to use a programmatic approach.\nLet’s use the example before and store the results in a new data frame results:\n\nresults &lt;-\n  fruits |&gt; \n  group_by(brand) |&gt;\n  summarise(mean(weight), mean(width), mean(height))\n\nData frames can be saved as text files with write.table, write.csv or write_csv. Here we use write_csv (with underscore, not dot) from package readr:\n\nlibrary(\"readr\")\nwrite_csv(results, file=\"output-data.csv\")\n\nIn addition to these basic functions R has a wealth of possibilities to save output and data for later use in reports and presentations. All of them are of course documented in the online help, e.g. print, print.table, cat for text files, and pdf, png for figures. The add-on packages xtable contains functions for creating LaTeX or HTML-tables while full HTML output is supported by the R2HTML or knitr packages."
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#plots-with-ggplot2",
    "href": "tutorials/s1-introductory-r-session.html#plots-with-ggplot2",
    "title": "An Introductory R Session",
    "section": "7.3 Plots with ggplot2",
    "text": "7.3 Plots with ggplot2\nIn addition to the plot functions we used so far, other plot packages exist, for example lattice or ggplot2. Here a few small examples with the very popular **ggplot2* package:\n\nlibrary(\"ggplot2\")\nfruits |&gt;\n  ggplot(aes(brand, weight)) + geom_boxplot()\n\n\n\n\nOr a scatterplot to compare weight and width of the fruits\n\nfruits |&gt; ggplot(aes(weight, width)) + geom_point(aes(color=brand))\n\n\n\n\nwhere the brand is indicated as color. Another option could be:\n\nfruits |&gt; ggplot(aes(weight, width)) + \n  geom_point() + \n  geom_smooth(method=\"lm\") + \n  facet_grid(~brand)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercises-1",
    "href": "tutorials/s1-introductory-r-session.html#exercises-1",
    "title": "An Introductory R Session",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\nR contains lots of data sets for exploring its graphical and statistical functions and that can be activated by using the data function, e.g. data(iris) or data(cars). A description of the data set can be found as usual in the help files, e.g. ?iris, ?cars.\nUse one of these data sets and try\n\nways to access columns, to select rows and to create subsets\nways for summary statistics and visualization with R’s base plot functions and optionally with ggplot."
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html",
    "href": "tutorials/s2-multivar-3d.html",
    "title": "Ordination and Clustering",
    "section": "",
    "text": "The following demo demonstrates the reduction of a three dimensional data set into two dimensions by principal component analysis (PCA) and nonmetric multidimensional scaling.\nThe demo uses artificial data and a 3D plotting package, so the code may look somewhat technical. But don’t worry. This is for demonstration of the main principles. Practical demonstrations with real data will follow."
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#packages-and-data-set",
    "href": "tutorials/s2-multivar-3d.html#packages-and-data-set",
    "title": "Ordination and Clustering",
    "section": "Packages and data set",
    "text": "Packages and data set\nFirst we load the required packages and a test data set multivar.csv. It consists of 3 clusters of correlated multivariate normally distributed data points that were generated with the rmvnorm function. Each data subset has a separate mean value and a common variance-covariance matrix \\(\\sigma\\) that is created at random. A separate script is used to generate the data.\n\nlibrary(\"rgl\")\nlibrary(\"vegan\")\nlibrary(\"vegan3d\")\n\nA <- read.csv(\"../data/multivar.csv\")"
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#plotting",
    "href": "tutorials/s2-multivar-3d.html#plotting",
    "title": "Ordination and Clustering",
    "section": "Plotting",
    "text": "Plotting\nThe data set has three columns, so we can, in principle, plot all column variables versus each other, or use a 3D plot.\nNote: The figure below can be rotated and zoomed with the mouse. Here we employ R’s 3D graphics interface package rgl and the multivariate 3D visualisation package vegan3d.\n\nnsamp    <- nrow(A) / 3  # number of points in each of the 3 samples\nmycolors <- rep(c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\"), each = nsamp)\n\npar(mfrow=c(1,3))\nplot(y ~ x, data=A, col=mycolors)\nplot(y ~ z, data=A, col=mycolors)\nplot(z ~ x, data=A, col=mycolors)\n\n\n\n\nWe see that the colors overlap more or less in the 3 figures, so let’s try to improve this in the 3D view. The goal is to rotate the axes such, so that the colored dots form well separated clusters.\n\nordirgl(A, type=\"p\", ax.col = \"black\", col=mycolors, box=FALSE)\nview3d(theta = 5, phi = 15, fov=30, zoom=0.7)\naxes3d(labels=FALSE)\n\n\n\n\n\nWe see that the coordinate axes can be rotated in the direction of maximum variance of the data, so that overlap between data points is minimized. We can try to do this by hand, or let the computer do this. This is then calles a “principal components analysis” (PCA)."
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#principal-components",
    "href": "tutorials/s2-multivar-3d.html#principal-components",
    "title": "Ordination and Clustering",
    "section": "Principal components",
    "text": "Principal components\nR contains several functions for principal components analysis, for example princomp and prcomp in the stats package or function rda in the vegan package. The result is, that the coordinate system is rotated in the direction om maximum variance, using Eigen value calculations. The resulting “synthetic” new dimensions are then the principal components.\nEach principal component represents then a specific fraction of the total variance of the data in ascending order. This can be plotted as bar chart or printed with summary.\n\npc <- prcomp(A)\nplot(pc)\nbox()\n\n\n\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2    PC3\nStandard deviation     3.2753 1.6495 1.1778\nProportion of Variance 0.7231 0.1834 0.0935\nCumulative Proportion  0.7231 0.9065 1.0000\n\n\nThe objects in the rotated coordinate axes can then be visualized with function biplot in 2 dimensions (shown later with real data) or with ordirgl in 3D. Function view3d rotates the plot in the direction of PC1 and PC2, but you can use the mouse to see that there is still a 3rd dimension.\n\npc <- prcomp(A)\nordirgl(pc, type=\"p\", display=\"sites\", \n        ax.col = \"black\", col=mycolors)\nview3d(theta = 0, phi = 0, fov=0, zoom=0.7)\naxes3d()"
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#nonmetric-multi-dimensional-scaling-nmds",
    "href": "tutorials/s2-multivar-3d.html#nonmetric-multi-dimensional-scaling-nmds",
    "title": "Ordination and Clustering",
    "section": "Nonmetric multi-dimensional scaling (NMDS)",
    "text": "Nonmetric multi-dimensional scaling (NMDS)\nThe PCA is a very useful technique as it reduces dimensions without bias, but as we have seen, large proportions of information may still be found at higher dimensions.\nThis is, where the nonmetric dimensional scaling comes into play. Here, we can request a number of dimensions \\(k\\), typically \\(k=2\\) or \\(k=3\\) and then the computer tries hard to squeeze the information as much as possible into these dimensions. The aim is, that the distances between in all dimensions are represented in two or three dimensions only. Though it is done in an iterative way, it is clear, that this is not perfectly possible and will introduce considerable distortion, called stress.\n\nmds <- metaMDS(A, distance=\"euclid\", scale=TRUE, autotransform = FALSE, k=2)\n\n'comm' has negative data: 'autotransform', 'noshare' and 'wascores' set to FALSE\n\n\nRun 0 stress 0.07782409 \nRun 1 stress 0.0778241 \n... Procrustes: rmse 9.84156e-06  max resid 8.281049e-05 \n... Similar to previous best\nRun 2 stress 0.0778241 \n... Procrustes: rmse 4.936877e-06  max resid 3.551071e-05 \n... Similar to previous best\nRun 3 stress 0.0778241 \n... Procrustes: rmse 4.525509e-06  max resid 4.367485e-05 \n... Similar to previous best\nRun 4 stress 0.0972286 \nRun 5 stress 0.07782409 \n... New best solution\n... Procrustes: rmse 9.187398e-06  max resid 0.0001044343 \n... Similar to previous best\nRun 6 stress 0.07783594 \n... Procrustes: rmse 0.0003803165  max resid 0.003854196 \n... Similar to previous best\nRun 7 stress 0.07782411 \n... Procrustes: rmse 2.020656e-05  max resid 0.0002300249 \n... Similar to previous best\nRun 8 stress 0.0778241 \n... Procrustes: rmse 1.582622e-05  max resid 0.0001673336 \n... Similar to previous best\nRun 9 stress 0.0778241 \n... Procrustes: rmse 1.337722e-05  max resid 0.0001057814 \n... Similar to previous best\nRun 10 stress 0.0778241 \n... Procrustes: rmse 1.685195e-05  max resid 0.0001768136 \n... Similar to previous best\nRun 11 stress 0.09722252 \nRun 12 stress 0.07783593 \n... Procrustes: rmse 0.0003791719  max resid 0.003846868 \n... Similar to previous best\nRun 13 stress 0.07783594 \n... Procrustes: rmse 0.000380595  max resid 0.003862313 \n... Similar to previous best\nRun 14 stress 0.07782409 \n... New best solution\n... Procrustes: rmse 2.086695e-06  max resid 9.961298e-06 \n... Similar to previous best\nRun 15 stress 0.0778241 \n... Procrustes: rmse 1.245177e-05  max resid 0.0001201323 \n... Similar to previous best\nRun 16 stress 0.07782411 \n... Procrustes: rmse 1.931238e-05  max resid 0.0001827211 \n... Similar to previous best\nRun 17 stress 0.07783594 \n... Procrustes: rmse 0.0003818455  max resid 0.003870978 \n... Similar to previous best\nRun 18 stress 0.0972286 \nRun 19 stress 0.07782409 \n... Procrustes: rmse 8.625595e-06  max resid 5.923825e-05 \n... Similar to previous best\nRun 20 stress 0.07782411 \n... Procrustes: rmse 1.884456e-05  max resid 0.0001995062 \n... Similar to previous best\n*** Best solution repeated 6 times\n\n\nThe resulting stress and the representation of distances in two (or three) dimensions can then be shown in a so-called stressplot. Here, one should not overestimate the hich \\(r^2\\) values, that are always big, even in bad cases. The shape of the step-line does not matter much either. However, the value of stress is most important and the data points should be close to the red line.\n\nstressplot(mds)\n\n\n\n\nThe stress itself shuld be small. As a rule of thumb, a stress value of \\(>0.2\\) means that the NMDS was not successful, stress \\(< 0.1\\) is considered as sufficient, \\(<0.05\\) as good, \\(<0.025\\) as very good and \\(\\approx 0\\) as perfect.\n\nmds\n\n\nCall:\nmetaMDS(comm = A, distance = \"euclid\", k = 2, autotransform = FALSE,      scale = TRUE) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     A \nDistance: euclidean \n\nDimensions: 2 \nStress:     0.07782409 \nStress type 1, weak ties\nBest solution was repeated 6 times in 20 tries\nThe best solution was from try 14 (random start)\nScaling: centring, PC rotation \nSpecies: scores missing\n\n\n\nxyz <- as.data.frame(scores(mds, display=\"sites\"))\nxyz$. <- 0\nordirgl(xyz, type=\"p\", ax.col = \"black\", col=mycolors)\nview3d(theta = 0, phi = 0, fov=0, zoom=0.7)\naxes3d(labels=FALSE, expand=1.5)\n\n\n\n\n\nHere, stress is \\(<0.1\\). This is o.k., so that we now can have a look at the 2D representation. The figure is again technically 3D, so use the mouse to see that the data are now at a plane."
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#cluster-analysis",
    "href": "tutorials/s2-multivar-3d.html#cluster-analysis",
    "title": "Ordination and Clustering",
    "section": "Cluster analysis",
    "text": "Cluster analysis\nThe methods discussd so far try to map high dimensional structures to lower dimensions as good as possible, but there is still variation left that is not shown, either because it is in a higher dimension as in PCA or because the mapping is “under stress” (NMDS). This means that points shown closely together may not as similar as they appear in the projection plane.\nCluster analysis takes another route. It shows the distance, not the location. Many different cluster analysis methods exist, here we show just an example of hierarchical clustering with “ward.D2” as agglomeration scheme. More about this will be discussed later.\n\nhc <- hclust(dist(A), method=\"ward.D2\")\nplot(hc)\n\n\n\n\nThe result can also be combined with PCA or NMDS, here again a 3D visualization.\n\nhc <- hclust(dist(A), method=\"ward.D2\")\nordirgltree(ord=mds, cluster=hc, col=mycolors)\naxes3d(expand=1.5)\n\n\n\n\n\nThe plot can again be rotated and zoomed in. The x- and y axes show the NMDS coordinates and the y axis the euclidean distance. Two-dimensional plots are of course also possible, practical examples will be given later."
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#distance-and-dissimilarity-measures",
    "href": "tutorials/s2-multivar-3d.html#distance-and-dissimilarity-measures",
    "title": "Ordination and Clustering",
    "section": "Distance and dissimilarity measures",
    "text": "Distance and dissimilarity measures\n\nQuantitative form\nwith \\(x_{ij}, x_{ik}\\) abundance of species \\(i\\) at sites (\\(j, k\\)).\nEuclidean distance:\n[ d_{jk} = ]\nManhattan distance: [ d_{jk} = |x_{ij}-x_{ik}| ]\nGower distance: [ d_{jk} = ]\nBray-Curtis dissimilarity: [ d_{jk} = ]\n\n\nBinary form\nThe binary form is applicable to binary and factor variables, where:\n\n\\(A, B\\) = numbers of species on compared sites\n\\(J\\) = (joint) is the number of species that occur on both compared sites\n\\(M\\) = number of columns (excluding missing values)\n\nEuclidean: \\(\\sqrt{A+B-2J}\\)\nManhattan: \\(A+B-2J\\)\nGower: \\(\\frac{A+B-2J}{M}\\)\nBray-Curtis: \\(\\frac{A+B-2J}{A+B}\\)\nJaccard: \\(\\frac{2b}{1+b}\\) with \\(b\\) = Bray-Curtis dissimilarity\n\n\nApplications\nAdditional distance measures and application suggestions are found in the vegdist help page."
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html",
    "href": "tutorials/s3-multivar-lakes.html",
    "title": "Multivariate Lake Data Example",
    "section": "",
    "text": "The following example demonstrates basic multivariate principles by means of a teaching example. A detailed description of theory and applications is found in excellent books of Legendre & Legendre (1998) and Borcard et al. (2018). Practical help is found in the tutorials of the vegan package (Oksanen et al., 2020)."
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#data-set-and-terms-of-use",
    "href": "tutorials/s3-multivar-lakes.html#data-set-and-terms-of-use",
    "title": "Multivariate Lake Data Example",
    "section": "Data set and terms of use",
    "text": "Data set and terms of use\nThe lake data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe document and codes provided here can be shared according to CC BY 4.0."
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#load-the-data",
    "href": "tutorials/s3-multivar-lakes.html#load-the-data",
    "title": "Multivariate Lake Data Example",
    "section": "Load the data",
    "text": "Load the data\nHere we load the data set and add English column names and abbreviated lake identifiers as row names to the table, that are useful for the multivariate plotting functions.\n\nlibrary(\"readxl\") # read Excel files directly\nlibrary(\"vegan\")  # multivariate statistics in ecology\nlakes <- as.data.frame(\n  read_excel(\"../data/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) <- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nrownames(lakes) <- paste0(1:nrow(lakes), substr(lakes$name, 1, 4))\n\nText columns, e.g Federal State names and lake type are removed and rows with missing data excluded. If population is not used, the analysis can be repeated with more lakes.\n\nvalid_columns <- c(\"drainage\", \"population\", \"altitude\", \"z_mean\",\n                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                   \"shore_devel\", \"drain_ratio\")\n\n#valid_columns <- c(\"drainage\", \"altitude\", \"z_mean\",\n#                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n#                   \"shore_devel\",\"drain_ratio\")\ndat <- lakes[valid_columns]\ndat <- na.omit(dat)"
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#data-inspection",
    "href": "tutorials/s3-multivar-lakes.html#data-inspection",
    "title": "Multivariate Lake Data Example",
    "section": "Data inspection",
    "text": "Data inspection\nIt is alwas a good idea to plot the data first, as time series or boxplots for example, dependingon the type of data. Here we use boxplots, that we scale (z-transform) to a mean zero and standard deviation one to have comparable values.\nAs we can see a number of high extreme values, we apply also a square root transformation, that is less extreme than log transform and not sensitive against zero values, but because altitude contains a negative value (below sea level) we replace this with zero. As it is a small value, it does not influence our analysis, but we should always be very careful to document such workarounds.\n\npar(mfrow = c(1, 1))\npar(mar = c(7, 4, 2, 1) + .1)\nboxplot(scale(dat), las = 2)\n\n\n\ndat$altitude <- ifelse(dat$altitude < 0, 0, dat$altitude)\nboxplot(scale(sqrt(dat)), las=2)"
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#multivariate-analysis",
    "href": "tutorials/s3-multivar-lakes.html#multivariate-analysis",
    "title": "Multivariate Lake Data Example",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\n\nPrincipal Components: PCA\n\npc <- prcomp(scale(dat))\nsummary(pc)\n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.305 1.4737 1.1459 1.0686 0.84953 0.50024 0.24164\nProportion of Variance 0.483 0.1974 0.1194 0.1038 0.06561 0.02275 0.00531\nCumulative Proportion  0.483 0.6805 0.7998 0.9036 0.96925 0.99200 0.99731\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.12590 0.08400 0.07563 0.03077\nProportion of Variance 0.00144 0.00064 0.00052 0.00009\nCumulative Proportion  0.99875 0.99939 0.99991 1.00000\n\nplot(pc)\n\n\n\nbiplot(pc)\n\n\n\n\nAs the PCA with the untransformed data looks somewhat asymmetric, we repeat it with square transformed data. In addition, also the 3rd PC is plotted.\n\ndat2 <- sqrt(dat)\npc2 <- prcomp(scale(dat2))\nsummary(pc2)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.1886 1.5906 1.2499 1.0634 0.79782 0.44854 0.28572\nProportion of Variance 0.4354 0.2300 0.1420 0.1028 0.05786 0.01829 0.00742\nCumulative Proportion  0.4354 0.6654 0.8075 0.9103 0.96812 0.98641 0.99383\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.17665 0.13833 0.12041 0.05528\nProportion of Variance 0.00284 0.00174 0.00132 0.00028\nCumulative Proportion  0.99666 0.99840 0.99972 1.00000\n\npar(mfrow=c(1,2))\npar(mar=c(5, 4, 4, 2) + 0.1)\nbiplot(pc2, cex=0.6)\nbiplot(pc2, cex=0.6, choices=c(3, 2))\n\n\n\n\nA PCA is also possible with the rda function of the vegan package. The syntax of the plot functions is somewhat different. Instead of biplot as above, we can directly use plot. Details are found in the vegan documentation.\n\npar(mfrow=c(1,1))\npc3 <- rda(dat2, scale = TRUE)\npc3\n\nCall: rda(X = dat2, scale = TRUE)\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\nInertia is correlations \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n4.790 2.530 1.562 1.131 0.637 0.201 0.082 0.031 0.019 0.014 0.003 \n\n#summary(pc3)\nplot(pc3)\n\n\n\n\n\n\nNonmetric Multidimensional Scaling: NMDS\nLt’s now perform an NMDS for the data set. Function metaMDS runs a series of NMDS fits with different start values to avoid local minima. It has also some automatic transformations built in and works usually with the Bray-Curtis dissimilarity, that is used for plants and animal species abundance data. As we work with physical data here, we set the distance measure to “euclidean”.\n\nmd <- metaMDS(dat2, scale = TRUE, distance = \"euclid\")\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1181117 \nRun 1 stress 0.1181117 \n... Procrustes: rmse 0.0002506129  max resid 0.0006992928 \n... Similar to previous best\nRun 2 stress 0.1207018 \nRun 3 stress 0.1207018 \nRun 4 stress 0.1230331 \nRun 5 stress 0.1230331 \nRun 6 stress 0.1230331 \nRun 7 stress 0.1207023 \nRun 8 stress 0.1188533 \nRun 9 stress 0.1207019 \nRun 10 stress 0.1181116 \n... New best solution\n... Procrustes: rmse 0.0001348121  max resid 0.000382903 \n... Similar to previous best\nRun 11 stress 0.1181117 \n... Procrustes: rmse 0.0001308674  max resid 0.000376738 \n... Similar to previous best\nRun 12 stress 0.1768603 \nRun 13 stress 0.1230331 \nRun 14 stress 0.1230331 \nRun 15 stress 0.1207021 \nRun 16 stress 0.1913607 \nRun 17 stress 0.1230331 \nRun 18 stress 0.2155508 \nRun 19 stress 0.1207018 \nRun 20 stress 0.1188533 \n*** Best solution repeated 2 times\n\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\n\n\n\n\nCluster analysis\nHere we apply a hierarchical cluster analysis with square root transformed data and two different agglomeration schemes, “complete linkage” and “Ward’s method”.\n\npar(mfrow=c(2,1))\nhc <- hclust(dist(scale(dat2)), method=\"complete\") # the default\nplot(hc)\n\nhc2 <- hclust(dist(scale(dat2)), method=\"ward.D2\")\nplot(hc2)\n\n\n\n\nWe can also use the clusters to indicate groups in the NMDS plot. Function rect.hclust indicates a given number of clusters in the dendrogram, then we cut the tree with cutree and use the groups grp as color codes. R has 8 standard colors. If we need more, we can define an own palette.\n\nplot(hc, hang = -1)\nrect.hclust(hc, 5)\n\n\n\ngrp <- cutree(hc, 5)\n# grp                  # can be used to show the groups\nplot(md, type = \"n\")\ntext(md$points, row.names(dat2), col = grp)\n\n\n\n\nInstead of hierarchical clustering, we can also use a non-hierarchical method, e.g. k-means clustering. This is an iterative method, and avoids the problem that cluster assignment depends on the order of clustering and the agglomeration method.\nDepending on the question, it may be a disadvantage, that the number of clusters needs to be specified beforehand (e.g. from hierarchical clustering) and that we do not get a tree diagramm."
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#task",
    "href": "tutorials/s3-multivar-lakes.html#task",
    "title": "Multivariate Lake Data Example",
    "section": "Task",
    "text": "Task\n\nTry to understand the analysis,\ndiscuss the results,\nask questions.\nThe idea is to work on this report together and to make it more complete."
  },
  {
    "objectID": "slides/10-multivariate.html#data-set-and-terms-of-use",
    "href": "slides/10-multivariate.html#data-set-and-terms-of-use",
    "title": "Multivariate methods",
    "section": "Data set and terms of use",
    "text": "Data set and terms of use\n\n\nThe “uba-lakes” data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe “bm-lakes” data set is a teaching data set, derived from historical measurements of lakes in Brandenburg and Mecklenburg. The data werde taken from the literature (koschel_primary_1985?) and adapted to teaching purposes.\nThe document itself, the codes and the ebedded images are own work and can be shared according to CC BY 4.0."
  },
  {
    "objectID": "slides/10-multivariate.html#an-introductory-example",
    "href": "slides/10-multivariate.html#an-introductory-example",
    "title": "Multivariate methods",
    "section": "An introductory example",
    "text": "An introductory example"
  },
  {
    "objectID": "slides/10-multivariate.html#references",
    "href": "slides/10-multivariate.html#references",
    "title": "Multivariate methods",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nUmweltbundesamt. (2021). Kenndaten ausgewählter seen deutschlands. https://www.umweltbundesamt.de/daten/wasser/zustand-der-seen#okologischer-zustand-der-seen"
  },
  {
    "objectID": "slides/10-multivariate.html#correlation-between-all-variables",
    "href": "slides/10-multivariate.html#correlation-between-all-variables",
    "title": "Multivariate methods",
    "section": "Correlation between all variables?",
    "text": "Correlation between all variables?\n\nlibrary(\"readxl\") # read Excel files directly\nlakes <- as.data.frame(\n  read_excel(\"../data/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) <- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nstr(lakes)\n\n'data.frame':   56 obs. of  14 variables:\n $ name        : chr  \"Ammersee\" \"Arendsee\" \"Bleilochtalsperre\" \"Bodensee\" ...\n $ state       : chr  \"BY\" \"ST\" \"TH\" \"BW\" ...\n $ drainage    : num  993 29.8 1239.9 11477 28.2 ...\n $ population  : num  114900 3200 180000 1400000 28 ...\n $ altitude    : num  532.9 22.8 404 395.4 13.1 ...\n $ z_mean      : num  37.6 28.6 23.3 85 2.4 ...\n $ z_max       : num  81.1 48.7 55 254 4.8 58.3 32.5 73.4 1.7 18.8 ...\n $ t_ret       : num  2.7 50 0.526 4.2 1.9 16.3 NA 1.26 0.1 2.3 ...\n $ volume      : num  1.75 0.147 0.215 48.522 0.009 ...\n $ area        : num  46.6 5.14 9.2 571.5 3.9 ...\n $ shore_length: num  43 9 NA 273 10.4 13.7 17.5 64 7.5 10.1 ...\n $ shore_devel : num  1.78 1.12 NA 2.26 1.48 2.09 1.64 2.02 2.22 1.6 ...\n $ drain_ratio : num  20.3 5.8 NA 21.9 7.2 ...\n $ wfd_type    : chr  \"Typ 4\" \"Typ 13\" \"Typ 5\" \"Typ 4\" ...\n\n\n\nnames(lakes) replaces the original German column names by abbreviated English abbreviations"
  },
  {
    "objectID": "slides/10-multivariate.html#plot-everything-together",
    "href": "slides/10-multivariate.html#plot-everything-together",
    "title": "Multivariate methods",
    "section": "PLot everything together",
    "text": "PLot everything together\n\nplot(lakes)"
  },
  {
    "objectID": "slides/10-multivariate.html#basic-concepts",
    "href": "slides/10-multivariate.html#basic-concepts",
    "title": "Multivariate methods",
    "section": "Basic concepts",
    "text": "Basic concepts\n\nMultivariate statistics\n\nunivariate: one variable\nbivariate: one dependent, one independent variable\nmultiple: one dependent, several independent variables\n\nSimilarity and correlation\n\ndistance and similarity: how different or similar are the observations?\ncorrelation and covariance: are variables interdependent?\ndimension reduction: try to show essential parts of information on a lower number of dimensions\ncluster analysis: show which observations are closely together\nordination: plot data at lower dimentsions, so that similar observations are closely together"
  },
  {
    "objectID": "slides/10-multivariate.html#distance-and-dissimilarity",
    "href": "slides/10-multivariate.html#distance-and-dissimilarity",
    "title": "Multivariate methods",
    "section": "Distance and dissimilarity",
    "text": "Distance and dissimilarity\n\nVariety of measures for distance and similarity\n\nAxiomatic definition\nFor a measure of distance \\(d\\) between the multidimensional points \\(x_i\\) and \\(x_j\\) the following conditions apply:\n\n\\(d(x_i, x_j) \\ge 0\\) (distances are similar or equal to zero),\n\\(d(x_i, x_j)=d(x_j,x_i)\\) (the distance from A to B is the same as from B to A),\n\\(d(x_i, x_i)=0\\) (the distance from a given point to itself is zero).\n\nBeyond that, a distance measure is termed metric, if:\n\n\\(d=0\\) applies in the case of equality only, and\nthe triangle inequality (the indirect route is longer than the direct route) applies too.\n\nIf one or both of the additonal conditions are violated, we speak about nonmetric measures and use the term dissimilarity instead of distance."
  },
  {
    "objectID": "slides/10-multivariate.html#similarity",
    "href": "slides/10-multivariate.html#similarity",
    "title": "Multivariate methods",
    "section": "Similarity",
    "text": "Similarity\nA measure of similarity \\(s\\) can be defined in a similar way:\n\n\\(s(x_i,x_j) \\le s_{max}\\)\n\\(s(x_i,x_j)=s(x_j,x_i)\\)\n\\(s(x_i,x_i)=s_{max}\\)\n\nand it is metric, if:\n\n\\(s_{max}\\) applies only in the case of equality and\nthe triangle inequality applies too."
  },
  {
    "objectID": "slides/10-multivariate.html#most-important-distance-and-dissimilarity-measures-in-our-field",
    "href": "slides/10-multivariate.html#most-important-distance-and-dissimilarity-measures-in-our-field",
    "title": "Multivariate methods",
    "section": "Most important distance and dissimilarity measures in our field",
    "text": "Most important distance and dissimilarity measures in our field\n\n\nEuclidean distance (shortest connection between 2 points in space),\nManhattan distance (around the corner, as in Manhattans grid-like streets),\nChi-square distance for comparison of frequencies,\nMahalanobis distance (takes covariance into account),\nBray-Curtis dissimilarity index (created specifically for comparison of species lists in ecology).\nJaccard index for binary (presence-absence) data\nGower dissimilarity is used for mixed-type variables"
  },
  {
    "objectID": "slides/10-multivariate.html#formula",
    "href": "slides/10-multivariate.html#formula",
    "title": "Multivariate methods",
    "section": "Formula",
    "text": "Formula\nMetric data\nwith \\(x_{ij}, x_{ik}\\) abundance of species \\(i\\) at sites (\\(j, k\\)).\nEuclidean distance:\n[ d_{jk} = ]\nManhattan distance: [ d_{jk} = |x_{ij}-x_{ik}| ]\nGower distance: [ d_{jk} = ]\nBray-Curtis dissimilarity: [ d_{jk} = ]"
  },
  {
    "objectID": "slides/10-multivariate.html#binary-variables",
    "href": "slides/10-multivariate.html#binary-variables",
    "title": "Multivariate methods",
    "section": "Binary variables",
    "text": "Binary variables\nThe binary form is applicable to binary and factor variables, where:\n\n\\(A, B\\) = numbers of species on compared sites\n\\(J\\) = (joint) is the number of species that occur on both compared sites\n\\(M\\) = number of columns (excluding missing values)\n\nEuclidean: \\(\\sqrt{A+B-2J}\\)\nManhattan: \\(A+B-2J\\)\nGower: \\(\\frac{A+B-2J}{M}\\)\nBray-Curtis: \\(\\frac{A+B-2J}{A+B}\\)\nJaccard: \\(\\frac{2b}{1+b}\\) with \\(b\\) = Bray-Curtis dissimilarity\nApplications\nAdditional distance measures and application suggestions are found in the vegdist help page."
  },
  {
    "objectID": "slides/10-multivariate.html#load-the-data",
    "href": "slides/10-multivariate.html#load-the-data",
    "title": "Multivariate methods",
    "section": "Load the data",
    "text": "Load the data\nHere we load the data set and add English column names and abbreviated lake identifiers as row names to the table, that are useful for the multivariate plotting functions.\n\nlibrary(\"readxl\") # read Excel files directly\nlibrary(\"vegan\")  # multivariate statistics in ecology\nlakes <- as.data.frame(\n  read_excel(\"../data/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) <- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nrownames(lakes) <- paste0(1:nrow(lakes), substr(lakes$name, 1, 4))\n\nText columns, e.g Federal State names and lake type are removed and rows with missing data excluded. If population is not used, the analysis can be repeated with more lakes.\n\nvalid_columns <- c(\"drainage\", \"population\", \"altitude\", \"z_mean\",\n                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                   \"shore_devel\", \"drain_ratio\")\n\n#valid_columns <- c(\"drainage\", \"altitude\", \"z_mean\",\n#                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n#                   \"shore_devel\",\"drain_ratio\")\ndat <- lakes[valid_columns]\ndat <- na.omit(dat)"
  },
  {
    "objectID": "slides/10-multivariate.html#data-inspection",
    "href": "slides/10-multivariate.html#data-inspection",
    "title": "Multivariate methods",
    "section": "Data inspection",
    "text": "Data inspection\n\npar(mfrow = c(1, 1))\npar(mar = c(7, 4, 2, 1) + .1)\nboxplot(scale(dat), las = 2)\n\n\n\ndat$altitude <- ifelse(dat$altitude < 0, 0, dat$altitude)\nboxplot(scale(sqrt(dat)), las=2)"
  },
  {
    "objectID": "slides/10-multivariate.html#multivariate-analysis",
    "href": "slides/10-multivariate.html#multivariate-analysis",
    "title": "Multivariate methods",
    "section": "Multivariate Analysis",
    "text": "Multivariate Analysis\nPrincipal Components: PCA\n\npc <- prcomp(scale(dat))\nsummary(pc)\n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.305 1.4737 1.1459 1.0686 0.84953 0.50024 0.24164\nProportion of Variance 0.483 0.1974 0.1194 0.1038 0.06561 0.02275 0.00531\nCumulative Proportion  0.483 0.6805 0.7998 0.9036 0.96925 0.99200 0.99731\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.12590 0.08400 0.07563 0.03077\nProportion of Variance 0.00144 0.00064 0.00052 0.00009\nCumulative Proportion  0.99875 0.99939 0.99991 1.00000\n\nplot(pc)\n\n\n\nbiplot(pc)\n\n\n\n\nAs the PCA with the untransformed data looks somewhat asymmetric, we repeat it with square transformed data. In addition, also the 3rd PC is plotted.\n\ndat2 <- sqrt(dat)\npc2 <- prcomp(scale(dat2))\nsummary(pc2)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.1886 1.5906 1.2499 1.0634 0.79782 0.44854 0.28572\nProportion of Variance 0.4354 0.2300 0.1420 0.1028 0.05786 0.01829 0.00742\nCumulative Proportion  0.4354 0.6654 0.8075 0.9103 0.96812 0.98641 0.99383\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.17665 0.13833 0.12041 0.05528\nProportion of Variance 0.00284 0.00174 0.00132 0.00028\nCumulative Proportion  0.99666 0.99840 0.99972 1.00000\n\npar(mfrow=c(1,2))\npar(mar=c(5, 4, 4, 2) + 0.1)\nbiplot(pc2, cex=0.6)\nbiplot(pc2, cex=0.6, choices=c(3, 2))\n\n\n\n\nA PCA is also possible with the rda function of the vegan package. The syntax of the plot functions is somewhat different. Instead of biplot as above, we can directly use plot. Details are found in the vegan documentation.\n\npar(mfrow=c(1,1))\npc3 <- rda(dat2, scale = TRUE)\npc3\n\nCall: rda(X = dat2, scale = TRUE)\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\nInertia is correlations \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n4.790 2.530 1.562 1.131 0.637 0.201 0.082 0.031 0.019 0.014 0.003 \n\n#summary(pc3)\nplot(pc3)\n\n\n\n\nNonmetric Multidimensional Scaling: NMDS\nLt’s now perform an NMDS for the data set. Function metaMDS runs a series of NMDS fits with different start values to avoid local minima. It has also some automatic transformations built in and works usually with the Bray-Curtis dissimilarity, that is used for plants and animal species abundance data. As we work with physical data here, we set the distance measure to “euclidean”.\n\nmd <- metaMDS(dat2, scale = TRUE, distance = \"euclid\")\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1181117 \nRun 1 stress 0.1768603 \nRun 2 stress 0.1188525 \nRun 3 stress 0.1207019 \nRun 4 stress 0.1181118 \n... Procrustes: rmse 7.877917e-05  max resid 0.0002305161 \n... Similar to previous best\nRun 5 stress 0.1230331 \nRun 6 stress 0.1181116 \n... New best solution\n... Procrustes: rmse 0.0001656214  max resid 0.0004659157 \n... Similar to previous best\nRun 7 stress 0.2033062 \nRun 8 stress 0.120702 \nRun 9 stress 0.1181116 \n... New best solution\n... Procrustes: rmse 8.554226e-05  max resid 0.0002946338 \n... Similar to previous best\nRun 10 stress 0.1207021 \nRun 11 stress 0.1208973 \nRun 12 stress 0.1208973 \nRun 13 stress 0.1207022 \nRun 14 stress 0.1181118 \n... Procrustes: rmse 0.0001947428  max resid 0.0005781717 \n... Similar to previous best\nRun 15 stress 0.1230331 \nRun 16 stress 0.1230331 \nRun 17 stress 0.1230331 \nRun 18 stress 0.1208973 \nRun 19 stress 0.1230331 \nRun 20 stress 0.1181117 \n... Procrustes: rmse 0.0001136805  max resid 0.0003839603 \n... Similar to previous best\n*** Best solution repeated 3 times\n\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\n\n\nCluster analysis\nHere we apply a hierarchical cluster analysis with square root transformed data and two different agglomeration schemes, “complete linkage” and “Ward’s method”.\n\npar(mfrow=c(2,1))\nhc <- hclust(dist(scale(dat2)), method=\"complete\") # the default\nplot(hc)\n\nhc2 <- hclust(dist(scale(dat2)), method=\"ward.D2\")\nplot(hc2)\n\n\n\n\nWe can also use the clusters to indicate groups in the NMDS plot. Function rect.hclust indicates a given number of clusters in the dendrogram, then we cut the tree with cutree and use the groups grp as color codes. R has 8 standard colors. If we need more, we can define an own palette.\n\nplot(hc, hang = -1)\nrect.hclust(hc, 5)\n\n\n\ngrp <- cutree(hc, 5)\n# grp                  # can be used to show the groups\nplot(md, type = \"n\")\ntext(md$points, row.names(dat2), col = grp)\n\n\n\n\nInstead of hierarchical clustering, we can also use a non-hierarchical method, e.g. k-means clustering. This is an iterative method, and avoids the problem that cluster assignment depends on the order of clustering and the agglomeration method.\nDepending on the question, it may be a disadvantage, that the number of clusters needs to be specified beforehand (e.g. from hierarchical clustering) and that we do not get a tree diagramm."
  },
  {
    "objectID": "slides/10-multivariate.html#task",
    "href": "slides/10-multivariate.html#task",
    "title": "Multivariate methods",
    "section": "Task",
    "text": "Task\n\nTry to understand the analysis,\ndiscuss the results,\nask questions.\nThe idea is to work on this report together and to make it more complete."
  },
  {
    "objectID": "slides/10-multivariate.html#data-sets-and-terms-of-use",
    "href": "slides/10-multivariate.html#data-sets-and-terms-of-use",
    "title": "Multivariate methods",
    "section": "Data sets and terms of use",
    "text": "Data sets and terms of use\n\n\nThe “uba-lakes” data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe “bm-lakes” data set is a teaching data set, derived from historical measurements of lakes in Brandenburg and Mecklenburg. The data werde taken from the literature (koschel_primary_1985?) and adapted to teaching purposes.\nThe document itself, the codes and the ebedded images are own work and can be shared according to CC BY 4.0."
  },
  {
    "objectID": "slides/10-multivariate.html#create-pairwise-scatter-plots-for-all-variables",
    "href": "slides/10-multivariate.html#create-pairwise-scatter-plots-for-all-variables",
    "title": "Multivariate methods",
    "section": "Create pairwise scatter plots for all variables?",
    "text": "Create pairwise scatter plots for all variables?\n\nplot(lakes)"
  },
  {
    "objectID": "slides/10-multivariate.html#create-pairwise-scatter-plots-for-all-variables-1",
    "href": "slides/10-multivariate.html#create-pairwise-scatter-plots-for-all-variables-1",
    "title": "Multivariate methods",
    "section": "Create pairwise scatter plots for all variables?",
    "text": "Create pairwise scatter plots for all variables?\n\n\nnot a good idea, 14 variables would produce 182 (or 91) plots\ncan lead to “statistical fishing”\nwe need methods to extract the main information with a small number of plots"
  },
  {
    "objectID": "slides/10-multivariate.html#conversion-between-dissimilarity-and-similarity",
    "href": "slides/10-multivariate.html#conversion-between-dissimilarity-and-similarity",
    "title": "Multivariate methods",
    "section": "Conversion between dissimilarity and similarity",
    "text": "Conversion between dissimilarity and similarity\n\n\ndifferent methods, as long as the \\(\\Rightarrow\\) transformation is monotonic.\n\n\n\n\n\nsimilarity\ndissimilarity\n\n\n\n\n\\(s=1-d/d_{max}\\)\n\\(d=1-s/s_{max}\\)\n\n\n\\(s=\\exp(-d)\\)\n\\(d= - \\ln(s-s_{min})\\)\n\n\n\n\n\ndistance goes from \\(0\\) to \\(\\infty\\)\nin most cases \\(s\\) is limited between \\((0, 1)\\) or between 0 and 100%."
  },
  {
    "objectID": "slides/10-multivariate.html#formulae",
    "href": "slides/10-multivariate.html#formulae",
    "title": "Multivariate methods",
    "section": "Formulae",
    "text": "Formulae\nMetric data\nwith \\(x_{ij}, x_{ik}\\) abundance of species \\(i\\) at sites (\\(j, k\\)).\nEuclidean distance:\n\\[\nd_{jk} = \\sqrt{\\sum (x_{ij}-x_{ik})^2}\n\\]\nManhattan distance: \\[\nd_{jk} = \\sum |x_{ij}-x_{ik}|\n\\]\nGower distance: \\[\nd_{jk} = \\frac{1}{M} \\sum\\frac{|x_{ij}-x_{ik}|}{\\max(x_i)-\\min(x_i)}\n\\]\nBray-Curtis dissimilarity: \\[\nd_{jk} = \\frac{\\sum{|x_{ij}-x_{ik}|}}{\\sum{(x_{ij}+x_{ik})}}\n\\]"
  },
  {
    "objectID": "slides/10-multivariate.html#the-uba-lakes-example",
    "href": "slides/10-multivariate.html#the-uba-lakes-example",
    "title": "Multivariate methods",
    "section": "The UBA-lakes example",
    "text": "The UBA-lakes example\nHere we load the data set and add English column names and abbreviated lake identifiers as row names to the table, that are useful for the multivariate plotting functions.\n\nlibrary(\"readxl\") # read Excel files directly\nlibrary(\"vegan\")  # multivariate statistics in ecology\nlakes <- as.data.frame(\n  read_excel(\"../data/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) <- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nrownames(lakes) <- paste0(1:nrow(lakes), substr(lakes$name, 1, 4))\n\nText columns, e.g Federal State names and lake type are removed and rows with missing data excluded. If population is not used, the analysis can be repeated with more lakes.\n\nvalid_columns <- c(\"drainage\", \"population\", \"altitude\", \"z_mean\",\n                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                   \"shore_devel\", \"drain_ratio\")\n\n#valid_columns <- c(\"drainage\", \"altitude\", \"z_mean\",\n#                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n#                   \"shore_devel\",\"drain_ratio\")\ndat <- lakes[valid_columns]\ndat <- na.omit(dat)"
  },
  {
    "objectID": "slides/10-multivariate.html#principal-components-pca",
    "href": "slides/10-multivariate.html#principal-components-pca",
    "title": "Multivariate methods",
    "section": "Principal Components: PCA",
    "text": "Principal Components: PCA\n\npc <- prcomp(scale(dat))\nsummary(pc)\n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.305 1.4737 1.1459 1.0686 0.84953 0.50024 0.24164\nProportion of Variance 0.483 0.1974 0.1194 0.1038 0.06561 0.02275 0.00531\nCumulative Proportion  0.483 0.6805 0.7998 0.9036 0.96925 0.99200 0.99731\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.12590 0.08400 0.07563 0.03077\nProportion of Variance 0.00144 0.00064 0.00052 0.00009\nCumulative Proportion  0.99875 0.99939 0.99991 1.00000\n\nplot(pc)\n\n\n\nbiplot(pc)\n\n\n\n\nAs the PCA with the untransformed data looks somewhat asymmetric, we repeat it with square transformed data. In addition, also the 3rd PC is plotted.\n\ndat2 <- sqrt(dat)\npc2 <- prcomp(scale(dat2))\nsummary(pc2)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.1886 1.5906 1.2499 1.0634 0.79782 0.44854 0.28572\nProportion of Variance 0.4354 0.2300 0.1420 0.1028 0.05786 0.01829 0.00742\nCumulative Proportion  0.4354 0.6654 0.8075 0.9103 0.96812 0.98641 0.99383\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.17665 0.13833 0.12041 0.05528\nProportion of Variance 0.00284 0.00174 0.00132 0.00028\nCumulative Proportion  0.99666 0.99840 0.99972 1.00000"
  },
  {
    "objectID": "slides/10-multivariate.html#nonmetric-multidimensional-scaling-nmds",
    "href": "slides/10-multivariate.html#nonmetric-multidimensional-scaling-nmds",
    "title": "Multivariate methods",
    "section": "Nonmetric Multidimensional Scaling: NMDS",
    "text": "Nonmetric Multidimensional Scaling: NMDS\nLt’s now perform an NMDS for the data set. Function metaMDS runs a series of NMDS fits with different start values to avoid local minima. It has also some automatic transformations built in and works usually with the Bray-Curtis dissimilarity, that is used for plants and animal species abundance data. As we work with physical data here, we set the distance measure to “euclidean”.\n\nmd <- metaMDS(dat2, scale = TRUE, distance = \"euclid\")\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1181117 \nRun 1 stress 0.1181117 \n... New best solution\n... Procrustes: rmse 0.0002188959  max resid 0.0006138061 \n... Similar to previous best\nRun 2 stress 0.1768603 \nRun 3 stress 0.2228467 \nRun 4 stress 0.1181118 \n... Procrustes: rmse 7.429828e-05  max resid 0.0002491791 \n... Similar to previous best\nRun 5 stress 0.1181117 \n... Procrustes: rmse 0.00020814  max resid 0.0005820668 \n... Similar to previous best\nRun 6 stress 0.1877812 \nRun 7 stress 0.1230331 \nRun 8 stress 0.120702 \nRun 9 stress 0.1181116 \n... New best solution\n... Procrustes: rmse 0.0001509633  max resid 0.0004388941 \n... Similar to previous best\nRun 10 stress 0.1731461 \nRun 11 stress 0.1208973 \nRun 12 stress 0.1768603 \nRun 13 stress 0.1208972 \nRun 14 stress 0.1230331 \nRun 15 stress 0.1230331 \nRun 16 stress 0.1181117 \n... Procrustes: rmse 6.859029e-05  max resid 0.0001820161 \n... Similar to previous best\nRun 17 stress 0.1188524 \nRun 18 stress 0.1230331 \nRun 19 stress 0.1188531 \nRun 20 stress 0.1230331 \n*** Best solution repeated 2 times\n\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")"
  },
  {
    "objectID": "slides/10-multivariate.html#cluster-analysis",
    "href": "slides/10-multivariate.html#cluster-analysis",
    "title": "Multivariate methods",
    "section": "Cluster analysis",
    "text": "Cluster analysis\nHere we apply a hierarchical cluster analysis with square root transformed data and two different agglomeration schemes, “complete linkage” and “Ward’s method”.\n\npar(mfrow=c(2,1))\nhc <- hclust(dist(scale(dat2)), method=\"complete\") # the default\nplot(hc)\n\nhc2 <- hclust(dist(scale(dat2)), method=\"ward.D2\")\nplot(hc2)\n\n\n\n\nWe can also use the clusters to indicate groups in the NMDS plot. Function rect.hclust indicates a given number of clusters in the dendrogram, then we cut the tree with cutree and use the groups grp as color codes. R has 8 standard colors. If we need more, we can define an own palette.\n\nplot(hc, hang = -1)\nrect.hclust(hc, 5)\n\n\n\ngrp <- cutree(hc, 5)\n# grp                  # can be used to show the groups\nplot(md, type = \"n\")\ntext(md$points, row.names(dat2), col = grp)\n\n\n\n\nInstead of hierarchical clustering, we can also use a non-hierarchical method, e.g. k-means clustering. This is an iterative method, and avoids the problem that cluster assignment depends on the order of clustering and the agglomeration method.\nDepending on the question, it may be a disadvantage, that the number of clusters needs to be specified beforehand (e.g. from hierarchical clustering) and that we do not get a tree diagramm."
  },
  {
    "objectID": "slides/10-multivariate.html#biplot",
    "href": "slides/10-multivariate.html#biplot",
    "title": "Multivariate methods",
    "section": "Biplot",
    "text": "Biplot\n\npar(mfrow=c(1,2))\npar(mar=c(5, 4, 4, 2) + 0.1)\nbiplot(pc2, cex=0.6)\nbiplot(pc2, cex=0.6, choices=c(3, 2))\n\n\n\n\nA PCA is also possible with the rda function of the vegan package. The syntax of the plot functions is somewhat different. Instead of biplot as above, we can directly use plot. Details are found in the vegan documentation.\n\npar(mfrow=c(1,1))\npc3 <- rda(dat2, scale = TRUE)\npc3\n\nCall: rda(X = dat2, scale = TRUE)\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\nInertia is correlations \n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n4.790 2.530 1.562 1.131 0.637 0.201 0.082 0.031 0.019 0.014 0.003 \n\n#summary(pc3)\nplot(pc3)"
  }
]