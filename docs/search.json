[
  {
    "objectID": "tutorials/s3-multivar-lakes.html",
    "href": "tutorials/s3-multivar-lakes.html",
    "title": "Multivariate Lake Data Example",
    "section": "",
    "text": "The following example demonstrates basic multivariate principles by means of a teaching example. A detailed description of theory and applications is found in excellent books of Legendre & Legendre (1998) and Borcard et al. (2018). Practical help is found in the tutorials of the vegan package (Oksanen et al., 2020).",
    "crumbs": [
      "Tutorials",
      "Multivariate Lake Data Example"
    ]
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#introduction",
    "href": "tutorials/s3-multivar-lakes.html#introduction",
    "title": "Multivariate Lake Data Example",
    "section": "",
    "text": "The following example demonstrates basic multivariate principles by means of a teaching example. A detailed description of theory and applications is found in excellent books of Legendre & Legendre (1998) and Borcard et al. (2018). Practical help is found in the tutorials of the vegan package (Oksanen et al., 2020).",
    "crumbs": [
      "Tutorials",
      "Multivariate Lake Data Example"
    ]
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#data-set-and-terms-of-use",
    "href": "tutorials/s3-multivar-lakes.html#data-set-and-terms-of-use",
    "title": "Multivariate Lake Data Example",
    "section": "2 Data set and terms of use",
    "text": "2 Data set and terms of use\nThe lake data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe document and codes provided here can be shared according to CC BY 4.0.",
    "crumbs": [
      "Tutorials",
      "Multivariate Lake Data Example"
    ]
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#load-the-data",
    "href": "tutorials/s3-multivar-lakes.html#load-the-data",
    "title": "Multivariate Lake Data Example",
    "section": "3 Load the data",
    "text": "3 Load the data\nHere we load the data set and add English column names and abbreviated lake identifiers as row names to the table, that are useful for the multivariate plotting functions.\n\nlibrary(\"readxl\") # read Excel files directly\nlibrary(\"vegan\")  # multivariate statistics in ecology\nlakes &lt;- as.data.frame(\n  read_excel(\"../data/uba/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) &lt;- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nrownames(lakes) &lt;- paste0(1:nrow(lakes), substr(lakes$name, 1, 4))\n\nText columns, e.g Federal State names and lake type are removed and rows with missing data excluded. If population is not used, the analysis can be repeated with more lakes.\n\nvalid_columns &lt;- c(\"drainage\", \"population\", \"altitude\", \"z_mean\",\n                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                   \"shore_devel\", \"drain_ratio\")\n\n#valid_columns &lt;- c(\"drainage\", \"altitude\", \"z_mean\",\n#                   \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n#                   \"shore_devel\",\"drain_ratio\")\ndat &lt;- lakes[valid_columns]\ndat &lt;- na.omit(dat)",
    "crumbs": [
      "Tutorials",
      "Multivariate Lake Data Example"
    ]
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#data-inspection",
    "href": "tutorials/s3-multivar-lakes.html#data-inspection",
    "title": "Multivariate Lake Data Example",
    "section": "4 Data inspection",
    "text": "4 Data inspection\nIt is alwas a good idea to plot the data first, as time series or boxplots for example, dependingon the type of data. Here we use boxplots, that we scale (z-transform) to a mean zero and standard deviation one to have comparable values.\nAs we can see a number of high extreme values, we apply also a square root transformation, that is less extreme than log transform and not sensitive against zero values, but because altitude contains a negative value (below sea level) we replace this with zero. As it is a small value, it does not influence our analysis, but we should always be very careful to document such workarounds.\n\npar(mfrow = c(1, 1))\npar(mar = c(7, 4, 2, 1) + .1)\nboxplot(scale(dat), las = 2)\n\n\n\n\n\n\n\ndat$altitude &lt;- ifelse(dat$altitude &lt; 0, 0, dat$altitude)\nboxplot(scale(sqrt(dat)), las=2)",
    "crumbs": [
      "Tutorials",
      "Multivariate Lake Data Example"
    ]
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#multivariate-analysis",
    "href": "tutorials/s3-multivar-lakes.html#multivariate-analysis",
    "title": "Multivariate Lake Data Example",
    "section": "5 Multivariate Analysis",
    "text": "5 Multivariate Analysis\n\n5.1 Principal Components: PCA\n\npc &lt;- prcomp(scale(dat))\nsummary(pc)\n\nImportance of components:\n                         PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.305 1.4737 1.1459 1.0686 0.84953 0.50024 0.24164\nProportion of Variance 0.483 0.1974 0.1194 0.1038 0.06561 0.02275 0.00531\nCumulative Proportion  0.483 0.6805 0.7998 0.9036 0.96925 0.99200 0.99731\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.12590 0.08400 0.07563 0.03077\nProportion of Variance 0.00144 0.00064 0.00052 0.00009\nCumulative Proportion  0.99875 0.99939 0.99991 1.00000\n\nplot(pc)\n\n\n\n\n\n\n\nbiplot(pc)\n\n\n\n\n\n\n\n\nAs the PCA with the untransformed data looks somewhat asymmetric, we repeat it with square transformed data. In addition, also the 3rd PC is plotted.\n\ndat2 &lt;- sqrt(dat)\npc2 &lt;- prcomp(scale(dat2))\nsummary(pc2)\n\nImportance of components:\n                          PC1    PC2    PC3    PC4     PC5     PC6     PC7\nStandard deviation     2.1886 1.5906 1.2499 1.0634 0.79782 0.44854 0.28572\nProportion of Variance 0.4354 0.2300 0.1420 0.1028 0.05786 0.01829 0.00742\nCumulative Proportion  0.4354 0.6654 0.8075 0.9103 0.96812 0.98641 0.99383\n                           PC8     PC9    PC10    PC11\nStandard deviation     0.17665 0.13833 0.12041 0.05528\nProportion of Variance 0.00284 0.00174 0.00132 0.00028\nCumulative Proportion  0.99666 0.99840 0.99972 1.00000\n\npar(mfrow=c(1,2))\npar(mar=c(5, 4, 4, 2) + 0.1)\nbiplot(pc2, cex=0.6)\nbiplot(pc2, cex=0.6, choices=c(3, 2))\n\n\n\n\n\n\n\n\nA PCA is also possible with the rda function of the vegan package. The syntax of the plot functions is somewhat different. Instead of biplot as above, we can directly use plot. Details are found in the vegan documentation.\n\npar(mfrow=c(1,1))\npc3 &lt;- rda(dat2, scale = TRUE)\npc3\n\n\nCall: rda(X = dat2, scale = TRUE)\n\n              Inertia Rank\nTotal              11     \nUnconstrained      11   11\n\nInertia is correlations\n\nEigenvalues for unconstrained axes:\n  PC1   PC2   PC3   PC4   PC5   PC6   PC7   PC8   PC9  PC10  PC11 \n4.790 2.530 1.562 1.131 0.637 0.201 0.082 0.031 0.019 0.014 0.003 \n\n#summary(pc3)\nplot(pc3)\n\n\n\n\n\n\n\n\n\n\n5.2 Nonmetric Multidimensional Scaling: NMDS\nLt’s now perform an NMDS for the data set. Function metaMDS runs a series of NMDS fits with different start values to avoid local minima. It has also some automatic transformations built in and works usually with the Bray-Curtis dissimilarity, that is used for plants and animal species abundance data. As we work with physical data here, we set the distance measure to “euclidean”.\n\nmd &lt;- metaMDS(dat2, scale = TRUE, distance = \"euclid\")\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1181117 \nRun 1 stress 0.1230331 \nRun 2 stress 0.1230331 \nRun 3 stress 0.185611 \nRun 4 stress 0.203306 \nRun 5 stress 0.1207018 \nRun 6 stress 0.1230331 \nRun 7 stress 0.1207021 \nRun 8 stress 0.1181117 \n... Procrustes: rmse 4.697944e-05  max resid 0.0001383349 \n... Similar to previous best\nRun 9 stress 0.1181116 \n... New best solution\n... Procrustes: rmse 9.57108e-05  max resid 0.0002791979 \n... Similar to previous best\nRun 10 stress 0.1230331 \nRun 11 stress 0.1230331 \nRun 12 stress 0.1230331 \nRun 13 stress 0.1207019 \nRun 14 stress 0.2014735 \nRun 15 stress 0.1207021 \nRun 16 stress 0.1999403 \nRun 17 stress 0.1207019 \nRun 18 stress 0.1230331 \nRun 19 stress 0.1230331 \nRun 20 stress 0.1230331 \n*** Best solution repeated 1 times\n\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\n\n\n\n\n\n\n\n\n5.3 Cluster analysis\nHere we apply a hierarchical cluster analysis with square root transformed data and two different agglomeration schemes, “complete linkage” and “Ward’s method”.\n\npar(mfrow=c(2,1))\nhc &lt;- hclust(dist(scale(dat2)), method=\"complete\") # the default\nplot(hc)\n\nhc2 &lt;- hclust(dist(scale(dat2)), method=\"ward.D2\")\nplot(hc2)\n\n\n\n\n\n\n\n\nWe can also use the clusters to indicate groups in the NMDS plot. Function rect.hclust indicates a given number of clusters in the dendrogram, then we cut the tree with cutree and use the groups grp as color codes. R has 8 standard colors. If we need more, we can define an own palette.\n\nplot(hc, hang = -1)\nrect.hclust(hc, 5)\n\n\n\n\n\n\n\ngrp &lt;- cutree(hc, 5)\n# grp                  # can be used to show the groups\nplot(md, type = \"n\")\ntext(md$points, row.names(dat2), col = grp)\n\n\n\n\n\n\n\n\nInstead of hierarchical clustering, we can also use a non-hierarchical method, e.g. k-means clustering. This is an iterative method, and avoids the problem that cluster assignment depends on the order of clustering and the agglomeration method.\nDepending on the question, it may be a disadvantage, that the number of clusters needs to be specified beforehand (e.g. from hierarchical clustering) and that we do not get a tree diagramm.",
    "crumbs": [
      "Tutorials",
      "Multivariate Lake Data Example"
    ]
  },
  {
    "objectID": "tutorials/s3-multivar-lakes.html#task",
    "href": "tutorials/s3-multivar-lakes.html#task",
    "title": "Multivariate Lake Data Example",
    "section": "6 Task",
    "text": "6 Task\n\nTry to understand the analysis,\ndiscuss the results,\nask questions.\nThe idea is to work on this report together and to make it more complete.",
    "crumbs": [
      "Tutorials",
      "Multivariate Lake Data Example"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html",
    "href": "tutorials/s1-introductory-r-session.html",
    "title": "An Introductory R Session",
    "section": "",
    "text": "The following section is intended to give you a first feeling what R (R Core Team, 2021) is and how it works. It assumes that the following software is installed:\n\nThe R system for statistical computing: https://www.r-project.org\nR Studio, a program that makes working with R more convenient: https://www.rstudio.org\n\nNote: please install R first before installing RStudio.\nYou may also consider to watch a Youtube video about R, a good example for the very beginning is the short video “R tutorial - The True Basics of R” from DataCamp, see https://youtu.be/SWxoJqTqo08. Besides this, Datacamp offers excellent R tutorials, part of them free of cost. Another useful R tutorial can be found at W3Schools.com.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#program-start-and-help-system",
    "href": "tutorials/s1-introductory-r-session.html#program-start-and-help-system",
    "title": "An Introductory R Session",
    "section": "2.1 Program start and help system",
    "text": "2.1 Program start and help system\nThe easiest way to learn R is the creative understanding and modification of given examples, the usage of R for solving practical problems and the diagnosis of the frequently occurring problems and error messages. Don’t worry: error messages are a normal phenomenon in scientific computing and not an indication of a dysfunction of the computer or the human brain. The opposite is true, a certain amount of stress hormones helps to acquire permanent learning effects. Then, after a certain level of experience reading the official R-Documentation “An Introduction to R” (Venables et al., 2021). or any good R-book is strongly recommended.\nThe first sections of this “crash course” are intended to give an overview over some of the most important elements of R and an insight into a typical work flow, that may be useful for the first statistical analyses and as a starting point for self-education.\nWe begin our first session by starting RStudio, a platform independent interface that makes working with R easier. RStudio divides the screen into 3 (resp. 4) windows (called panes), where some of them have additional tabs to switch between different views.\n\nFigure 1: R Studio with 4 panes. Use File – New R Script to open the the source code pane (shown top left). Then enter some code and don’t forget to explore the help files.\nIn a fresh RStudio session, one “Pane” should be the main help page of R. It is a good idea to browse a little bit around to get an impression about the amount and the typical style of the available help topics. The most important sections are “An Introduction to R”, “Search Engine & Keywords”, “Packages”, the “Frequently Asked Questions” and possibly “R Data Import/Export”.\nWe start now to explore the R-System itself.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#r-as-a-pocket-calculator",
    "href": "tutorials/s1-introductory-r-session.html#r-as-a-pocket-calculator",
    "title": "An Introductory R Session",
    "section": "2.2 R as a pocket calculator",
    "text": "2.2 R as a pocket calculator\nEntering an arithmetic expression like this:\n\n2 + 4\n\nshows that R can be used as a pocket calculator, that immediately outputs the result:\n\n\n[1] 6\n\n\nInstead of printing the result to the screen, it is also possible to save the result into a named variable using the assignment operator “&lt;-”.\n\na &lt;- 2 + 4\n\nIt seems that nothing happens, but the result is now saved in the variable a that can be recalled at any time by entering the variable name alone:\n\na\n\nVariable names in R start always with a character (or for special purposes a dot), followed by further characters, numerals, dots or underscores, where a distinction is made between small and capital letters, i.e. the variables value, Value and VALUE can contain different data. A few character combinations are reserved words and cannot be used as variables:\nbreak, for, function, if, in, next, repeat, while and “...” (three dots).\nOther identifiers like plot can be re-defined, but this should be done with care to avoid unwanted confusion and side effects.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#vectors",
    "href": "tutorials/s1-introductory-r-session.html#vectors",
    "title": "An Introductory R Session",
    "section": "2.3 Vectors",
    "text": "2.3 Vectors\nYou may have noticed, that the output of the example above had a leading [1], which means that the line begins with the first element of a. This brings us to a very important feature of R that variables can contain more than single values: vectors, matrices, lists, data frames (tables) and so on.\nThe most basic data type is the vector, that can be filled with data using the c (combine) function:\n\nvalues &lt;- c(2, 3, 5, 7, 8.3, 10)\nvalues\n\n[1]  2.0  3.0  5.0  7.0  8.3 10.0\n\n\nTo create a sequence of values, one can use the : (colon):\n\nx &lt;- 1:10\nx\n\nor, even more flexibly the seq function:\n\nx &lt;- seq(2, 4, 0.25)\nx\n\nSequences of repeated equal values can be obtained with rep:\n\nx &lt;- rep(2, 4)\nx",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercise",
    "href": "tutorials/s1-introductory-r-session.html#exercise",
    "title": "An Introductory R Session",
    "section": "2.4 Exercise",
    "text": "2.4 Exercise\nThere are many ways to use these functions, try for example:\n\nseq(0, 10)\nseq(0, 10, by = 2)\nseq(0, pi, length = 12)\nrep(c(0, 1, 2, 4, 9), times = 5)\nrep(c(0, 1, 2, 4, 9), each = 2)\nrep(c(0, 1, 2, 4, 9), each = 2, times = 5)",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#access-to-vector-elements",
    "href": "tutorials/s1-introductory-r-session.html#access-to-vector-elements",
    "title": "An Introductory R Session",
    "section": "2.5 Access to vector elements",
    "text": "2.5 Access to vector elements\nInstead of accessing vectors as a whole, it is also possible to extract single elements, where the index of the requested data is itself a vector:\n\nvalues[5]\nvalues[2:4]\nvalues[c(1, 3, 5)]\n\nSometimes, elements of a vector may have individual names, which makes it easy to access them:\n\nnamed &lt;- c(a = 1, b = 2.3, c = 4.5)\nnamed\nnamed[\"a\"]\n\nIn R (and in contrast to other languages like C/C++) vector indices start with 1. Negative indices are also possible, but they have the special purpose to delete one or several elements:\n\nvalues[-3]\n\nIt is also possible to extend a given vector by preceding or appending values with the combine function (c):\n\nc(1, 1, values, 0, 0)\n\nThe length of a vector can be determined with:\n\nlength(values)\n\nand it is also possible to have empty vectors, i.e. vectors that exist, but do not contain any values. Here the keyword NULL means “nothing” in contrast to “0” (zero) that has length 1:\n\nvalues &lt;- NULL\nvalues\nlength(values)\n\nSuch empty vectors are sometimes used as “containers” for appending data step by step:\n\nvalues &lt;- NULL\nvalues\nlength(values)\nvalues &lt;- c(values, 1)\nvalues\nvalues &lt;- c(values, 1.34)\nvalues\n\nIf a data element should be removed completely, this can be done using the remove function:\nrm(values)\nvalues\nError: Object \"values\" not found\nThe complete workspace can be deleted from the menu of R or RStudio (Session – Clear workspace) or from the command line with rm (remove):\n\nrm(list = ls(all = TRUE))\n\nThe R session can be closed by using the menu as usual or by entering:\n\nq()\n\nSometimes and depending of the configuration, R asks whether the “R workspace” should be saved to the disk. This may be useful for continuing work at a later time, but has the risk to clutter the workspace and to get irreproducible results at a later session, so it is recommended to say “No” for now, except if you exactly know why.\nLater we will learn how to save only the data (and commands) that are needed.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#r-as-function-plotter",
    "href": "tutorials/s1-introductory-r-session.html#r-as-function-plotter",
    "title": "An Introductory R Session",
    "section": "3.1 R as function plotter",
    "text": "3.1 R as function plotter\nNow, we will see how to use R as a function plotter by drawing sine or cosine functions within an interval between 0 to 10. First, we create two vectors with x and y. To obtain a smooth curve, it is reasonable to choose a small step size. As a rule of thumb I always recommend to use about 100…400 small steps as a good compromise between smoothness and memory requirements, so let’s set the step size to 0.1:\n\nx &lt;- seq(0, 10, 0.1)\ny &lt;- sin(x)\nplot(x, y)\n\n\n\n\n\n\n\n\nInstead of plotting points, we can also draw continuous lines. This is indicated by supplying an optional argument type = \"l\".\nNote: the symbol used here for type is the small letter “L” for “line” and not the – in printing very similar – numeral “1” (one)!\nWe see also, that optional arguments like type can be given as “keyword = value” pair. This has the advantage that the order of arguments does not matter, because arguments are referenced by their name:\n\nplot(x, y, type = \"l\")\n\nNow we want to add a cosine function with another color. This can be done with one of the function lines or points, for adding lines or points to an existing figure:\n\ny1 &lt;- cos(x)\nlines(x, y1, col = \"red\")\n\nWith the help of text it is also possible to add arbitrary text, by specifying first the x and y coordinates and then the text:\n\nx1 &lt;- 1:10\ntext(x1, sin(x1), x1, col = \"green\")\n\nMany options exist to modify the behavior of most graphics functions so the following specifies user-defined coordinate limits (xlim, ylim), axis labels and a heading (xlab, ylab, main).\n\nplot(x, y, xlim = c(-10, 10), ylim = c(-2, 2),\n    xlab = \"x-Values\", ylab = \"y-Values\", main = \"Example Graphics\")\n\nCode formatting and line breaks\nThe above example shows a rather long command that may not fit on a single line. In such cases, R displays a + (plus sign) to indicate that a command must be continued, e.g. because a closing parenthesis or a closing quote is still missing. Such a + at the beginning of a line is an automatic “prompt” similar to the ordinary &gt; prompt and must never be typed in manually. If, however, the + continuation prompt occurs by accident, press “ESC” to cancel this mode.\nIn contrast to the long line continuation prompt, it is also possible to write several commands on one line, separated by a semi-colon “;”. This is unseful in some cases, but as a general rule it is much better to use the script editor and then to:\n\nwrite each command to a separate line\navoid long lines with more than about 80 characters\nuse proper indentation, e.g. 2 characters per indentation level\nuse spacing to improve readability of the code, e.g. before and after the assignment operator &lt;-.\n\nFinally, a number symbol (or hash) # means that a complete line or the part of the line that follows # is a comment and should be ignored by R.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#additional-plotting-options",
    "href": "tutorials/s1-introductory-r-session.html#additional-plotting-options",
    "title": "An Introductory R Session",
    "section": "3.2 Additional plotting options",
    "text": "3.2 Additional plotting options\nIn order to explore the wealth of graphical functions, you may now have a more extensive look into the online help, especially regarding ?plot or ?plot.default, and you should experiment a little bit with different plotting parameters, like lty, pch, lwd, type, log etc. R contains uncountable possibilities to get full control over the style and content of your graphics, e.g. with user-specified axes (axis), legends (legend) or user-defined lines and areas (abline, rect, polygon). The general style of figures like (font size, margins, line width) can be influenced with the par function.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#high-level-plotting-functions",
    "href": "tutorials/s1-introductory-r-session.html#high-level-plotting-functions",
    "title": "An Introductory R Session",
    "section": "3.3 High level plotting functions",
    "text": "3.3 High level plotting functions\nIn addition, R and its packages contain numerous “high level”-graphics functions for specific purposes. To demonstrate a few, we first generate a data set with normally distributed random numbers (mean = 0, standard deviation sd = 1), then we plot them and create a histogram. Here, the function par(mfrow = c(2, 2)) divides the plotting area into 2 rows and 2 columns to show 4 separate figures:\n\npar(mfrow = c(2, 2))\nx &lt;- rnorm(100)\nplot(x)\nhist(x)\n\nNow, we add a so-called normal probability plot and a second histogram with relative frequencies together with the bell-shaped density curve of the standard normal distribution. The optional argument probability = TRUE makes sure that the histogram has the same scaling as the density function, so that both can be overlayed:\n\nqqnorm(x)\nqqline(x, col = \"red\")\nhist(x, probability = TRUE)\nxx &lt;- seq(-3, 3, 0.1)\nlines(xx, dnorm(xx, 0, 1), col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\nHere it may also be a good chance to do a little bit summary statistics like: z.B. mean(x), var(x), sd(x), range(x), summary(x), min(x), max(x), …\nOr we may consider to test if the generated random numbers x are approximately normal distributed using the Shapiro-Wilks-W-Test:\n\nx &lt;- rnorm(100)\nshapiro.test(x)\n\nA p-value bigger than 0.05 tells us that the test has no objections against normal distribution of the data. The concrete results may differ, because x contains random numbers, so it makes sense to repeat this several times. It can be also useful compare these normally distributed random numbers generated with rnorm with uniformly distributed random numbers generated with runif:\n\npar(mfrow=c(2,2))\ny &lt;- runif(100)\nplot(y)\nhist(y)\nqqnorm(y)\nqqline(y, col=\"red\")\nmean(y)\nvar(y)\nmin(y)\nmax(y)\nhist(y, probability=TRUE)\nyy &lt;- seq(min(y), max(y), length = 50)\nlines(yy, dnorm(yy, mean(y), sd(y)), col = \"red\")\nshapiro.test(y)\n\nAt the end, we compare the pattern of both data sets with box-and-whisker plots:\n\npar(mfrow=c(1, 1))\nboxplot(x, y)",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercises",
    "href": "tutorials/s1-introductory-r-session.html#exercises",
    "title": "An Introductory R Session",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\nRepeat this example with new random numbers and vary sample size (n), mean value (mean) and standard deviation (sd) for random numbers created with rnorm, and use different min and max for runif. Consult the help pages for an explanation of the functions and its arguments, and create boxplots with different data sets.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#numeric-and-character-vectors",
    "href": "tutorials/s1-introductory-r-session.html#numeric-and-character-vectors",
    "title": "An Introductory R Session",
    "section": "4.1 Numeric and character vectors",
    "text": "4.1 Numeric and character vectors\nAll data objects have the two built-in attributes mode (data type) and length (number of data in the object).\nModes can be “numeric” for calculations or “character” for text elements.\n\nx &lt;- c(1, 3, 4, 5)       # numeric\na &lt;- c(\"hello\", \"world\") # character\n\nThe following is also a character variable, because the numbers are given in quotes. It is then not possible to do calculations:\n\nx &lt;- c(\"1\", \"3\", \"4\", \"5\")  # character\nsum(x)\n\nError in sum(x) : invalid 'type' (character) of argument\n\nHere it is necessary to convert the character to numeric first:\n\ny &lt;- as.numeric(x)\nsum(y)\n\n[1] 9.040591",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#factors",
    "href": "tutorials/s1-introductory-r-session.html#factors",
    "title": "An Introductory R Session",
    "section": "4.2 Factors",
    "text": "4.2 Factors\nA special kind of mode is factor. This is, statistically speaking, a nominal variable that appears like characters, e.g. “control”, “treatment A”, “treatment B” …, but its levels are internally encoded as integer.\nHere a typical example with three factor levels. In a first step,let’s create a character variable:\n\ntext &lt;- rep(c(\"control\", \"treatment A\", \"treatment B\"), each=5)\ntext\n\n [1] \"control\"     \"control\"     \"control\"     \"control\"     \"control\"    \n [6] \"treatment A\" \"treatment A\" \"treatment A\" \"treatment A\" \"treatment A\"\n[11] \"treatment B\" \"treatment B\" \"treatment B\" \"treatment B\" \"treatment B\"\n\n\nand then convert it to a factor:\n\nf &lt;- factor(text)\nf\n\n [1] control     control     control     control     control     treatment A\n [7] treatment A treatment A treatment A treatment A treatment B treatment B\n[13] treatment B treatment B treatment B\nLevels: control treatment A treatment B\n\n\nWe see that the character variable is printed with quotes and the factor without quotes, but with an additional information about the Levels. The reason for this is, that the factor is internally encoded as integer values with assigned levels as a translation table:\n\nlevels(f)\n\n[1] \"control\"     \"treatment A\" \"treatment B\"\n\n\nTo show encoding, we can convert the factor into an integer\n\nas.integer(f)\n\n [1] 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3\n\n\nThe encoding is done in alphabetical order by default. It can be changed by using an additional levels argument:\n\nf2 &lt;- factor(text, levels=c(\"treatment A\", \"treatment B\", \"control\"))\nf2\n\n [1] control     control     control     control     control     treatment A\n [7] treatment A treatment A treatment A treatment A treatment B treatment B\n[13] treatment B treatment B treatment B\nLevels: treatment A treatment B control\n\nas.numeric(f2)\n\n [1] 3 3 3 3 3 1 1 1 1 1 2 2 2 2 2\n\n\nFactors are useful for statistical analyses like ANOVA and statistical tests, and also as categories for plotting:\n\nx &lt;- c(7.44, 6.45, 6.04, 5.58, 4.5, 8.13, 5.54, 7.34, 8.91, 5.16, 8.7, 7.74, 6.8, 6.49, 6.2)\nplot(x ~ f)\n\n\n\n\n\n\n\n\nWe see that a boxplot is created and no x-y-plot, because the explanation variable is a factor.\nNumeric variables (especially ordinal) can also be converted to factors. This is useful, if we want to make clear, that numbers are to be treated as name without order.\nHowever, conversion of such factors back into numeric variables types should be done with care, because a character “123” may be encoded with another value (e.g. 1) and not 123, see the following demonstration of a correct and wrong factor conversions:\n\nx &lt;- c(2, 4, 6, 5, 8)\nf &lt;- as.factor(x)\nas.numeric(f)               # wrong !!!\nas.numeric(as.character(f)) # correct\nas.numeric(levels(f))[f]    # even better\n\nSuch a factor coding is not specific to R and appears also in other statistics packages. Then they are sometimes called “dummy variables”.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#matrices-and-arrays",
    "href": "tutorials/s1-introductory-r-session.html#matrices-and-arrays",
    "title": "An Introductory R Session",
    "section": "4.3 Matrices and arrays",
    "text": "4.3 Matrices and arrays\nA matrix is a two-dimensional data structure that can be used for matrix algebra. To create a matrix, we can first create a one-dimensional vector and then reformat it as two-dimensional matrix with nrow rows and ncolcolumns:\n\nx &lt;- 1:20\nx\n\ny &lt;- matrix(x, nrow = 5, ncol = 4)\ny\n\nWe see that the matrix is filled rowwise. We can also convert it back to a vector:\n\nas.vector(y) # flattens the matrix to a vector\n\nAn array extends the matrix concept to more than two dimensions:\n\nx &lt;- array(1:24, dim=c(3, 4, 2))\n\nVectors, matrices and arrays have an important limitation: they can only contain one data type (mode), either numeric or character. So, if a single element is of type character, the whole matrix will be of mode character and appears in quotes:\n\nx &lt;- c(1, 2, 5, 2, \"a\")\nmode(x)\nx",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#lists",
    "href": "tutorials/s1-introductory-r-session.html#lists",
    "title": "An Introductory R Session",
    "section": "4.4 Lists",
    "text": "4.4 Lists\nThe most flexible data type of R is the list. It can contain arbitrary data of different modes. Lists can be nested to form a tree-like structure:\n\nl &lt;- list(x = 1:3, y = c(1,2,3,4), a = \"hello\", L = list(x = 1, y = 2))\n\nLists are extremely powerful and flexible and may be discussed later. The impatient may have a look at the tutorial of w3schools.com.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#data-frames",
    "href": "tutorials/s1-introductory-r-session.html#data-frames",
    "title": "An Introductory R Session",
    "section": "4.5 Data frames",
    "text": "4.5 Data frames\nThe typical data structure for data analysis in R is the so-called data.frame. It can contain both, columns with numeric data and columns of mode character. Some packages use ando extended versions of data frames, a so called tibbles.\nA data frame can be constructed from scratch directly in the R code or read from a file or the internet. As an example, students were asked in different years for their favorite number from one to 9. The results can be put in a data frame like follows:\n\nfavnum &lt;- data.frame(\n  favorite = 1:9,\n  obs2019  = c(1, 1, 6, 2, 2,  5,  8, 6, 3),\n  obs2020  = c(1, 2, 8, 1, 2,  2, 20, 2, 4),\n  obs2021  = c(2, 6, 8, 1, 6,  4, 13, 2, 4),\n  obs2022  = c(2, 3, 7, 8, 2, 10, 12, 6, 1)\n)",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#direct-input",
    "href": "tutorials/s1-introductory-r-session.html#direct-input",
    "title": "An Introductory R Session",
    "section": "5.1 Direct input",
    "text": "5.1 Direct input\nWe used this method already when creating vectors with the c (combine)-Function:\n\nx &lt;- c(1, 2, 5, 7, 3, 4, 5, 8)\nx\n\nIn the same way it is possible to create other data types like data frames:\n\ndat &lt;- data.frame(f = c(\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"),\n                  x = c(1,   4,   3,   3,   5,   7)\n       )\ndat\n\nor matrices:\n\nA &lt;- matrix(c(1:9), nrow=3)\nA\n\nWe see that a matrix is not much different from a vector, formatted into rows and columns.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#read-data-from-a-text-file",
    "href": "tutorials/s1-introductory-r-session.html#read-data-from-a-text-file",
    "title": "An Introductory R Session",
    "section": "5.2 Read data from a text file",
    "text": "5.2 Read data from a text file\nR has very flexible functions to read data from text files. Let’s for example use a table that contains some data from a lake area in north-eastern Germany (Table 1).\nTable 1: Morphometrical and chemical properties of selected lakes (S=Stechlinsee, NN=Nehmitzsee Nord, NS=Nehmitzsee Süd, BL=Breiter Luzin, SL = Schmaler Luzin, DA = Dagowsee, HS = Feldberger Haussee; z=mean depth (m), t=theoretical retention time (a), P=phosphorus concentration (\\(\\mathrm{\\mu g L^{-1}}\\)), N=nitrogen concentration (\\(\\mathrm{mg L{^-1}}\\)), Chl=chlorophyll concentration (\\(\\mathrm{\\mu g L^{-1}}\\)), PP=annual primary production (\\(\\mathrm{g C m^{-2} a^{-1}}\\)), SD = secchi depth (m)). The data are an adapted and simplified “toy version” taken from Casper (1985) and Koschel & Scheffler (1985).\n\n\n\n\n\nLake\nz\nt\nP\nN\nChl\nPP\nSD\n\n\n\n\nS\n23.7\n40\n2.5\n0.20\n0.7\n95\n8.4\n\n\nNN\n5.9\n10\n2.0\n0.20\n1.1\n140\n7.4\n\n\nNS\n7.1\n10\n2.5\n0.10\n0.9\n145\n6.5\n\n\nBL\n25.2\n17\n50.0\n0.10\n6.1\n210\n3.8\n\n\nSL\n7.8\n2\n30.0\n0.10\n4.7\n200\n3.7\n\n\nDA\n5.0\n4\n100.0\n0.50\n14.9\n250\n1.9\n\n\nHS\n6.3\n4\n1150.0\n0.75\n17.5\n420\n1.6\n\n\n\n\n\nThe data can be downloaded from https://tpetzoldt.github.io/datasets/data/lakes.csv.\n\n5.2.1 Set working directory\nR needs to know where to find the data on your computer. One way is to provide the full path to the data set, e.g. if it is c:/users/&lt;username&gt;/documents, then\n\nlakes &lt;- read.csv(\"c:/users/julia/documents/lakes.csv\")\n\nThis can be cumbersome and error-prone, so the preferred method is to set the working directory of R to the data directory. This can be done in RStudio like follows:\n\nLocate the folder with the data in the “Files” pane\nSelect “More”\nSelect “Set As Working Directory”\n\n Figure2: Setting the working directory in RStudio\nAfter this, data can be retrieved directly from the working directory :\n\nlakes &lt;- read.csv(\"lakes.csv\", header=TRUE)\n\nWe may also consider to create a sub-folder data of the working direktory and put the data in. Then we could use for example:\n\nlakes &lt;- read.csv(\"../data/lakes.csv\", header=TRUE)\n\nNote also that we use always the ordinary slash “/” and not the backslash “\\”, even on Windows.\nIn some countries that have the comma and not the dot as a decimal separator and then for example a semicolon as column separator, additional arguments dec = \",\", sep=\";\" may be required. The details are found on the read.table help page.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#read-data-from-the-internet",
    "href": "tutorials/s1-introductory-r-session.html#read-data-from-the-internet",
    "title": "An Introductory R Session",
    "section": "5.3 Read data from the internet",
    "text": "5.3 Read data from the internet\nIf the data are available on an internet server, it can be read directly from there:\n\nlakes &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/lakes.csv\")\n\nNow, as the data are saved in the data frame lakes it is possible to access them as usual:\n\nlakes\nsummary(lakes)\nboxplot(lakes[-1])\n\nHere summary shows a quick overview and boxplot creates a boxplot for all columns except the first, that contains no numbers.\nNow, we are ready to inspect the content of this new variable lakes. If we use RStudio a View can be invoked by clicking to lakes in the environment window.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#import-dataset-in-rstudio",
    "href": "tutorials/s1-introductory-r-session.html#import-dataset-in-rstudio",
    "title": "An Introductory R Session",
    "section": "5.4 “Import Dataset” in RStudio",
    "text": "5.4 “Import Dataset” in RStudio\nRStudio contains a handy feature that makes importing of data more convenient. Essentially, this “Import Dataset” wizard helps us to construct the correct read.table, read.csv or read_delim function interactively. It is possible to try different options until a satisfying result is obtained. Current versions of RStudio contain several different ways to import data. Here we demonstrate the “Import Dataset From Text (readr)” assistant:\n\nFrom the menu select: File – Import DataSet – From CSV.\nSelect the requested file and select suitable options like the name of the variable the data are to be assigned to, the delimiter character (comma or Tab) and whether the first row of the file contains variable names.\n\n\nImport Dataset From Text (readr) assistant of RStudio.\nHint: In the exmple above, the name of the data frame is identical to the file name, i.e. “lakes”. If we want to name it differently (e.g.: dat), we must not forget to change this setting.\nNote also that the Code Preview contains the commands that the wizard created. If we copy these commands to the script pane, you can re-read the data several times without going back to the menu system:\n\nlibrary(readr)\nlakes &lt;- read_csv(\"D:/DATA/lakes.csv\")\nView(lakes)",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#display-the-content-of-the-data-frame",
    "href": "tutorials/s1-introductory-r-session.html#display-the-content-of-the-data-frame",
    "title": "An Introductory R Session",
    "section": "6.1 Display the content of the data frame",
    "text": "6.1 Display the content of the data frame\nThe easiest way is to just enter the name of the data frame to the R console, e.g.:\n\nlakes\n\nor to click to the name of the data frame in the “Environment” explorer of RStudio. This executes then View(lakes), so that the data are shown.\nFor large tables it is often not very useful to display the full content with View, so it may be better to use the function str (structure) that gives a compact overview over type, size and content of a variable:\n\nstr(mydata)\n\nThe str function is universal and also suitable for complicated object types like lists. Of course, there are many more possibilities for inspecting the content of a variable:\n\nnames(lakes)\nmode(lakes)\nlength(lakes)\n\nand sometimes even:\n\nplot(lakes)",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#access-single-columns-with",
    "href": "tutorials/s1-introductory-r-session.html#access-single-columns-with",
    "title": "An Introductory R Session",
    "section": "6.2 Access single columns with $",
    "text": "6.2 Access single columns with $\nSingle columns of a data frame can be accessed by using indices (with []) similar to a vector or a matrix or by using the column name and the $) operator:\n\nmean(lakes[,2])\nmean(lakes$z)\nmean(lakes[,\"z\"])\nmean(lakes[[\"z\"]])\nplot(lakes$z, lakes$t)\n\nwhere z is the mean depth of the lakes and t the so-called mean residence time.\nWe should also nitice the subtle difference of the output of the [] and the [[]]- version. The difference is as follows: single brackets return a data frame with one column, but double square brackets return the content of the column as a vector without the caption.\nWarning: In some older books, the $-style is sometimes abbreviated using the attach and detach-functions. This “prehistoric relict” is strongly discouraged, as it can lead to data inconsistency and strange errors. If you find it somewhere where it is still used, then it is a good idea to use detach repeatedly until an error message confirms us that there is nothing else that can be detached. Finally: never use attach/detach in a package.\nInstead, it is much better to use another function width, that opens the data frame only temporarily:\n\nwith(lakes, plot(z, t))",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#subsets-and-logical-indices",
    "href": "tutorials/s1-introductory-r-session.html#subsets-and-logical-indices",
    "title": "An Introductory R Session",
    "section": "6.3 Subsets and logical indices",
    "text": "6.3 Subsets and logical indices\nIn the following, we use another data set that we read directly from the internet:\n\nfruits &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/clementines.csv\")\n\nIt contains weight, width and height measurements of two brands of Clementine fruits. After loading the data, we look at it with View(fruits) or the Environment explorer of Rstudio\nA very powerful feature of R is the use of logical vectors as “indices”, with similar results like data base queries. As an example, we can show the weight of all fruits of brand “A” with\n\nfruits[fruits$brand == \"A\", c(\"brand\", \"weight\")]\n\n   brand weight\n6      A     81\n7      A    113\n8      A     94\n9      A     86\n10     A    108\n11     A     91\n12     A     94\n13     A     86\n14     A     91\n15     A     88\n\n\nHere the first indext in the square brackets indicates the subset of rows that we want and the second argument the columns. If we want to see all columns, we leave the argument after the comma empty:\n\nfruits[fruits$brand == \"A\", ]\n\n   id no brand weight width height\n6   6  7     A     81    53     54\n7   7  8     A    113    61     60\n8   8  9     A     94    62     49\n9   9  3     A     86    58     53\n10 10  6     A    108    64     50\n11 11  1     A     91    59     51\n12 12 10     A     94    59     51\n13 13  4     A     86    55     55\n14 14  2     A     91    61     51\n15 15  5     A     88    54     55\n\n\nA logical comparison requires always a double “==”. Logical operations like & (and) and | (or) are also possible. Note that “and” has always precedence before “or”, except this is changed with parenthesis.\nA subset of a data frame can also be extracted with the subset function:\n\nbrand_B &lt;- subset(fruits, brand == \"B\")\nbrand_B\n\nLike in the example before, the condition argument allows also logical expressions with & (and) and | (or).\nWe can also access single elements in matrix-like manner.\nThe element from the 2nd row and the 4th column can be selected with:\n\nfruits[2, 4]\n\nthe complete 5th row with:\n\nfruits[5, ]\n\nand rows 5:10 of the 4th column (weight) with:\n\nfruits[5:10, 4]\n\nAdditional methods for working with matrices, data frames and lists can be found in R textbooks or in the official R documentation.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#pipelines-and-summaries",
    "href": "tutorials/s1-introductory-r-session.html#pipelines-and-summaries",
    "title": "An Introductory R Session",
    "section": "7.1 Pipelines and summaries",
    "text": "7.1 Pipelines and summaries\nThe last examples are intended to demonstrate how powerful a single line can be in R. How to analyse data sets evolved over the history of R, so there is for example a function aggregate to compute statistics (e.g. mean values) depending on given criteria.\nThis works well and is still used, but the modern methods are more compact and easier to understand. Here let’s introduce a modern concept first, that is called “pipelining”. The idea is, that the result of a function is directly pipelined to another function, so instead of writing:\n\nbrand_B &lt;- subset(fruits, brand == \"B\")\ncolumns_B &lt;- brand_B[c(\"weight\", \"width\", \"height\")]\nmeans_B &lt;- colMeans(columns_B)\nmeans_B\n\nwe can directly write:\n\nlibrary(\"dplyr\")\nfruits |&gt; filter(brand == \"B\") |&gt; select(weight, width, height) |&gt; colMeans()\n\nHere we load an add-on package dplyr first, that contains a lot of helpful functions for data management, for example filter that selects rows and select that selects columns. The pipeline operator |&gt; pipes then the data set fruitsto the filter- function and subsequently to the next. The “native pipeline operator” |&gt; was introduced with R 4.1. As an alternative we can also use the %&gt;% pipeline operator, that is loaded by the dplyr package. Its function would be identical in this case.\nTwo other extremely useful dplyr functions are group_by and summary, that allow to calculate arbitrary summary statistics in dependence of grouping variables. If we use the brand for grouping, we can summarize all groups simultaneously:\n\nfruits |&gt; \n  group_by(brand) |&gt;\n  summarise(mean(weight), mean(width), mean(height))\n\nThe summarize line above can still be made better, and there plenty of other stunning possibilities, so we will come back to this later.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#output-of-results",
    "href": "tutorials/s1-introductory-r-session.html#output-of-results",
    "title": "An Introductory R Session",
    "section": "7.2 Output of Results",
    "text": "7.2 Output of Results\nThe most simple method to save outputs from R is to copy it directly from the R console to any other program (e.g. LibreOffice, Microsoft Word or Powerpoint) via the Clipboard. This is convenient, but cannot be automated. Therefore, it is better to use a programmatic approach.\nLet’s use the example before and store the results in a new data frame results:\n\nresults &lt;-\n  fruits |&gt; \n  group_by(brand) |&gt;\n  summarise(mean(weight), mean(width), mean(height))\n\nData frames can be saved as text files with write.table, write.csv or write_csv. Here we use write_csv (with underscore, not dot) from package readr:\n\nlibrary(\"readr\")\nwrite_csv(results, file=\"output-data.csv\")\n\nIn addition to these basic functions R has a wealth of possibilities to save output and data for later use in reports and presentations. All of them are of course documented in the online help, e.g. print, print.table, cat for text files, and pdf, png for figures. The add-on packages xtable contains functions for creating LaTeX or HTML-tables while full HTML output is supported by the R2HTML or knitr packages.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#plots-with-ggplot2",
    "href": "tutorials/s1-introductory-r-session.html#plots-with-ggplot2",
    "title": "An Introductory R Session",
    "section": "7.3 Plots with ggplot2",
    "text": "7.3 Plots with ggplot2\nIn addition to the plot functions we used so far, other plot packages exist, for example lattice or ggplot2. Here a few small examples with the very popular **ggplot2* package:\n\nlibrary(\"ggplot2\")\nfruits |&gt;\n  ggplot(aes(brand, weight)) + geom_boxplot()\n\n\n\n\n\n\n\n\nOr a scatterplot to compare weight and width of the fruits\n\nfruits |&gt; ggplot(aes(weight, width)) + geom_point(aes(color=brand))\n\n\n\n\n\n\n\n\nwhere the brand is indicated as color. Another option could be:\n\nfruits |&gt; ggplot(aes(weight, width)) + \n  geom_point() + \n  geom_smooth(method=\"lm\") + \n  facet_grid(~brand)\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "tutorials/s1-introductory-r-session.html#exercises-1",
    "href": "tutorials/s1-introductory-r-session.html#exercises-1",
    "title": "An Introductory R Session",
    "section": "7.4 Exercises",
    "text": "7.4 Exercises\nR contains lots of data sets for exploring its graphical and statistical functions and that can be activated by using the data function, e.g. data(iris) or data(cars). A description of the data set can be found as usual in the help files, e.g. ?iris, ?cars.\nUse one of these data sets and try\n\nways to access columns, to select rows and to create subsets\nways for summary statistics and visualization with R’s base plot functions and optionally with ggplot.",
    "crumbs": [
      "Tutorials",
      "An Introductory R Session"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#markdown-rmarkdown-and-quarto",
    "href": "slides/x5-quarto-intro.html#markdown-rmarkdown-and-quarto",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Markdown, RMarkdown and Quarto",
    "text": "Markdown, RMarkdown and Quarto\n\nMarkdown .md\n\n” … is a lightweight markup language for creating formatted text using a plain-text editor” (Wikipedia, 2022).\nCan be written with any text editor, less perfect than Latex, but much easier.\nMarkdown supported by many programs and services (e.g. Github, StackOverflow, Matrix, RStudio, …)\n\nRMarkdown .Rmd\n\nis an extension of markdown that can embed R code.\nsuperseeded by Quarto\n\nQuarto .qmd\n\nis an extension of Markdown that can embed R, Python, Julia and Observable code.\nimproved capabilities to create reports, slides, websites, papers, books.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#why-markdown-or-quarto",
    "href": "slides/x5-quarto-intro.html#why-markdown-or-quarto",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Why Markdown or Quarto",
    "text": "Why Markdown or Quarto\n\n\nQuick note taking (documentation of ideas, experiments, SOPs, …)\nDocumentation of statistical analyses (Quarto + R)\nClearly structured documents (outline clearly visible)\nEasy literature referencing\nMuch easier than LaTeX \nWidely used technology, useful for Stackoverflow, Github or Matrix",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#software",
    "href": "slides/x5-quarto-intro.html#software",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Software",
    "text": "Software\n\n\nYou can use any text editor, e.g. Notepad++, your mail client … or even Word\n\n\n\nBetter: use an editor with Markdown support\n\nRStudio\nPanWriter, a basic writing program with an almost empty screen \\(\\rightarrow\\) distraction free writing\nJoplin, a note taking program with cloud connectivity and encryption\nmany online services: Github, Gitlab, StackOverflow, Matrix\nand more",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#many-markdown-programs-available-e.g.-panwriter",
    "href": "slides/x5-quarto-intro.html#many-markdown-programs-available-e.g.-panwriter",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Many Markdown Programs Available, e.g. Panwriter",
    "text": "Many Markdown Programs Available, e.g. Panwriter\n\nPanwriter with live preview",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#lets-use-rstudio.-supports-markdown-and-quarto",
    "href": "slides/x5-quarto-intro.html#lets-use-rstudio.-supports-markdown-and-quarto",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Let’s use RStudio. Supports Markdown and Quarto",
    "text": "Let’s use RStudio. Supports Markdown and Quarto\n\nRStudio",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#example",
    "href": "slides/x5-quarto-intro.html#example",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Example",
    "text": "Example\n\nSection titles are introduced with one or several hash symbols #, ##, praragraphs with empty lines, italic and bold face are indicated with one or two starts before and after a phrase, bullet points with a leading dash - or a star *. Weblinks are automatically activated. Here an example:\n    # First level\n\n    Text can be written with any editor, that can be formatted, e.g. *slanted*, **boldface**,\n    `verbatim text` weblinks: https://tu-dresden.de or bullet points:\n\n    * point 1\n    * point 2\n\n    Section titles start with one or more hash tags\n    \n    \n    ## Second level\n    \n    ### Third level\nThere are of course more formatting options, found in the docs or explained later.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#yaml-header",
    "href": "slides/x5-quarto-intro.html#yaml-header",
    "title": "x05-Creation of Reports with Quarto",
    "section": "YAML Header",
    "text": "YAML Header\n\nQuarto and markdown documents can have a few special lines on top, enclosed within three dashes ---. This so called “YAML header” is used to set text settings and formatting options:\n    ---\n    title: \"Test\"\n    author: \"Who wrote this\"\n    date: '2024-11-12'\n    format: html\n    ---\n    \n    # First Section\n\n    Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy\n    eirmod tempor invidunt ut labore et dolore magna aliquyam erat.\n    \n    At vero eos et accusam et justo duo dolores et ea rebum.\n    \n    # Second Section\n     \n    Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum \n    dolor sit amet. \nYAML: yet another markup language. A list format coming from the python world.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#layout-and-format-conversion",
    "href": "slides/x5-quarto-intro.html#layout-and-format-conversion",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Layout and format conversion",
    "text": "Layout and format conversion\n\nUser can concentrate on writing, formatting is done automatically.\nSeveral tools exist to convert markdown to other document formats.\nOne of the most popular is pandoc. It is built-in in Rstudio.\n\n\n\n\n\n\n\nRendering is done with the pandoc utility to convert Quarto or Markdown text to\nHTML for web pages, pdf for printing, or Word for further editing.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#what-is-pandoc",
    "href": "slides/x5-quarto-intro.html#what-is-pandoc",
    "title": "x05-Creation of Reports with Quarto",
    "section": "What is Pandoc?",
    "text": "What is Pandoc?\n\n\nPandoc is a universal text conversion tool\nIt is said to be the “swiss-army knife” to convert between formats\nOpen Source licensed: GPL 2.0 resp. MIT license\nAvailable from: https://pandoc.org/\n… or embedded in RStudio",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#exercise",
    "href": "slides/x5-quarto-intro.html#exercise",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Exercise",
    "text": "Exercise\n\n\nWrite your first Quarto document in RStudio.\nRender it to HTML and Word\n\nOptional without warranty\n\nCreate PDF output\nNeeds LaTeX type setting system installed\nCan be done with R’s tinytex package:\n\n::: {.cell}\n\n:::\n… or by installing tinytex from the Terminal of RStudio\nquarto install tinytex\nand then include format: pdf in the YAML header\ntitle: \"My document\"\nformat: pdf\nSee more at: https://quarto.org/docs/output-formats/pdf-basics.html",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#section-titles",
    "href": "slides/x5-quarto-intro.html#section-titles",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Section Titles",
    "text": "Section Titles\n\nSection titles can be formatted with hashtags or by underlining:\n# A Section\n\n## A subsection\nor:\nA Section\n=========\n\nA Subsection\n------------\nAutomatic numbering can optionally be enabled in the YAML header:\n---\ntitle: \"My document\"\nformat:\n  html:\n    toc: true\n    number-sections: true\n---",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#weblinks",
    "href": "slides/x5-quarto-intro.html#weblinks",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Weblinks",
    "text": "Weblinks\n\nFormatted weblinks\n[further reading](https://rmarkdown.rstudio.com)\nis then formatted as: further reading\nExample\nThe [*Markdown Wikipedia page*](https://de.wikipedia.org/wiki/Markdown) contains examples.\nThe Markdown Wikipedia page contains examples.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#images",
    "href": "slides/x5-quarto-intro.html#images",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Images",
    "text": "Images\nImages are similar to weblinks, but with a leading !\n![figure title](mushrooms.jpg)\n\nfigure title",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#verbatim-text",
    "href": "slides/x5-quarto-intro.html#verbatim-text",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Verbatim Text",
    "text": "Verbatim Text\nVerbatim text can be created with several methods:\n\nInline: enclose text within single backticks `verbatim text` \\(\\rightarrow\\) verbatim text\nIndentation by 4 spaces\nUse so-called fencing with ``` before and after a tect or code block.\n\n```\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy\neirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam\nvoluptua.\n```\nappears as:\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy\neirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam\nvoluptua.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#tables",
    "href": "slides/x5-quarto-intro.html#tables",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Tables",
    "text": "Tables\nSource code:\n| Right | Left | Default | Center |\n|------:|:-----|---------|:------:|\n|    12 | 34   |   56    |   78   |\n| this  | is   |   a     | table  |\nHTML Output:\n\n\n\nRight\nLeft\nDefault\nCenter\n\n\n\n\n12\n34\n56\n78\n\n\nthis\nis\na\ntable\n\n\n\nPDF output:\n\nStyle similar to a scientific paper.\n\nBigger or more complex tables: create the table in Excel or LibreOffice and add markdown formatting, or use R and kable to create the table from data.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#citations",
    "href": "slides/x5-quarto-intro.html#citations",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Citations",
    "text": "Citations\n\nCreate database file in .bib-format, e.g. references.bib\n\ncan be exported from Zotero\nput bibliography file to the document folder\n\nUse @bib_key-syntax\n\ntextual citation: American Psychological Association (2020b) \\(\\leftarrow\\) @APA2020b\nparenthetical citation: (American Psychological Association, 2020a) \\(\\leftarrow\\) [@APA2020a]\n\n\nDeclare bibliography in YAML header\nbibliography: references.bib\ncsl: apa\n\nand optionally a .csl-style e.g. apa.csl, that is copied to the folder of the document\ncsl styles can be found here: https://citationstyles.org/authors/",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#mathematical-formulae",
    "href": "slides/x5-quarto-intro.html#mathematical-formulae",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Mathematical Formulae",
    "text": "Mathematical Formulae\n\nMarkdown and Quarto support a subset of the LaTeX formula syntax\n\nInline formula\n$s_x = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}{N-1}$ \\(\\quad \\rightarrow \\qquad s_x = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}{N-1}\\)\n #### Display formula\n$$s_x = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}{N-1}$$\n\\[s_x = \\frac{\\sum_{i=1}^{N} (x_i - \\bar{x})^2}{N-1}\\]\nMathematical symbols\n\\(\\rightarrow, \\le, \\approx, \\mu, \\delta, \\int, \\infty, \\mathrm{m^3s^{-1}}\\)\n$\\rightarrow, \\le, \\approx, \\mu, \\delta, \\int, \\infty, \\mathrm{m^3s^{-1}}$",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#more-maths-and-chemistry",
    "href": "slides/x5-quarto-intro.html#more-maths-and-chemistry",
    "title": "x05-Creation of Reports with Quarto",
    "section": "More maths and chemistry",
    "text": "More maths and chemistry\n\n\\[\\begin{align}\n\\frac{dX_1}{dt} &=    k_1 \\cdot X_1 -  k_2 X_1 X_2 \\\\\n\\frac{dX_2}{dt} &=  - k_4 \\cdot X_2 + k_3 X_1 X_2 \\\\\n\\end{align}\\]\n\\begin{align}\n\\frac{dX_1}{dt} &=  k_1 \\cdot X_1 -  k_2 X_1 X_2 \\\\\n\\frac{dX_2}{dt} &=  k_3 X_1 X_2 - k_4 \\cdot X_2 \\\\\n\\end{align}\n\n\\[\\rm 6CO_2 + 6H_2O \\rightarrow C_6H_{12}O_6 + 6O_2\n\\quad \\Delta H^0 = +2870 kJ mol^{-1}\\]\n$$\\rm 6CO_2 + 6H_2O \\rightarrow C_6H_{12}O_6 + 6O_2 \\quad \\Delta H^0 = +2870 kJ mol^{-1}$$",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#embedding-of-r-in-quarto-documents",
    "href": "slides/x5-quarto-intro.html#embedding-of-r-in-quarto-documents",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Embedding of R in Quarto Documents",
    "text": "Embedding of R in Quarto Documents\n\nCreate a Quarto template from the File menu in RStudio. Then make your changes and click the Render button. Then, a document will be generated that includes both content as well as the output of any embedded R code chunks.\n\nThen embed your own R code chunks like this:\n```{r iris_summary}\nsummary(iris)\n```\n To show both, the code and the output\n\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#tables-from-r",
    "href": "slides/x5-quarto-intro.html#tables-from-r",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Tables from R",
    "text": "Tables from R\n\nIf you want to include real tables, you can create the table in R and then format it with knitr::kable\n\n```{r iris_table}\nknitr::kable(iris[1:4, ])\n```\n\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n\n\n\nThe kable function has several functions for configuring table layout, see kable help page for details.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#include-plots",
    "href": "slides/x5-quarto-intro.html#include-plots",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Include Plots",
    "text": "Include Plots\nYou can also embed plots, for example:\n```{r iris_sepal}\nplot(Sepal.Width ~ Sepal.Length, data=iris, pch=16, col=Species)\n```",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#suppress-code",
    "href": "slides/x5-quarto-intro.html#suppress-code",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Suppress Code",
    "text": "Suppress Code\nThe code chunks can be modified with additional options. In the following example the figure size is adjusted and an option echo = FALSE was added to prevent printing of the R code that generated the plot.\n```{r iris_sepal3, fig.width=3, fig.height=3, echo=FALSE}\nplot(Sepal.Width ~ Sepal.Length, data=iris, pch=16, col=Species)\n```\nShows the plot without the code:",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#flowcharts-and-graphs",
    "href": "slides/x5-quarto-intro.html#flowcharts-and-graphs",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Flowcharts and graphs",
    "text": "Flowcharts and graphs\n\n… can be created with the DiagrammeR package that supports the graphviz language.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#r-code-of-the-flowchart",
    "href": "slides/x5-quarto-intro.html#r-code-of-the-flowchart",
    "title": "x05-Creation of Reports with Quarto",
    "section": "R code of the flowchart",
    "text": "R code of the flowchart\n\nlibrary(\"DiagrammeR\")\ngrViz(\"digraph pandoc {\n         graph [rankdir = LR]\n           node [shape = 'box']\n                Markdown HTML PDF Word\n           node [shape = cds]\n             pandoc\n           node [shape = oval]\n             figs bib\n           edge [penwidth=2]\n             Markdown -&gt; pandoc\n             {figs bib} -&gt; {pandoc}\n             pandoc -&gt; {HTML PDF Word}\n}\")\nMore at: https://graphviz.org/",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#further-reading",
    "href": "slides/x5-quarto-intro.html#further-reading",
    "title": "x05-Creation of Reports with Quarto",
    "section": "Further reading",
    "text": "Further reading\n\n\\(\\rightarrow\\) Quarto cheat sheetFor more details, see https://quarto.org and R Core Team (2024)“, RStudio Team (2021), Xie (2015) and The Quarto Dev Team (2024).\nThesis templates for Hydrobiology students at TU Dresden: https://github.com/tpetzoldt/hyb-tud-thesis-starterkit in Word, Latex and Quarto format. It may also be useful for you.",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x5-quarto-intro.html#references",
    "href": "slides/x5-quarto-intro.html#references",
    "title": "x05-Creation of Reports with Quarto",
    "section": "References",
    "text": "References\n\n\n\n\nAmerican Psychological Association. (2020a). Concise guide to APA style: The official APA style guide for students (7th ed.).\n\n\nAmerican Psychological Association. (2020b). Publication manual of the American Psychological Association: The official guide to APA style. (7th ed.).\n\n\nR Core Team. (2024). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/\n\n\nRStudio Team. (2021). RStudio: Integrated development environment for R. RStudio, PBC. http://www.rstudio.com/\n\n\nThe Quarto Dev Team. (2024). Quarto. An open-source scientific and technical publishing system. https://quarto.org\n\n\nWikipedia. (2022). Markdown. Wikipedia. https://en.wikipedia.org/w/index.php?title=Markdown&oldid=1079645429\n\n\nXie, Y. (2015). Dynamic documents with R and knitr (2nd ed.). Chapman; Hall/CRC. https://yihui.org/knitr/",
    "crumbs": [
      "R Topics",
      "x05-Creation of Reports with Quarto"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#functions-bring-life-to-the-r-language",
    "href": "slides/x3-r-functions.html#functions-bring-life-to-the-r-language",
    "title": "x3-Functions Everywhere",
    "section": "Functions bring life to the R language",
    "text": "Functions bring life to the R language\n\nsin(x), log(x), plot(x, y), summary(x), anova(lm.object), mean(x), monod(S, vmax, ks), simulate_phytoplankton(N, P, T, Zoo, ...)\n\nFunctions in R\n\nhave a name, followed by parenthesis ()\ncan have 1, 2 or more arguments (or no argument)\nusually return something (an object)\ncan have side-effects (e.g. plotting)",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#what-are-functions",
    "href": "slides/x3-r-functions.html#what-are-functions",
    "title": "x3-Functions Everywhere",
    "section": "What are functions",
    "text": "What are functions\n\nParentheses and arguments\n\nall functions are followed by parentheses and arguments\nfunctions: log(x) par()\npar &lt;- c(a=5, b=3)\n\n\\(\\rightarrow\\) here, par is a variable, c() a function\nReturn value and/or side effect\n\nsin(x), log(x), mean(x) are functions with return value\nprint(x), plot(x, y) are functions with side effect\nhist(x) is a function with both, side effect and return value\n\nPredefined and user-defined functions\n\npredefined: available in R\nuser defined: users become programmers",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#arguments-of-functions",
    "href": "slides/x3-r-functions.html#arguments-of-functions",
    "title": "x3-Functions Everywhere",
    "section": "Arguments of functions",
    "text": "Arguments of functions\nUsage\n\ndnorm(x, mean = 0, sd = 1, log = FALSE)\npnorm(q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nqnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\nrnorm(n, mean = 0, sd = 1)\n\n\n\n\n\n\n\n\nx, q\nvector of quantiles.\n\n\np\nvector of probabilities.\n\n\nn\nnumber of observations. If length(n) &gt; 1, the length is taken to be …\n\n\nlog.p\nif TRUE, probabilities p are given as log(p).\n\n\nlower.tail\nif TRUE (default), …\n\n\n\nArguments\n\nrequired arguments: have no default\noptional arguments: have default values\nnamed arguments: argument mathing with = allows to specify arguments in arbitrary order\nargument order: arguments can occur without names when in defined order\n“…”: dots-arguments are passed down to other called functions",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#examples",
    "href": "slides/x3-r-functions.html#examples",
    "title": "x3-Functions Everywhere",
    "section": "Examples",
    "text": "Examples\n\n\n\nrnorm(10)                       # x given, other arguments = defaults\nrnorm(10, 0, 1)                 # order matters\nrnorm(n = 10, mean = 0, sd = 1) # use argument names\nrnorm(10, sd = 1, mean = 0)     # named arguments in arbitrary order\nrnorm(10, m = 5, s = 1)         # abbreviated arguments: = bad style\nargs(rnorm)                     # all arguments from rnorm",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#the-ellipsis-argument",
    "href": "slides/x3-r-functions.html#the-ellipsis-argument",
    "title": "x3-Functions Everywhere",
    "section": "The ellipsis argument",
    "text": "The ellipsis argument\n\nplot(x, y, ...)\n\n\nSome functions have a … argument, called “ellipsis”.\nThis means that additional arguments are passed to other functions.\nMakes R flexible and extensible, but is sometimes tricky.\n\n\npar(mfrow=c(1, 3))\nx &lt;- 1:10; y &lt;- rnorm(10)\nplot(x, y)\nplot(x, y, type = \"h\")\nplot(x, y, type = \"s\", col=\"red\")",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#plot.default",
    "href": "slides/x3-r-functions.html#plot.default",
    "title": "x3-Functions Everywhere",
    "section": "plot.default",
    "text": "plot.default\n\nplot(x, y = NULL, type = \"p\",  xlim = NULL, ylim = NULL,\n     log = \"\", main = NULL, sub = NULL, xlab = NULL, ylab = NULL,\n     ann = par(\"ann\"), axes = TRUE, frame.plot = axes,\n     panel.first = NULL, panel.last = NULL, asp = NA, ...)\n\nObject orientation\n\nplot is a generic function\nworks automagic differently for different classes of objects\nplot.default is the basic function\n... see ?par for additional graphical parameters, e.g.:\n\n\n\n\ncol\ncolor\n\n\nbg\nbackground color for two-color symbols\n\n\npch\nsymbol (plotting character)\n\n\ncex\nsize of symbol (character extension)\n\n\nlty\nline type\n\n\nlwd\nline width",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#a-user-defined-monod-function",
    "href": "slides/x3-r-functions.html#a-user-defined-monod-function",
    "title": "x3-Functions Everywhere",
    "section": "A user-defined Monod function",
    "text": "A user-defined Monod function\n\n\ndescribes substrate dependence of biochemical turnover\nwidely used in biochemistry and in models\ne.g. organic matter turnover in wastewater treatment\n\n\\[\nv = \\frac{v_{max} \\cdot S}{k_S + S}\n\\]\n\npar(mar=c(4,4,1,1))\npar(mfrow=c(3, 1))\nmonod &lt;- function(S, vmax, ks) {\n  vmax * S / (ks + S)\n}\n\n\nS &lt;- 1:10\nP &lt;- seq(0, 20, 0.1)\nkP &lt;- 5; mumax &lt;- 1.2;\n\n## different ways to call the function\nplot(S, monod(S, 2, 2))                # simple call\nplot(P, monod(S=P, vmax=mumax, ks=kP)) # named arguments\nplot(P, monod(P, mumax, kP))           # argument position\n\n\nnames of caller and function can be different",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#seasonal-light-intensity-in-dresden",
    "href": "slides/x3-r-functions.html#seasonal-light-intensity-in-dresden",
    "title": "x3-Functions Everywhere",
    "section": "Seasonal Light Intensity in Dresden",
    "text": "Seasonal Light Intensity in Dresden\n\n\\[\nI_t = 997 - 816 \\cos(2 \\pi t / 365) + 126 \\sin(2 \\pi t / 365)\n\\]\n\nFunctions as a knowledge base\n\nput knowledge in function and use it\nforget what is inside\n\n\n\nrad &lt;- function(t) {\n  ## fill equation in\n}\n\nt &lt;- 1:365\nplot(t, rad(t), type = \"l\")",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#oxygen-saturation-in-fresh-and-sea-water",
    "href": "slides/x3-r-functions.html#oxygen-saturation-in-fresh-and-sea-water",
    "title": "x3-Functions Everywhere",
    "section": "Oxygen saturation in fresh and sea water",
    "text": "Oxygen saturation in fresh and sea water\n\\[\nc_{O_2, 100\\%} = ... ?\n\\]\n\no2sat &lt;- function(t) {\n  K &lt;- t + 273.15 # Celsius to Kelvin\n  exp(-139.34411 + (157570.1/K) - (66423080/K^2) +\n   (1.2438e+10/K^3) - (862194900000/K^4))\n}\n\no2sat(20)\n\n[1] 9.092426\n\n\n A more precise formula is found in package marelac\n\nlibrary(marelac)\ngas_O2sat(t = 20, S = 0, method = \"APHA\")\n\n[1] 9.092426\n\n\nconsult ?gas_O2sat for citations.",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#local-and-global-variables",
    "href": "slides/x3-r-functions.html#local-and-global-variables",
    "title": "x3-Functions Everywhere",
    "section": "Local and global variables",
    "text": "Local and global variables\n\nVariables in a function are local:\n\nnot visible from outside.\nno collisions with existing variables in the calling environment\n\nLexical Scoping\n\nfunctions can see variables of the calling function\nuseful for interactive work\ndangerous for (exported) functions in packages\nexcept in special cases, e.g. for functions within functions",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#local-and-global-variables-ii",
    "href": "slides/x3-r-functions.html#local-and-global-variables-ii",
    "title": "x3-Functions Everywhere",
    "section": "Local and global variables II",
    "text": "Local and global variables II\n\n\nrm(list = ls()) # remove all objects\no2sat &lt;- function(t) {\n  K &lt;- t + 273.15 # Celsius to Kelvin\n  exp(-139.34411 + (157570.1/K) - (66423080/K^2) +\n   (1.2438e+10/K^3) - (862194900000/K^4))\n}\n\no2sat(20)\nK\n\nK &lt;- 0\no2sat(20)\n\nNow outcomment:\n\n# K &lt;- t + 273.15\n\nand try again.",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#logistic-growth",
    "href": "slides/x3-r-functions.html#logistic-growth",
    "title": "x3-Functions Everywhere",
    "section": "Logistic growth",
    "text": "Logistic growth\n\nThe logistic growth function describes saturated growth of a population abundance \\(N_t\\), dependent of an initial value \\(N_0\\), growth rate \\(r\\) and carrying capacity \\(K\\).\n\\[\nN_t = \\frac{K N_0 e^{rt}}{K + N_0 (e^{rt}-1)}\n\\]\n\nlogistic &lt;- function(t, r, K, N0) {\n  K*N0*exp(r*t)/(K+N0*(exp(r*t)-1))\n}\n\n\nmu &lt;- 0.1; K = 10; N0 = 0.1\ntimes &lt;- 1:100",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#functional-response-types-in-ecology",
    "href": "slides/x3-r-functions.html#functional-response-types-in-ecology",
    "title": "x3-Functions Everywhere",
    "section": "Functional response types in Ecology",
    "text": "Functional response types in Ecology\n\nHolling type I \\(P = \\min(k \\cdot N, P_{max})\\)\nHolling type II \\(P = \\frac{\\alpha N}{1 + \\alpha H N}\\)\nHolling type III \\(P = \\frac{\\alpha N^b}{1 + \\alpha H N^b}\\)\n\nwith\n\n\n\n\\(P\\)\npredation rate\n\n\n\\(N\\)\nabundance of prey\n\n\n\\(P_{max}\\)\nmaximum predation rate\n\n\n\\(k\\)\na constant\n\n\n\\(\\alpha\\)\nattack rate\n\n\n\\(H\\)\nhandling time\n\n\n\\(b\\)\nexponent \\(&gt;1\\)\n\n\n\n\nWrite a function for each functional reponse type and plot it.\nWrite a universal function for all types.",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x3-r-functions.html#further-reading",
    "href": "slides/x3-r-functions.html#further-reading",
    "title": "x3-Functions Everywhere",
    "section": "Further Reading",
    "text": "Further Reading\n\nMore presentations\n\nR Basics\nGraphics in R\n\nManuals\nMore details in the official R manuals, especially in An Introduction to R\nVideos\nMany videos can be found on Youtube, at the Posit webpage and somewhere else.",
    "crumbs": [
      "R Topics",
      "x3-Functions Everywhere"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#r-is-more-convenient-with-rstudio",
    "href": "slides/x1-r-basics.html#r-is-more-convenient-with-rstudio",
    "title": "x1-R Basics",
    "section": "R is more convenient with RStudio",
    "text": "R is more convenient with RStudio",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#r-and-rstudio",
    "href": "slides/x1-r-basics.html#r-and-rstudio",
    "title": "x1-R Basics",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nEngine and Control\n\nR The main engine for computations and graphics.\nRstudio the IDE (integrated development environment) that embeds and controls R and provides additional facilities.\nR can also be used without RStudio.\n\n\nCitation\nCite R and optionally RStudio.\nR Core Team (2022). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. https://www.R-project.org/\nRStudio Team (2022). RStudio: Integrated Development Environment for R. RStudio, PBC, Boston, MA URL http://www.rstudio.com/",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#expressions-and-assignments",
    "href": "slides/x1-r-basics.html#expressions-and-assignments",
    "title": "x1-R Basics",
    "section": "Expressions and Assignments",
    "text": "Expressions and Assignments\n\n\nExpression\n\n\n1 - pi + exp(1.7)\n\n[1] 3.332355\n\n\n\n\nresult is printed to the screen\nthe [1] indicates that the value shown at the beginning of the line is the first (and here the only) element\n\n\nAssignment\n\n\na &lt;- 1 - pi + exp(1.7)\n\n\n\nThe expression on the left hand side is assigned to the variable on the right.\nThe arrow is spelled as “a gets …”\nTo avoid confusion: use &lt;- for assignment and let = for parameter matching",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#constants-variables-and-assignments",
    "href": "slides/x1-r-basics.html#constants-variables-and-assignments",
    "title": "x1-R Basics",
    "section": "Constants, variables and assignments",
    "text": "Constants, variables and assignments\nAssignment of constants and variables to a variable\n\n\nx &lt;- 1.3      # numeric constant\ny &lt;- \"hello\"  # character constant\na &lt;- x        # a and x both variables\n\nAssignment in opposite direction (rarely used)\n\nx -&gt; b\n\nMultiple assignment\n\nx &lt;- a &lt;- b\n\n\nDo not use the following constructs\n\n# Equal sign has two meanings: parameter matching and assignment\n# - Don't use it for assignment!\nx = a\n\n# Super assignment, useful for programmers in special cases\nx &lt;&lt;- 2",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#objects-constants-variables",
    "href": "slides/x1-r-basics.html#objects-constants-variables",
    "title": "x1-R Basics",
    "section": "Objects, constants, variables",
    "text": "Objects, constants, variables\n\nEverything stored in R’s memory is an object:\n\ncan be simple or complex\ncan be constants or variables\nconstants: 1, 123, 5.6, 5e7, “hello”\nvariables: can change their value, are referenced by variable names\n\n\n\nx &lt;- 2.0 # x is a variable, 2.0 is a constant\n\nA syntactically valid variable name consists of:\n\nletters, numbers, underline (_), dot (.)\nstarts with a letter or the dot\nif starting with the dot, not followed by a number\n\nSpecial characters, except _ and . (underscore and dot) are not allowed.\nInternational characters (e.g German umlauts ä, ö, ü, …) are possible, but not recommended.",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#allowed-and-disallowed-identifiers",
    "href": "slides/x1-r-basics.html#allowed-and-disallowed-identifiers",
    "title": "x1-R Basics",
    "section": "Allowed and disallowed identifiers",
    "text": "Allowed and disallowed identifiers\n\ncorrect:\n\nx, y, X, x1, i, j, k\nvalue, test, myVariableName, do_something\n`.hidden, .x1``\n\nforbidden:\n\n1x, .1x (starts with a number)\n!, @, \\$, #, space, comma, semicolon and other special characters\n\nreserved words cannot be used as variable names:\n\nif, else, repeat, while, function, for, in, next, break\nTRUE, FALSE, NULL, Inf, NaN, NA, NA_integer_, NA_real_, NA_complex_, NA_character\\_\n..., ..1, ..2\n\nNote: R is case sensitive, x and X, value and Value are different.",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#operators",
    "href": "slides/x1-r-basics.html#operators",
    "title": "x1-R Basics",
    "section": "Operators",
    "text": "Operators\n\n\n\n\n\n\noperator\nsymbol\n\n\n\n\nAddition\n+\n\n\nSubtraction\n-\n\n\nNegation\n-\n\n\nMultiplication\n*\n\n\nDivision\n/\n\n\nModulo\n%%\n\n\nInteger Divison\n%/%\n\n\nPower\n^\n\n\nMatrix product\n%*%\n\n\nOuter product\n%o%\n\n\n\n\n\n\n\n\n\n\n\noperator\nsymbol\n\n\n\n\nNegation\n!\n\n\nAnd\n&\n\n\nOr\n|\n\n\nEqual\n==\n\n\nUnequal\n!=\n\n\nLess than\n&lt;\n\n\nGreater than\n&gt;\n\n\nLess or equal\n&lt;=\n\n\nGreater or equal\n&gt;=\n\n\nAssignment\n&lt;-\n\n\nElement of a list\n$\n\n\nPipeline\n|&gt;\n\n\n\n\n\n… and more",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#functions",
    "href": "slides/x1-r-basics.html#functions",
    "title": "x1-R Basics",
    "section": "Functions",
    "text": "Functions\nPre-defined functions:\n\nwith return value: sin(x), log(x)\nwith side effect: plot(x), print(x)\nwith both return value and side efect: hist(x)\n\nArguments: mandatory or optional, un-named or named\n\nplot(1:4, c(3, 4, 3, 6), type = \"l\", col = \"red\")\nif named arguments are used (with the “=” sign), argument order does not matter\n\nUser-defined functions:\n\ncan be used to extend R\nwill be discussed later\n\n\\(\\rightarrow\\) Functions have always a name followed by arguments in round parentheses.",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#parentheses",
    "href": "slides/x1-r-basics.html#parentheses",
    "title": "x1-R Basics",
    "section": "Parentheses",
    "text": "Parentheses",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#vectors-matrices-and-arrays",
    "href": "slides/x1-r-basics.html#vectors-matrices-and-arrays",
    "title": "x1-R Basics",
    "section": "Vectors, matrices and arrays",
    "text": "Vectors, matrices and arrays\n\nvectors = 1D, matrices = 2D and arrays = n-dimensional\ndata are arranged into rows, columns, layers, …\ndata filled in column-wise, can be changed\ncreate vector\n\n\nx &lt;- 1:20\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\n\n\nconvert it to matrix\n\n\ny &lt;- matrix(x, nrow = 5, ncol = 4)\ny\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    6   11   16\n[2,]    2    7   12   17\n[3,]    3    8   13   18\n[4,]    4    9   14   19\n[5,]    5   10   15   20\n\n\n\nback-convert (flatten) to vector\n\n\nas.vector(y) # flattens the matrix to a vector\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#vectors-matrices-and-arrays-ii",
    "href": "slides/x1-r-basics.html#vectors-matrices-and-arrays-ii",
    "title": "x1-R Basics",
    "section": "Vectors, matrices and arrays II",
    "text": "Vectors, matrices and arrays II\n\nrecycling rule if the number of elements is too small\n\n\nx &lt;- matrix(0, nrow=5, ncol=4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    0    0    0    0\n[2,]    0    0    0    0\n[3,]    0    0    0    0\n[4,]    0    0    0    0\n[5,]    0    0    0    0\n\nx &lt;- matrix(1:4, nrow=5, ncol=4)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    2    3    4    1\n[3,]    3    4    1    2\n[4,]    4    1    2    3\n[5,]    1    2    3    4",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#transpose-rows-and-columns",
    "href": "slides/x1-r-basics.html#transpose-rows-and-columns",
    "title": "x1-R Basics",
    "section": "Transpose rows and columns",
    "text": "Transpose rows and columns\n\nrow-wise creation of a matrix\n\n\nx &lt;- matrix(1:20, nrow = 5, ncol = 4, byrow = TRUE)\nx\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    4\n[2,]    5    6    7    8\n[3,]    9   10   11   12\n[4,]   13   14   15   16\n[5,]   17   18   19   20\n\n\n\ntranspose of a matrix\n\n\nx &lt;- t(x)\nx\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#access-array-elements",
    "href": "slides/x1-r-basics.html#access-array-elements",
    "title": "x1-R Basics",
    "section": "Access array elements",
    "text": "Access array elements\n\n\na three dimensional array\nrow, column, layer/page\nsub-matrices (slices)\n\n\nx &lt;- array(1:24, dim=c(3, 4, 2))\nx\n\n, , 1\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n, , 2\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\n\n\n\n\n\nelements of a matrix or array\n\n\nx[1, 3, 1] # single element\n\n[1] 7\n\nx[ , 3, 1] # 3rd column of 1st layer\n\n[1] 7 8 9\n\nx[ ,  , 2] # second layer\n\n     [,1] [,2] [,3] [,4]\n[1,]   13   16   19   22\n[2,]   14   17   20   23\n[3,]   15   18   21   24\n\nx[1,  ,  ] # another slice\n\n     [,1] [,2]\n[1,]    1   13\n[2,]    4   16\n[3,]    7   19\n[4,]   10   22",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#reordering-and-indirect-indexing",
    "href": "slides/x1-r-basics.html#reordering-and-indirect-indexing",
    "title": "x1-R Basics",
    "section": "Reordering and indirect indexing",
    "text": "Reordering and indirect indexing\n\nOriginal matrix\n\n(x &lt;- matrix(1:20, nrow = 4))\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    2    6   10   14   18\n[3,]    3    7   11   15   19\n[4,]    4    8   12   16   20\n\n\nInverted row order\n\nx[4:1, ]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    4    8   12   16   20\n[2,]    3    7   11   15   19\n[3,]    2    6   10   14   18\n[4,]    1    5    9   13   17\n\n\n\nIndirect index\n\nx[c(1, 2, 1, 2), c(1, 3, 2, 5, 4)]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    9    5   17   13\n[2,]    2   10    6   18   14\n[3,]    1    9    5   17   13\n[4,]    2   10    6   18   14\n\n\nLogical selection\n\nx[c(FALSE, TRUE, FALSE, TRUE), ]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    2    6   10   14   18\n[2,]    4    8   12   16   20\n\n\nSurprise?\n\nx[c(0, 1, 0, 1), ]\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    5    9   13   17\n[2,]    1    5    9   13   17",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#matrix-multiplication-explained",
    "href": "slides/x1-r-basics.html#matrix-multiplication-explained",
    "title": "x1-R Basics",
    "section": "Matrix multiplication explained",
    "text": "Matrix multiplication explained\n\n\nTwo matrices: A and B\n\nA &lt;- matrix(c(1, 2, 3,\n              5, 4, 2), \n            nrow = 2, byrow = TRUE)\n\nB &lt;- matrix(c(1, 2, 3, 4,\n              6, 8, 4, 2,\n              3, 1, 3, 2), \n            nrow = 3, byrow = TRUE)\n\nMultiplication: \\(A \\cdot B\\)\n\nA %*% B\n\n     [,1] [,2] [,3] [,4]\n[1,]   22   21   20   14\n[2,]   35   44   37   32",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#transpose-and-inverse",
    "href": "slides/x1-r-basics.html#transpose-and-inverse",
    "title": "x1-R Basics",
    "section": "Transpose and inverse",
    "text": "Transpose and inverse\nMatrix\n\nX &lt;- matrix(c(1, 2, 3, \n              4, 3, 2, \n              5, 4, 6),\n            nrow = 3)\nX\n\n     [,1] [,2] [,3]\n[1,]    1    4    5\n[2,]    2    3    4\n[3,]    3    2    6\n\n\nTranspose\n\nt(X)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    3    2\n[3,]    5    4    6\n\n\nInverse (\\(X^{-1}\\))\n\nsolve(X)\n\n\n\n        [,1]    [,2]    [,3]\n[1,] -0.6667  0.9333 -0.0667\n[2,]  0.0000  0.6000 -0.4000\n[3,]  0.3333 -0.6667  0.3333",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#multiplication-of-a-matrix-with-its-inverse",
    "href": "slides/x1-r-basics.html#multiplication-of-a-matrix-with-its-inverse",
    "title": "x1-R Basics",
    "section": "Multiplication of a matrix with its inverse",
    "text": "Multiplication of a matrix with its inverse\n\n\n\\[X \\cdot X^{-1} = I\\]\n\nX %*% solve(X)\n\n     [,1] [,2] [,3]\n[1,]    1    0    0\n[2,]    0    1    0\n[3,]    0    0    1\n\n\n\n\n\\(I\\): identity matrix",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#linear-system-of-equations",
    "href": "slides/x1-r-basics.html#linear-system-of-equations",
    "title": "x1-R Basics",
    "section": "Linear system of equations",
    "text": "Linear system of equations\n\n\\[\\begin{align}\n3x && +  && 2y   && -  && z  && =  && 1 \\\\\n2x && -  && 2y   && +  && 4z && =  && -2 \\\\\n-x && +  && 1/2y && -  && z  && =  && 0\n\\end{align}\\]\n\n\nA &lt;- matrix(c(3,  2,   -1,\n             2,  -2,    4,\n            -1,   0.5, -1), nrow=3, byrow=TRUE)\nb &lt;- c(1, -2, 0)\n\n\n\\[\\begin{align}\nAx &= b\\\\\nx  &= A^{-1}b\n\\end{align}\\]\n\n\nsolve(A) %*% b\n\n     [,1]\n[1,]    1\n[2,]   -2\n[3,]   -2",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#data-frames",
    "href": "slides/x1-r-basics.html#data-frames",
    "title": "x1-R Basics",
    "section": "Data frames",
    "text": "Data frames\n\nrepresent tabular data\nsimilar to matrices, but different types of data in columns possible\ntypically imported from a file with read.table or read.csv\n\n\n\ncities &lt;- read.csv(\"cities.csv\")\ncities\n\n\n\n\n               Name    Country Population Latitude Longitude IsCapital\n1  Fürstenfeldbruck    Germany      34033  48.1690   11.2340     FALSE\n2             Dhaka Bangladesh   13000000  23.7500   90.3700      TRUE\n3       Ulaanbaatar   Mongolia    3010000  47.9170  106.8830      TRUE\n4           Shantou      China    5320000  23.3500  116.6700     FALSE\n5           Kampala     Uganda    1659000   0.3310   32.5830      TRUE\n6           Cottbus    Germany     100000  51.7650   14.3280     FALSE\n7           Nairobi      Kenya    3100000   1.2833   36.8167      TRUE\n8             Hanoi    Vietnam    1452055  21.0300  105.8400      TRUE\n9          Bacgiang    Vietnam      53739  21.2800  106.1900     FALSE\n10       Addis Abba   Ethiopia    2823167   9.0300   38.7400      TRUE\n11        Hyderabad      India    3632094  17.4000   78.4800     FALSE\n\n\n\\(\\rightarrow\\) download data set",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#what-is-a-csv-file",
    "href": "slides/x1-r-basics.html#what-is-a-csv-file",
    "title": "x1-R Basics",
    "section": "What is a CSV file?",
    "text": "What is a CSV file?\n\ncomma separated values.\nfirst line contains column names\ndecimal is dec=\".\", column separator is sep=\",\"\n\nExample CSV file (Data from Wikipedia, 2023)\n\nName,Country,Population,Latitude,Longitude\nDhaka,Bangladesh,10278882,23.75,90.37\nUlaanbaatar,Mongolia,1672627,47.917,106.883\nShantou,China,5502031,23.35,116.67\nKampala,Uganda,1680600,0.331,32.583\nBerlin,Germany,3850809,52.52,13.405\nNairobi,Kenya,4672000,1.2833,36.8167\nHanoi,Vietnam,8435700,21.03,105.84\nAddis Abba,Ethiopia,3945000,9.03,38.74\nHyderabad,India,9482000,17.4,78.48\n\nHints\n\nsome countries use dec = \",\" and sep = \";\"\nExcel may export mixed style with dec = \".\" and sep = \";\"\ncomments above the header line can be skipped",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#different-read-funktions",
    "href": "slides/x1-r-basics.html#different-read-funktions",
    "title": "x1-R Basics",
    "section": "Different read-Funktions",
    "text": "Different read-Funktions\n\nR contains several read-functions for different file types.\nSome are more flexible, some more automatic, some faster, some more robust …\n\nTo avoid confusion, we use only the following:\nBase R\n\nread.table(): this is the most flexible standard function, see help file for details\nread.csv(): default options for standard csv files (with dec=\".\" and sep=,)\n\nTidyverse readr-package\n\nread_delim(): similar to read.table() but more modern, automatic and faster\nread_csv(): similar to read.csv() with more automatism, e.g. date detection",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#the-most-versatile-read.table",
    "href": "slides/x1-r-basics.html#the-most-versatile-read.table",
    "title": "x1-R Basics",
    "section": "The most versatile: read.table()",
    "text": "The most versatile: read.table()\n\nread.table(file, header = FALSE, sep = \"\", quote = \"\\\"'\",\n           dec = \".\", numerals = c(\"allow.loss\", \"warn.loss\", \"no.loss\"),\n           row.names, col.names, as.is = !stringsAsFactors, tryLogical = TRUE,\n           na.strings = \"NA\", colClasses = NA, nrows = -1,\n           skip = 0, check.names = TRUE, fill = !blank.lines.skip,\n           strip.white = FALSE, blank.lines.skip = TRUE,\n           comment.char = \"#\",\n           allowEscapes = FALSE, flush = FALSE,\n           stringsAsFactors = FALSE,\n           fileEncoding = \"\", encoding = \"unknown\", text, skipNul = FALSE)\n\nExamples\n\nread.table(\"cities.csv\", sep = \",\",  dec = \".\")  # same as read.csv\nread.table(\"cities.txt\", sep = \"\\t\", dec = \".\")  # tab delimited\nread.table(\"cities.csv\", sep = \";\",  dec = \",\")  # German csv\n\nread.table(\"cities.csv\", sep = \",\", dec = \".\", skip = 5) # skip first 5 lines",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#recommendation",
    "href": "slides/x1-r-basics.html#recommendation",
    "title": "x1-R Basics",
    "section": "Recommendation",
    "text": "Recommendation\n\nMost of our course examples are plain CSV files, so we can use read.csv() or read_csv().\n\n\nlibrary(\"readr\")\ncities &lt;- read_csv(\"cities.csv\")\ncities\n\n\n\n\n# A tibble: 11 × 6\n   Name             Country    Population Latitude Longitude IsCapital\n   &lt;chr&gt;            &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;    \n 1 Fürstenfeldbruck Germany         34033   48.2        11.2 FALSE    \n 2 Dhaka            Bangladesh   13000000   23.8        90.4 TRUE     \n 3 Ulaanbaatar      Mongolia      3010000   47.9       107.  TRUE     \n 4 Shantou          China         5320000   23.4       117.  FALSE    \n 5 Kampala          Uganda        1659000    0.331      32.6 TRUE     \n 6 Cottbus          Germany        100000   51.8        14.3 FALSE    \n 7 Nairobi          Kenya         3100000    1.28       36.8 TRUE     \n 8 Hanoi            Vietnam       1452055   21.0       106.  TRUE     \n 9 Bacgiang         Vietnam         53739   21.3       106.  FALSE    \n10 Addis Abba       Ethiopia      2823167    9.03       38.7 TRUE     \n11 Hyderabad        India         3632094   17.4        78.5 FALSE",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#data-import-assistant-of-rstudio",
    "href": "slides/x1-r-basics.html#data-import-assistant-of-rstudio",
    "title": "x1-R Basics",
    "section": "Data import assistant of RStudio",
    "text": "Data import assistant of RStudio\n\nFile –&gt; Import Dataset\nSeveral options are available:\n\n“From text (base)” uses the classical R functions\n“From text (readr)” is more modern and uses an add-on package\n“From Excel” can read Excel files if (and only if) they have a clear tabular structure",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#from-text-base",
    "href": "slides/x1-r-basics.html#from-text-base",
    "title": "x1-R Basics",
    "section": "From text (base)",
    "text": "From text (base)",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#from-text-readr",
    "href": "slides/x1-r-basics.html#from-text-readr",
    "title": "x1-R Basics",
    "section": "From text (readr)",
    "text": "From text (readr)",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#save-data-to-excel-compatible-format",
    "href": "slides/x1-r-basics.html#save-data-to-excel-compatible-format",
    "title": "x1-R Basics",
    "section": "Save data to Excel-compatible format",
    "text": "Save data to Excel-compatible format\nEnglish number format (“.” as decimal):\n\nwrite.table(cities, \"output.csv\", row.names = FALSE, sep=\",\")\n\nGerman number format (“,” as decimal):\n\nwrite.table(cities, \"output.csv\", row.names = FALSE, sep=\";\", dec=\",\")\n\n ## Creation of data frames\n\n\ntypical: read data from external file, e.g. csv-files.\nsmall data frames can be created inline in a script\n\nInline creation of a data frame\n\nclem &lt;- data.frame(\n  brand = c(\"EP\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \n            \"EB\", \"EB\", \"EB\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EB\", \"EP\"),\n  weight = c(88, 96, 100, 96, 90, 100, 92, 92, 102, 99, 86, 89, 99, 89, 75, 80, \n             81, 96, 82, 98, 80, 107, 88)\n)",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#conversion-between-matrices-and-data-frames",
    "href": "slides/x1-r-basics.html#conversion-between-matrices-and-data-frames",
    "title": "x1-R Basics",
    "section": "Conversion between matrices and data frames",
    "text": "Conversion between matrices and data frames\n\nMatrix to data frame\n\nx &lt;- matrix(1:16, nrow=4)\ndf &lt;- as.data.frame(x)\ndf\n\n  V1 V2 V3 V4\n1  1  5  9 13\n2  2  6 10 14\n3  3  7 11 15\n4  4  8 12 16\n\n\n\nData frame to matrix\n\nas.matrix(df)\n\n     V1 V2 V3 V4\n[1,]  1  5  9 13\n[2,]  2  6 10 14\n[3,]  3  7 11 15\n[4,]  4  8 12 16\n\n\n\nAppend column\n\ndf2 &lt;- cbind(df,\n         id = c(\"first\", \"second\", \"third\", \"fourth\")\n       )\n\nOr simply\n\ndf2$id &lt;- c(\"first\", \"second\", \"third\", \"fourth\")\n\n\nData frame with character column\n\nas.matrix(df2)\n\n     V1  V2  V3   V4   id      \n[1,] \"1\" \"5\" \" 9\" \"13\" \"first\" \n[2,] \"2\" \"6\" \"10\" \"14\" \"second\"\n[3,] \"3\" \"7\" \"11\" \"15\" \"third\" \n[4,] \"4\" \"8\" \"12\" \"16\" \"fourth\"\n\n\n\nall columns are now character\nmatrix does not support mixed data",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#selection-of-data-frame-columns",
    "href": "slides/x1-r-basics.html#selection-of-data-frame-columns",
    "title": "x1-R Basics",
    "section": "Selection of data frame columns",
    "text": "Selection of data frame columns\nCreate a data frame from a matrix\n\nx &lt;- matrix(1:16, nrow=4)\ndf &lt;- as.data.frame(x)\ndf\n\n  V1 V2 V3 V4\n1  1  5  9 13\n2  2  6 10 14\n3  3  7 11 15\n4  4  8 12 16\n\n\nAdd names to the columns\n\nnames(df) &lt;- c(\"N\", \"P\", \"O2\", \"C\")\ndf\n\n  N P O2  C\n1 1 5  9 13\n2 2 6 10 14\n3 3 7 11 15\n4 4 8 12 16\n\n\nSelect 3 columns and change order\n\ndf2 &lt;- df[c(\"C\", \"N\", \"P\")]\ndf2\n\n   C N P\n1 13 1 5\n2 14 2 6\n3 15 3 7\n4 16 4 8",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#data-frame-indexing-like-a-matrix",
    "href": "slides/x1-r-basics.html#data-frame-indexing-like-a-matrix",
    "title": "x1-R Basics",
    "section": "Data frame indexing like a matrix",
    "text": "Data frame indexing like a matrix\n\nA data frame\n\ndf\n\n  N P O2  C\n1 1 5  9 13\n2 2 6 10 14\n3 3 7 11 15\n4 4 8 12 16\n\n\nA single value\n\ndf[2, 3]\n\n[1] 10\n\n\nComplete column\n\ndf[,1]\n\n[1] 1 2 3 4\n\n\nComplete row\n\ndf[2,]\n\n  N P O2  C\n2 2 6 10 14\n\n\n\nConditional selection of rows\n\ndf[df$P &gt; 6, ]\n\n  N P O2  C\n3 3 7 11 15\n4 4 8 12 16\n\n\n\nDifferences between [], [[]] and $\n\ndf[\"P\"]     # a single column data frame\n\n  P\n1 5\n2 6\n3 7\n4 8\n\ndf[[\"P\"]]   # a vector\n\n[1] 5 6 7 8\n\ndf$P        # a vector\n\n[1] 5 6 7 8",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#lists-1",
    "href": "slides/x1-r-basics.html#lists-1",
    "title": "x1-R Basics",
    "section": "Lists",
    "text": "Lists\n\nmost flexible data type in R\ncan contain arbitrary data objects as elements of the list\nallows tree-like structure\n\nExamples\n\nOutput of many R functions, e.g. return value of hist:\n\n\nL &lt;- hist(rnorm(100), plot=FALSE)\nstr(L)\n\nList of 6\n $ breaks  : num [1:12] -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 2 ...\n $ counts  : int [1:11] 2 5 12 16 19 17 12 7 5 4 ...\n $ density : num [1:11] 0.04 0.1 0.24 0.32 0.38 0.34 0.24 0.14 0.1 0.08 ...\n $ mids    : num [1:11] -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 2.25 ...\n $ xname   : chr \"rnorm(100)\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#creation-of-lists",
    "href": "slides/x1-r-basics.html#creation-of-lists",
    "title": "x1-R Basics",
    "section": "Creation of lists",
    "text": "Creation of lists\n\nL1 &lt;- list(a=1:10, b=c(1,2,3), x=\"hello\")\n\nNested list (lists within a list)\n\nL2 &lt;- list(a=5:7, b=L1)\n\nstr shows tree-like structure\n\nstr(L2)\n\nList of 2\n $ a: int [1:3] 5 6 7\n $ b:List of 3\n  ..$ a: int [1:10] 1 2 3 4 5 6 7 8 9 10\n  ..$ b: num [1:3] 1 2 3\n  ..$ x: chr \"hello\"\n\n\n\nAccess to list elements by names\n\nL2$a\n\n[1] 5 6 7\n\nL2$b$a\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\n\n\nor with indices\n\nL2[1]   # a list with 1 element\n\n$a\n[1] 5 6 7\n\nL2[[1]] # content of 1st element\n\n[1] 5 6 7",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#lists-ii",
    "href": "slides/x1-r-basics.html#lists-ii",
    "title": "x1-R Basics",
    "section": "Lists II",
    "text": "Lists II\n\nConvert list to vector\n\n\nL &lt;- unlist(L2)\nstr(L)\n\n Named chr [1:17] \"5\" \"6\" \"7\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\" \"7\" \"8\" \"9\" \"10\" \"1\" ...\n - attr(*, \"names\")= chr [1:17] \"a1\" \"a2\" \"a3\" \"b.a1\" ...\n\n\n\n\nFlatten list (remove only top level of list)\n\n\nL &lt;- unlist(L2, recursive = FALSE)\nstr(L)\n\nList of 6\n $ a1 : int 5\n $ a2 : int 6\n $ a3 : int 7\n $ b.a: int [1:10] 1 2 3 4 5 6 7 8 9 10\n $ b.b: num [1:3] 1 2 3\n $ b.x: chr \"hello\"",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#naming-of-list-elements",
    "href": "slides/x1-r-basics.html#naming-of-list-elements",
    "title": "x1-R Basics",
    "section": "Naming of list elements",
    "text": "Naming of list elements\nDuring creation\n\nx &lt;- c(a=1.2, b=2.3, c=6)\nL &lt;- list(a=1:3, b=\"hello\")\n\nWith names-function\nOriginal names:\n\nnames(L)\n\n[1] \"a\" \"b\"\n\n\nRename list elements:\n\nnames(L) &lt;- c(\"numbers\", \"text\")\nnames(L)\n\n[1] \"numbers\" \"text\"   \n\n\nThe names-functions works also with vectors. The pre-defined vectors letters contains lower case and LETTERS uppercase letters:\n\nx &lt;- 1:5\nnames(x) &lt;- letters[1:5]\nx\n\na b c d e \n1 2 3 4 5",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#apply-a-function-to-multiple-rows-and-columns",
    "href": "slides/x1-r-basics.html#apply-a-function-to-multiple-rows-and-columns",
    "title": "x1-R Basics",
    "section": "Apply a function to multiple rows and columns",
    "text": "Apply a function to multiple rows and columns\n\nExample data frame\n\ndf  # data frame of previous slide\n\n  N P O2  C\n1 1 5  9 13\n2 2 6 10 14\n3 3 7 11 15\n4 4 8 12 16\n\n\nApply a function to all elements of a list\n\nlapply(df, mean)  # returns list\n\n$N\n[1] 2.5\n\n$P\n[1] 6.5\n\n$O2\n[1] 10.5\n\n$C\n[1] 14.5\n\nsapply(df, mean)  # returns vector\n\n   N    P   O2    C \n 2.5  6.5 10.5 14.5 \n\n\n\n\n\nRow wise apply\n\napply(df, MARGIN = 1, sum)\n\n[1] 28 32 36 40\n\n\nColumn wise apply\n\napply(df, MARGIN = 2, sum)\n\n N  P O2  C \n10 26 42 58 \n\n\nApply user defined function\n\nse &lt;- function(x)\n  sd(x)/sqrt(length(x))\n\nsapply(df, se)\n\n\n\n     N      P     O2      C \n0.6455 0.6455 0.6455 0.6455",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#for-loop",
    "href": "slides/x1-r-basics.html#for-loop",
    "title": "x1-R Basics",
    "section": "for-loop",
    "text": "for-loop\nA simple for-loop\n\n\nfor (i in 1:4) {\n  cat(i, 2*i, \"\\n\")\n}\n\n1 2 \n2 4 \n3 6 \n4 8 \n\n\n\nNested for-loops\n\n\nfor (i in 1:3) {\n  for (j in c(1,3,5)) {\n    cat(i, i*j, \"\\n\")\n  }\n}\n\n1 1 \n1 3 \n1 5 \n2 2 \n2 6 \n2 10 \n3 3 \n3 9 \n3 15",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#repeat-and-while-loops",
    "href": "slides/x1-r-basics.html#repeat-and-while-loops",
    "title": "x1-R Basics",
    "section": "repeat and while-loops",
    "text": "repeat and while-loops\nRepeat until a break condition occurs\n\n\nx &lt;- 1\nrepeat {\n x &lt;- 0.1*x\n cat(x, \"\\n\")\n if (x &lt; 1e-4) break\n}\n\n0.1 \n0.01 \n0.001 \n1e-04 \n1e-05 \n\n\nLoop as long as a whilecondition is TRUE:\n\nj &lt;- 1; x &lt;- 0\nwhile (j &gt; 1e-3) {\n  j &lt;- 0.1 * j\n  x &lt;- x + j\n  cat(j, x, \"\\n\")\n}\n\n0.1 0.1 \n0.01 0.11 \n0.001 0.111 \n1e-04 0.1111 \n\n\n\nIn many cases, loops can be avoided by using vectors and matrices or apply.",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#avoidable-loops",
    "href": "slides/x1-r-basics.html#avoidable-loops",
    "title": "x1-R Basics",
    "section": "Avoidable loops",
    "text": "Avoidable loops\n\nColumn means of a data frame\n\n## a data frame\ndf &lt;- data.frame(\n  N=1:4, P=5:8, O2=9:12, C=13:16\n)\n\n## loop\nm &lt;- numeric(4)\nfor(i in 1:4) {\n m[i] &lt;- mean(df[,i])\n}\nm\n\n[1]  2.5  6.5 10.5 14.5\n\n\n\\(\\rightarrow\\) easier without loop\n\nsapply(df, mean)\n\n   N    P   O2    C \n 2.5  6.5 10.5 14.5 \n\n\n… also possible colMeans\n\n\n\nAn infinite series:\n\\[\n\\sum_{k=1}^{\\infty}\\frac{(-1)^{k-1}}{2k-1} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7}\n\\]\n\nx &lt;- 0\nfor (k in seq(1, 1e5)) {\n  enum  &lt;- (-1)^(k-1)\n  denom &lt;- 2*k-1\n  x &lt;- x + enum/denom\n}\n4 * x\n\n[1] 3.141583\n\n\n\\(\\Rightarrow\\) Can you vectorize this?",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#unavoidable-loop",
    "href": "slides/x1-r-basics.html#unavoidable-loop",
    "title": "x1-R Basics",
    "section": "Unavoidable loop",
    "text": "Unavoidable loop\nThe same series:\n\\[\n\\sum_{k=1}^{\\infty}\\frac{(-1)^{k-1}}{2k-1} = 1 - \\frac{1}{3} + \\frac{1}{5} - \\frac{1}{7}\n\\]\n\nx &lt;- 0\nk &lt;- 0\nrepeat {\n  k &lt;- k + 1\n  enum  &lt;- (-1)^(k-1)\n  denom &lt;- 2*k-1\n  delta &lt;- enum/denom\n  x &lt;- x + delta\n  if (abs(delta) &lt; 1e-6) break\n}\n4 * x\n\n[1] 3.141595\n\n\n\nnumber of iterations not known in advance\nconvergence criterium, stop when required precision is reached\nno allocation of long vectors –&gt; less memory than for loop\n\n\n\nNote: there are more efficient methods to calculate \\(\\pi\\).",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#if-clause",
    "href": "slides/x1-r-basics.html#if-clause",
    "title": "x1-R Basics",
    "section": "if-clause",
    "text": "if-clause\n\nThe example before showed already an if-clause. The syntax is as follows:\n\nif (&lt;condition&gt;)\n  &lt;statement&gt;\nelse if (&lt;condition&gt;)\n  &lt;statement&gt;\nelse\n  &lt;statement&gt;\n\n\nProper indentation improves readability.\nRecommended: 2 characters\nProfessionals indent always.\nPlease do!\n\n\n\n\nUse of {} to group statements\n\nstatement can of be a compound statement with curly brackets {}\nto avoid common mistakes and be on the safe side, use always {}:\n\nExample:\n\nif (x == 0) {\n  print(\"x is Null\")\n} else if (x &lt; 0) {\n  print(\"x is negative\")\n} else {\n  print(\"x is positive\")\n}",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#vectorized-if",
    "href": "slides/x1-r-basics.html#vectorized-if",
    "title": "x1-R Basics",
    "section": "Vectorized if",
    "text": "Vectorized if\nOften, a vectorized ifelse is more appropropriate than an if-function.\nLet’s assume we have a data set of chemical measurements x with missing NA values, and “nondetects” that are encoded with -99. First we want to replace the nontetects with half of the detection limit (e.g. 0.5):\n\nx &lt;- c(3, 6, NA, 5, 4, -99, 7, NA,  8, -99, -99, 9)\nx2 &lt;- ifelse(x == -99, 0.5, x)\nx2\n\n [1] 3.0 6.0  NA 5.0 4.0 0.5 7.0  NA 8.0 0.5 0.5 9.0\n\n\nNow let’s remove the NAs:\n\nx3 &lt;- na.omit(x2)\nx3\n\n [1] 3.0 6.0 5.0 4.0 0.5 7.0 8.0 0.5 0.5 9.0\nattr(,\"na.action\")\n[1] 3 8\nattr(,\"class\")\n[1] \"omit\"",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/x1-r-basics.html#further-reading",
    "href": "slides/x1-r-basics.html#further-reading",
    "title": "x1-R Basics",
    "section": "Further reading",
    "text": "Further reading\n\nFollow-up presentations:\n\nFunctions everywhere\nGraphics in R\n\nMore details in the official R manuals, especially in “An Introduction to R”\nMany videos can be found on Youtube, at the Posit webpage and somewhere else.\nThis tutorial was made with Quarto\nAuthor: tpetzoldt +++ Homepage +++ Github page",
    "crumbs": [
      "R Topics",
      "x1-R Basics"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#data-sets-and-terms-of-use",
    "href": "slides/11-multivariate-1.html#data-sets-and-terms-of-use",
    "title": "11-Multivariate methods I",
    "section": "Data sets and terms of use",
    "text": "Data sets and terms of use\n\n\nThe “UBA-lakes” data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe “gauernitz” data set contains simplified teaching versions from research data, of the study from Winkelmann et al. (2011)\nThe document itself, the codes and the ebedded images are own work and can be shared according to CC BY 4.0.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#an-introductory-example",
    "href": "slides/11-multivariate-1.html#an-introductory-example",
    "title": "11-Multivariate methods I",
    "section": "An introductory example",
    "text": "An introductory example",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#correlation-between-all-variables",
    "href": "slides/11-multivariate-1.html#correlation-between-all-variables",
    "title": "11-Multivariate methods I",
    "section": "Correlation between all variables?",
    "text": "Correlation between all variables?\n\nlibrary(\"readxl\") # read Excel files directly\nlakes &lt;- as.data.frame(\n  read_excel(\"../data/uba/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx\", sheet=\"Tabelle1\", skip=3)\n)\nnames(lakes) &lt;- c(\"name\", \"state\", \"drainage\", \"population\", \"altitude\", \n                  \"z_mean\", \"z_max\", \"t_ret\", \"volume\", \"area\", \"shore_length\", \n                  \"shore_devel\", \"drain_ratio\", \"wfd_type\")\nstr(lakes)\n\n'data.frame':   56 obs. of  14 variables:\n $ name        : chr  \"Ammersee\" \"Arendsee\" \"Bleilochtalsperre\" \"Bodensee\" ...\n $ state       : chr  \"BY\" \"ST\" \"TH\" \"BW\" ...\n $ drainage    : num  993 29.8 1239.9 11477 28.2 ...\n $ population  : num  114900 3200 180000 1400000 28 ...\n $ altitude    : num  532.9 22.8 404 395.4 13.1 ...\n $ z_mean      : num  37.6 28.6 23.3 85 2.4 ...\n $ z_max       : num  81.1 48.7 55 254 4.8 58.3 32.5 73.4 1.7 18.8 ...\n $ t_ret       : num  2.7 50 0.526 4.2 1.9 16.3 NA 1.26 0.1 2.3 ...\n $ volume      : num  1.75 0.147 0.215 48.522 0.009 ...\n $ area        : num  46.6 5.14 9.2 571.5 3.9 ...\n $ shore_length: num  43 9 NA 273 10.4 13.7 17.5 64 7.5 10.1 ...\n $ shore_devel : num  1.78 1.12 NA 2.26 1.48 2.09 1.64 2.02 2.22 1.6 ...\n $ drain_ratio : num  20.3 5.8 NA 21.9 7.2 ...\n $ wfd_type    : chr  \"Typ 4\" \"Typ 13\" \"Typ 5\" \"Typ 4\" ...\n\n\n\nnames(lakes) replaces the original German column names by abbreviated English abbreviations",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables",
    "href": "slides/11-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables",
    "title": "11-Multivariate methods I",
    "section": "Create pairwise scatter plots for all variables?",
    "text": "Create pairwise scatter plots for all variables?\n\nplot(lakes)",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables-1",
    "href": "slides/11-multivariate-1.html#create-pairwise-scatter-plots-for-all-variables-1",
    "title": "11-Multivariate methods I",
    "section": "Create pairwise scatter plots for all variables?",
    "text": "Create pairwise scatter plots for all variables?\n\n\nnot a good idea, 14 variables would produce 182 (or 91) plots\ncan lead to “statistical fishing”\nwe need methods to extract the main information with a small number of plots",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#the-multivariate-approach",
    "href": "slides/11-multivariate-1.html#the-multivariate-approach",
    "title": "11-Multivariate methods I",
    "section": "The multivariate approach",
    "text": "The multivariate approach\n\nWork with the complete original variables directly\n\n\\(\\rightarrow\\) Multivariate statistics\n\ndependent variables + explanation variables optional\nanalyze distance and location in multidimensional space\nfind way to vizualize relationship in lower dimensions\n\n\nFor comparison\n\nunivariate: 1 variable\nbivariate: 1 dependent, 1 independent variable\nmultiple: 1 dependent, &gt;1 independent variables",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#two-approches-of-multivariate-statistics",
    "href": "slides/11-multivariate-1.html#two-approches-of-multivariate-statistics",
    "title": "11-Multivariate methods I",
    "section": "Two approches of multivariate statistics",
    "text": "Two approches of multivariate statistics\n\nOrdination\n\n\n\n\n\n\n\n\n\n\ndimension reduction\nrelate observations and variables\n\n\nCluster analysis\n\n\n\n\n\n\n\n\n\n\ndistance and similarity\nidentification of groups",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#basic-concepts",
    "href": "slides/11-multivariate-1.html#basic-concepts",
    "title": "11-Multivariate methods I",
    "section": "Basic concepts",
    "text": "Basic concepts\n\nSimilarity and correlation\n\ndistance and similarity: \\(\\rightarrow\\) How different or similar are the observations?\ncorrelation and covariance: \\(\\rightarrow\\) Are variables interdependent?\ndimension reduction: \\(\\rightarrow\\) Try to show essential parts of information on a lower number of dimensions.\ncluster analysis: \\(\\rightarrow\\) Show which observations are closely together.\nordination: \\(\\rightarrow\\) Plot data at lower dimensions, similar observations closely together.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#a-data-set-from-german-lakes",
    "href": "slides/11-multivariate-1.html#a-data-set-from-german-lakes",
    "title": "11-Multivariate methods I",
    "section": "A data set from German lakes",
    "text": "A data set from German lakes\n\n\n\n\n\n\n GPS data from Wikipedia and Google Maps, map from https://www.openstreetmap.org/",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#example-a-simplified-subset-from-the-uba-lake-data",
    "href": "slides/11-multivariate-1.html#example-a-simplified-subset-from-the-uba-lake-data",
    "title": "11-Multivariate methods I",
    "section": "Example: A simplified subset from the UBA lake data",
    "text": "Example: A simplified subset from the UBA lake data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nshortname\nz_mean\nz_max\nt_ret\nvolume\narea\np_tot\nn_no3\nchl\nwfd_type\n\n\n\n\nAmmer\nAmmersee\nAmmer\n37.60\n81.1\n2.70\n1.75000\n46.600\n7.3\n1.09\n2.80\nTyp 4\n\n\nArend\nArendsee\nArend\n28.60\n48.7\n50.00\n0.14700\n5.140\n375.0\n0.05\n22.30\nTyp 13\n\n\nBoden\nBodensee\nBoden\n85.00\n254.0\n4.20\n48.52150\n571.500\n6.9\n0.84\n2.10\nTyp 4\n\n\nChiem\nChiemsee\nChiem\n25.60\n73.4\n1.26\n2.04800\n79.900\n9.2\n0.55\n3.80\nTyp 4\n\n\nDober\nDobersdorfer See\nDober\n5.40\n18.8\n2.30\n0.01690\n3.120\n63.9\n0.64\n27.30\nTyp 14\n\n\nMuegg\nGroßer Müggelsee\nMuegg\n4.85\n7.5\n0.20\n0.03500\n7.200\n189.9\n0.17\n32.90\nTyp 11\n\n\nPloen\nGroßer Plöner See\nPloen\n12.40\n58.0\n3.10\n0.37200\n29.970\n62.3\n0.22\n8.80\nTyp 13\n\n\nKumme\nKummerower See\nKumme\n8.10\n23.3\n1.50\n0.26300\n32.500\n65.3\n0.78\n16.60\nTyp 11\n\n\nMueritz\nMüritz (Außenmüritz)\nMueritz\n6.50\n28.1\n6.00\n0.68000\n105.300\n19.7\n0.11\n6.30\nTyp 14\n\n\nMuerB\nMüritz (Binnenmüritz)\nMuerB\n9.80\n30.3\n6.00\n0.03800\n3.910\n34.2\n0.11\n6.70\nTyp 10\n\n\nPlaue\nPlauer See\nPlaue\n6.80\n25.5\n3.00\n0.30000\n38.400\n26.0\n0.09\n6.80\nTyp 10\n\n\nSacro\nSacrower See\nSacro\n18.01\n36.0\n15.00\n0.01930\n1.072\n79.8\n0.04\n8.60\nTyp 10\n\n\nSchar\nScharmützelsee\nSchar\n9.00\n29.5\n16.00\n0.10823\n12.090\n35.3\n0.12\n10.40\nTyp 13\n\n\nSchwA\nSchweriner See (Außensee)\nSchwA\n9.40\n52.4\n10.00\n0.33100\n35.200\n100.0\n0.23\n11.70\nTyp 13\n\n\nSchwI\nSchweriner See (Innensee)\nSchwI\n13.50\n44.6\n5.30\n0.35600\n26.400\n246.5\n0.19\n5.86\nTyp 13\n\n\nStarn\nStarnberger See\nStarn\n53.20\n127.8\n21.00\n2.99900\n56.400\n5.9\n0.32\n1.84\nTyp 3\n\n\nStech\nStechlinsee\nStech\n22.80\n68.0\n32.00\n0.09700\n4.250\n15.8\n0.04\n2.60\nTyp 13\n\n\nStein\nSteinhuder Meer\nStein\n1.35\n2.9\n2.30\n0.04200\n29.100\n53.3\n0.12\n29.00\nTyp 11\n\n\n\n\n\nmean and maximum depth (\\(\\mathrm{m}\\)): z_mean, z_max; retention time (years): t_ret; volume (\\(\\mathrm{10^9 m^3}\\)); area (\\(\\mathrm{km^2}\\)), total phosphorus P (\\(\\mathrm{\\mu g/L}\\)): p_tot; nitrogen-N (\\(\\mathrm{mg/L}\\)): n_no3, chlorophyll (\\(\\mathrm{\\mu g/L}\\)): chl, water framework directive lake type: wfd_type\n\nData set from UBA (Umweltbundesamt = German Federal Environmental Agency), data modified Original data source: https://www.umweltbundesamt.de/themen/wasser/seen Terms of use: https://www.umweltbundesamt.de/datenschutz-haftung-urheberrecht",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#code-to-read-the-data",
    "href": "slides/11-multivariate-1.html#code-to-read-the-data",
    "title": "11-Multivariate methods I",
    "section": "Code to read the data",
    "text": "Code to read the data\n\nlakes &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/lakes-combined-data.csv\")\nvalid_columns &lt;- c(\"name\", \"shortname\", \"z_mean\", \"z_max\",  \"t_ret\", \"volume\", \n  \"area\", \"p_tot\", \"n_no3\", \"chl\", \"wfd_type\")\nlakes &lt;- lakes[valid_columns] |&gt; na.omit()\nrow.names(lakes) &lt;- lakes$shortname\n\nlake_ids &lt;- lakes[c(\"name\", \"shortname\")]\nlakedata &lt;- lakes[, -c(1, 2)]\n\n## remove wfd_type for now\nlakedata$wfd_type &lt;- NULL\n\n\n\nSource of original data: Uwweltbundesamt https://www.uba.de\nData files downloadable from https://github.com/tpetzoldt/elements/tree/main/data/uba\nTerms of use, see: Copyright and description of derived data",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#data-transformation-and-normalization",
    "href": "slides/11-multivariate-1.html#data-transformation-and-normalization",
    "title": "11-Multivariate methods I",
    "section": "Data transformation and normalization",
    "text": "Data transformation and normalization\n\npar(mfrow = c(1, 4), mar = c(6, 4, 3, 1), las = 2)\nboxplot(lakedata, main = \"raw data\")\nboxplot(scale(lakedata), main = \"normalized\")\nboxplot(scale(sqrt(lakedata)), main = \"sqrt + normalized\")\nboxplot(scale(log(lakedata)), main = \"log + normalized\")\n\n\n\nscale() performs normalisation (z-transformation)\naim: make different scales better comparable",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#principal-component-analysis-pca",
    "href": "slides/11-multivariate-1.html#principal-component-analysis-pca",
    "title": "11-Multivariate methods I",
    "section": "Principal Component Analysis: PCA",
    "text": "Principal Component Analysis: PCA\n\n\nidentify cvovariance or correlation structure\nrotate coordinate system, so that it points in the diretions of maximum variance\n\\(k\\) dimensions in original space are transformed into \\(k\\) orthogonal (rectangular) coordinates in principal components space.\nworks with any number of dimensions\nvisualisation by a 3D example",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#correlation-structure-of-the-lakes-data-set",
    "href": "slides/11-multivariate-1.html#correlation-structure-of-the-lakes-data-set",
    "title": "11-Multivariate methods I",
    "section": "Correlation structure of the lakes data set",
    "text": "Correlation structure of the lakes data set\n\n\n\nround(cor(lakedata), 2)\n\n       z_mean z_max t_ret volume  area p_tot n_no3   chl\nz_mean   1.00  0.97  0.20   0.81  0.78 -0.17  0.48 -0.49\nz_max    0.97  1.00  0.07   0.88  0.87 -0.26  0.47 -0.54\nt_ret    0.20  0.07  1.00  -0.12 -0.18  0.48 -0.41 -0.04\nvolume   0.81  0.88 -0.12   1.00  0.98 -0.20  0.44 -0.27\narea     0.78  0.87 -0.18   0.98  1.00 -0.26  0.45 -0.32\np_tot   -0.17 -0.26  0.48  -0.20 -0.26  1.00 -0.32  0.47\nn_no3    0.48  0.47 -0.41   0.44  0.45 -0.32  1.00 -0.15\nchl     -0.49 -0.54 -0.04  -0.27 -0.32  0.47 -0.15  1.00\n\n\n\n\n\nLet’s pick 3 variables for a 3D visualization: z_mix, z_max and volume.\nUse log-transformation to make them symetrically distributed.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#z_mix-z_max-and-volume-are-highly-corelated",
    "href": "slides/11-multivariate-1.html#z_mix-z_max-and-volume-are-highly-corelated",
    "title": "11-Multivariate methods I",
    "section": "z_mix, z_max and volume are highly corelated",
    "text": "z_mix, z_max and volume are highly corelated\n\n\\(\\rightarrow\\) We see that the three variables carry redundant information.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#how-rotation-of-axes-works",
    "href": "slides/11-multivariate-1.html#how-rotation-of-axes-works",
    "title": "11-Multivariate methods I",
    "section": "How rotation of axes works",
    "text": "How rotation of axes works\n\nOriginal coordinates\n\n\n\n\n\n\n\n\nPCA rotated coordinates\n\n\n\n\n\n\n\n\n\nThis slide contains interactive 3D graphics, that can be rotated with the mouse.\nRotate the left image, so that the points on both size show similar patterns.\nRotate the right image to show PC 3\n\n\\(\\rightarrow\\) Most of the 3D information of the data can be vizualized in 2D.\nNote: log-transformed variables were used in this example.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#pca-is-an-orthogonal-model-ii-regression",
    "href": "slides/11-multivariate-1.html#pca-is-an-orthogonal-model-ii-regression",
    "title": "11-Multivariate methods I",
    "section": "PCA is an orthogonal (model II) regression",
    "text": "PCA is an orthogonal (model II) regression\n\n\n\nPC1, PC2, PC3 are the principal components\nOLS is ordinary least squares regression (linear model lm)",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#now-analyse-all-numeric-variables",
    "href": "slides/11-multivariate-1.html#now-analyse-all-numeric-variables",
    "title": "11-Multivariate methods I",
    "section": "Now analyse all numeric variables",
    "text": "Now analyse all numeric variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nshortname\nz_mean\nz_max\nt_ret\nvolume\narea\np_tot\nn_no3\nchl\n\n\n\n\nAmmer\nAmmersee\nAmmer\n37.60\n81.1\n2.70\n1.75000\n46.600\n7.3\n1.09\n2.80\n\n\nArend\nArendsee\nArend\n28.60\n48.7\n50.00\n0.14700\n5.140\n375.0\n0.05\n22.30\n\n\nBoden\nBodensee\nBoden\n85.00\n254.0\n4.20\n48.52150\n571.500\n6.9\n0.84\n2.10\n\n\nChiem\nChiemsee\nChiem\n25.60\n73.4\n1.26\n2.04800\n79.900\n9.2\n0.55\n3.80\n\n\nDober\nDobersdorfer See\nDober\n5.40\n18.8\n2.30\n0.01690\n3.120\n63.9\n0.64\n27.30\n\n\nMuegg\nGroßer Müggelsee\nMuegg\n4.85\n7.5\n0.20\n0.03500\n7.200\n189.9\n0.17\n32.90\n\n\nPloen\nGroßer Plöner See\nPloen\n12.40\n58.0\n3.10\n0.37200\n29.970\n62.3\n0.22\n8.80\n\n\nKumme\nKummerower See\nKumme\n8.10\n23.3\n1.50\n0.26300\n32.500\n65.3\n0.78\n16.60\n\n\nMueritz\nMüritz (Außenmüritz)\nMueritz\n6.50\n28.1\n6.00\n0.68000\n105.300\n19.7\n0.11\n6.30\n\n\nMuerB\nMüritz (Binnenmüritz)\nMuerB\n9.80\n30.3\n6.00\n0.03800\n3.910\n34.2\n0.11\n6.70\n\n\nPlaue\nPlauer See\nPlaue\n6.80\n25.5\n3.00\n0.30000\n38.400\n26.0\n0.09\n6.80\n\n\nSacro\nSacrower See\nSacro\n18.01\n36.0\n15.00\n0.01930\n1.072\n79.8\n0.04\n8.60\n\n\nSchar\nScharmützelsee\nSchar\n9.00\n29.5\n16.00\n0.10823\n12.090\n35.3\n0.12\n10.40\n\n\nSchwA\nSchweriner See (Außensee)\nSchwA\n9.40\n52.4\n10.00\n0.33100\n35.200\n100.0\n0.23\n11.70\n\n\nSchwI\nSchweriner See (Innensee)\nSchwI\n13.50\n44.6\n5.30\n0.35600\n26.400\n246.5\n0.19\n5.86\n\n\nStarn\nStarnberger See\nStarn\n53.20\n127.8\n21.00\n2.99900\n56.400\n5.9\n0.32\n1.84\n\n\nStech\nStechlinsee\nStech\n22.80\n68.0\n32.00\n0.09700\n4.250\n15.8\n0.04\n2.60\n\n\nStein\nSteinhuder Meer\nStein\n1.35\n2.9\n2.30\n0.04200\n29.100\n53.3\n0.12\n29.00\n\n\n\n\n\n\n\ncolumns: called variables or species\nrows: observations or objects",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#pca-with-the-uba-lake-data",
    "href": "slides/11-multivariate-1.html#pca-with-the-uba-lake-data",
    "title": "11-Multivariate methods I",
    "section": "PCA with the UBA lake data",
    "text": "PCA with the UBA lake data\n\npc &lt;- prcomp(scale(lakedata))\n\nEigenvalues (proportion of variance) indicate importance of components.\n\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2   PC3     PC4    PC5     PC6     PC7     PC8\nStandard deviation     2.0692 1.2735 1.047 0.76067 0.5499 0.30497 0.12230 0.10618\nProportion of Variance 0.5352 0.2027 0.137 0.07233 0.0378 0.01163 0.00187 0.00141\nCumulative Proportion  0.5352 0.7379 0.875 0.94729 0.9851 0.99672 0.99859 1.00000",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#pca-biplot",
    "href": "slides/11-multivariate-1.html#pca-biplot",
    "title": "11-Multivariate methods I",
    "section": "PCA Biplot",
    "text": "PCA Biplot\n\nbiplot(pc)",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#pca-with-sqrt-transformed-data",
    "href": "slides/11-multivariate-1.html#pca-with-sqrt-transformed-data",
    "title": "11-Multivariate methods I",
    "section": "PCA with sqrt transformed data",
    "text": "PCA with sqrt transformed data\n\n\nlakedata2 &lt;- sqrt(lakedata)\npc &lt;- prcomp(scale(lakedata2))\nsummary(pc)\n\nImportance of components:\n                         PC1    PC2    PC3     PC4     PC5     PC6     PC7    PC8\nStandard deviation     2.111 1.3059 0.9748 0.70830 0.50321 0.29742 0.17060 0.1266\nProportion of Variance 0.557 0.2132 0.1188 0.06271 0.03165 0.01106 0.00364 0.0020\nCumulative Proportion  0.557 0.7702 0.8889 0.95165 0.98330 0.99436 0.99800 1.0000\n\n\n\n\nThe PCA with the untransformed data looked very asymmetric, we repeat it with square root transformed data.\nhelps to get a better “resolution”\nmust be taken into account when interpreting the results\nlog transformation is also possible",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#biplot-of-sqrt-transformed-data-1.-3.-pc",
    "href": "slides/11-multivariate-1.html#biplot-of-sqrt-transformed-data-1.-3.-pc",
    "title": "11-Multivariate methods I",
    "section": "Biplot of sqrt transformed data (1.-3. PC)",
    "text": "Biplot of sqrt transformed data (1.-3. PC)\n\npar(mfrow=c(1, 2), mar=c(5, 4, 4, 2.4), las=1)\nbiplot(pc)\nbiplot(pc, choices=c(3, 2))",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#pca-with-the-vegan-package",
    "href": "slides/11-multivariate-1.html#pca-with-the-vegan-package",
    "title": "11-Multivariate methods I",
    "section": "PCA with the vegan package",
    "text": "PCA with the vegan package\n\npc &lt;- rda(lakedata2, scale = TRUE)\nsummary(pc)\n\n\nCall:\nrda(X = lakedata2, scale = TRUE) \n\nPartitioning of correlations:\n              Inertia Proportion\nTotal               8          1\nUnconstrained       8          1\n\nEigenvalues, and their contribution to the correlations \n\nImportance of components:\n                        PC1    PC2    PC3     PC4     PC5     PC6      PC7      PC8\nEigenvalue            4.456 1.7053 0.9503 0.50170 0.25322 0.08846 0.029105 0.016020\nProportion Explained  0.557 0.2132 0.1188 0.06271 0.03165 0.01106 0.003638 0.002003\nCumulative Proportion 0.557 0.7702 0.8889 0.95165 0.98330 0.99436 0.997997 1.000000\n\n\n\nThe two first PCs explain PC 1 = 77% of variance of the square root transformed observations.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#variance-importance-and-biplot",
    "href": "slides/11-multivariate-1.html#variance-importance-and-biplot",
    "title": "11-Multivariate methods I",
    "section": "Variance importance and biplot",
    "text": "Variance importance and biplot\n\nImportance of components\n\nPC 1 = 56%\nPC 2 = 21%\nPC 3 = 12%\n\n\\(\\Rightarrow\\) The two first PCs explain PC 1 = 77% of variance of the square root transformed observations.\n\n\nbiplot(pc)",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#interpretation",
    "href": "slides/11-multivariate-1.html#interpretation",
    "title": "11-Multivariate methods I",
    "section": "Interpretation",
    "text": "Interpretation\n\nVariables\n\nonly long arrows can be interpreted\nsame direction: positive correlation\nopposite direction: negative correlation\nrectangular: no correlation\n\nObservations\n\nidentification of groups and extreme cases\n\nCombined View\n\nrelative size of variable for observations\narrow direction: high values\nopposite direction: low values\naround middle: average\n\n\\(\\Rightarrow\\) Important: always read perpendicular to PCs and arrows!",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#biplot-in-3d",
    "href": "slides/11-multivariate-1.html#biplot-in-3d",
    "title": "11-Multivariate methods I",
    "section": "Biplot in 3D",
    "text": "Biplot in 3D\n\n\nVariance\n\nPC 1 = 56%\nPC 2 = 21%\nPC 3 = 12%\nsum of PC 4 … 8 = 11%\n\n\n\\(\\rightarrow\\) Use the mouse to rotate and zoom.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#nonmetric-multidimensional-scaling-nmds",
    "href": "slides/11-multivariate-1.html#nonmetric-multidimensional-scaling-nmds",
    "title": "11-Multivariate methods I",
    "section": "Nonmetric Multidimensional Scaling: NMDS",
    "text": "Nonmetric Multidimensional Scaling: NMDS\n\nmd &lt;- metaMDS(lakedata2, scale = TRUE, distance = \"euclid\", trace=FALSE)\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\nThe NMDS tries to map the higher dimensions even better to 2D or 3D. However, it accepts some distortion. More about this in the next chapter.",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#the-code",
    "href": "slides/11-multivariate-1.html#the-code",
    "title": "11-Multivariate methods I",
    "section": "The code",
    "text": "The code\n\nlakes &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/lakes-combined-data.csv\")\nvalid_columns &lt;- c(\"name\", \"shortname\", \"z_mean\", \"z_max\",  \"t_ret\", \"volume\", \n  \"area\", \"p_tot\", \"n_no3\", \"chl\", \"wfd_type\")\n\n# set row names and limit data set to complete records without missing data\nrow.names(lakes) &lt;- lakes$shortname\nlakes &lt;- na.omit(lakes[valid_columns])\n\n# only the numerical variables\nlakedata &lt;- lakes[c(\"z_mean\", \"z_max\",  \"t_ret\", \"volume\", \n                     \"area\", \"p_tot\", \"n_no3\", \"chl\")] \n\n## PCA with the vegan package\nlibrary(\"vegan\")\nlakedata2 &lt;- sqrt(lakedata)\npc &lt;- rda(lakedata2, scale = TRUE)\nsummary(pc)\nbiplot(pc)\nbiplot(pc, choices=c(3, 2))\n\n## 3D Plot\nlibrary(\"vegan3d\")\nordirgl(pc, col = \"yellow\")\norgltext(pc, display = \"species\", col=\"red\")\norgltext(pc, display = \"sites\", col=\"blue\", pos=4)\nview3d(theta = 5, phi = 15, fov=30, zoom=1)\n\n## NMDS\nmd &lt;- metaMDS(lakedata2, scale = TRUE, distance = \"euclid\", trace=FALSE)\nplot(md, type=\"text\")\nabline(h=0, col=\"grey\", lty=\"dotted\")\nabline(v=0, col=\"grey\", lty=\"dotted\")\n\n\nData and full source code is available from https://github.com/tpetzoldt/elements/",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#summary",
    "href": "slides/11-multivariate-1.html#summary",
    "title": "11-Multivariate methods I",
    "section": "Summary",
    "text": "Summary\n\n\nMultivariate methods can be used to analyze data sets, where multiple variables depend on each other.\nPCA is a dimension reduction technique that tries to map high-dimensional data to a lower number of dimensions.\nIt can be used as an explorative technique to find relationships between data.\nCompared to creating multiple plots between all variables, it helps to see the overall picture, saves time and avoids statistical fishing.\nThe next chapter will introduce more methods.\n\n\nFurther reading\n\nOksanen (2010)\nBorcard et al. (2018)",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/11-multivariate-1.html#references",
    "href": "slides/11-multivariate-1.html#references",
    "title": "11-Multivariate methods I",
    "section": "References",
    "text": "References\n\n\n\n\n\nBorcard, D., Gillet, F., & Legendre, P. (2018). Numerical ecology with R. Springer International Publishing. https://doi.org/10.1007/978-3-319-71404-2\n\n\nOksanen, J. (2010). Multivariate analysis of ecological communities in R: Vegan tutorial.\n\n\nUmweltbundesamt. (2021). Kenndaten ausgewählter Seen Deutschlands. https://www.umweltbundesamt.de/daten/wasser/zustand-der-seen#okologischer-zustand-der-seen\n\n\nWinkelmann, C., Hellmann, C., Worischka, S., Petzoldt, T., & Benndorf, J. (2011). Fish predation affects the structure of a benthic community. Freshwater Biology, 56(6), 1030–1046. https://doi.org/10.1111/j.1365-2427.2010.02543.x",
    "crumbs": [
      "Basic Statistics",
      "11-Multivariate methods I"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#example-1-co2-in-the-atmosphere",
    "href": "slides/09-timeseries-basics.html#example-1-co2-in-the-atmosphere",
    "title": "09-Time Series Basics",
    "section": "Example 1: CO2 in the atmosphere",
    "text": "Example 1: CO2 in the atmosphere\n\n\n\nShow the code\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nco2 &lt;- read_csv(\"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", \n                skip = 40, show_col_types = FALSE)\n\nco2 |&gt; \n  mutate(date=as_date(paste(year, month, 15, sep=\".\"))) |&gt;\n  ggplot(aes(date, average)) + \n  geom_line() + ylab(expression(CO[2]~\"in the atmosphere (ppm)\")) + \n  ggtitle(\"Mauna Loa Observatory, Hawaii, Monthly Averages\") +\n  geom_smooth()\n\n\n\n\nData source:\nKeeling et al. (2001), Tans & Keeling (2023), https://gml.noaa.gov/ccgg/trends/data.html",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#example-2-monthly-mean-air-temperature",
    "href": "slides/09-timeseries-basics.html#example-2-monthly-mean-air-temperature",
    "title": "09-Time Series Basics",
    "section": "Example 2: Monthly mean air temperature",
    "text": "Example 2: Monthly mean air temperature\n\n\nShow the code\nlibrary(dplyr)\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\ntempdata &lt;- read_csv(\"../data/airtemp_dresden_daily.csv\") |&gt;\n  select(ZEIT, TM) |&gt;\n  rename(date = ZEIT) |&gt;\n  mutate(year = year(date), month = month(date)) |&gt;\n  group_by(year, month) |&gt;\n  summarise(temp = mean(TM)) |&gt;\n  mutate(date = date(paste(year, month, 15, sep=\"-\")))\n\ntempdata |&gt;\n  ggplot(aes(date, temp)) + \n  geom_line() +\n  ylab(\"Temperature (°C)\") + \n  ggtitle(\"Monthly mean air temperature in Dresden\")\n\n\n\nData downloaded from https://rekis.hydro.tu-dresden.de (Kronenberg, 2021), original source Deutscher Wetterdienst, https://www.dwd.de, data modified and averaged.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#example-3-annual-mean-air-temperature",
    "href": "slides/09-timeseries-basics.html#example-3-annual-mean-air-temperature",
    "title": "09-Time Series Basics",
    "section": "Example 3: Annual mean air temperature",
    "text": "Example 3: Annual mean air temperature\n\n\nShow the code\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"lubridate\")\nlibrary(\"readr\")\n## adapt the following line if data were downloaded:\nfile &lt;- \"../data/airtemp_dresden_daily.csv\"\n## or, uncomment to access data directly from the internet:\n#file &lt;- \"https://tpetzoldt.github.io/datasets/data/airtemp_dresden_daily.csv\"\nread_csv(file, show_col_types = FALSE) |&gt;\n  select(ZEIT, TM) |&gt;\n  rename(date = ZEIT) |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(year) |&gt;\n  summarise(temp = mean(TM)) |&gt;\n  ggplot(aes(year, temp)) + \n  geom_line() +\n  geom_smooth(method=\"lm\") +\n  ylab(\"Temperature (°C)\") + \n  ggtitle(\"Annual average air temperature in Dresden\")\n\n\n\nData downloaded from https://rekis.hydro.tu-dresden.de (Kronenberg, 2021), original source Deutscher Wetterdienst, https://www.dwd.de, data modified and averaged.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#characteristics-of-the-examples",
    "href": "slides/09-timeseries-basics.html#characteristics-of-the-examples",
    "title": "09-Time Series Basics",
    "section": "Characteristics of the examples",
    "text": "Characteristics of the examples\n\n\nObservations as a function of time\nMeasurements can be serially interdependent, e.g.:\n\ntrend\nseasonality\n\nNo “true” replicates\n\n\n\\(\\Rightarrow\\) independency assumption of simple linear regression is violated",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#time-series-analysis",
    "href": "slides/09-timeseries-basics.html#time-series-analysis",
    "title": "09-Time Series Basics",
    "section": "Time series analysis",
    "text": "Time series analysis\n\n\nDeals with development of processes in time: \\(x(t)\\),\nHuge number of specific approaches, only a few examples can be presented here.\n\nAims\n\nDoes a trend exist? (significance)\nHow strong is a trend? (effect size)\nIdentification of covariates (influencing factors)\nTrend, seasonality and random error (component model)\nStatistical modeling and forecasting\nBreakpoint analysis, intervention analysis, …, and more",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#stationarity",
    "href": "slides/09-timeseries-basics.html#stationarity",
    "title": "09-Time Series Basics",
    "section": "Stationarity",
    "text": "Stationarity\n\n\nTime series methods have certain assumptions.\nOne of the most common assumptions is stationarity.\n\n\nNote: We are usually not primarily interested in stationarity itself!\n\\(\\rightarrow\\) It is just “the ticket” for further analyses.\n\nExample 1: trend analysis\n\nWe test stationarity to check if a (simple) trend test would be appropriate.\n\nExample 2: linear models\n\nEstimation of a linear trend or a breakpoint model.\nWe check residuals for stationarity to get in formation about reliability of the fitted model.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#stationarity-a-central-concept-in-time-series-analysis",
    "href": "slides/09-timeseries-basics.html#stationarity-a-central-concept-in-time-series-analysis",
    "title": "09-Time Series Basics",
    "section": "Stationarity: a central concept in time series analysis",
    "text": "Stationarity: a central concept in time series analysis\n\n\nShow the code\n  set.seed(123)\n  x &lt;- 1:100\n  par(mfrow = c(1, 3), cex=1.4)\n  plot(x, rnorm(x), type=\"l\", main=\"stationary\", xlab=\"time\", ylab=\"x\")\n  plot(x, 0.01 * x + rnorm(x), type=\"l\", main=\"linear trend\", xlab=\"time\", ylab=\"x\")\n  plot(x, rnorm(x, sd=seq(0.1,1,length=100)), type=\"l\",\n    main=\"increasing variance\", xlab=\"time\", ylab=\"x\")\n\n\n\n\nStrictly or strong stationary process: distribution of \\((x_{s+t})\\) independent from index \\(s\\).\nWide-sense or weakly stationary processes: requires only that 1st and 2nd moments (mean, variance and covariance) do not vary with time.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#three-types-of-basic-time-series",
    "href": "slides/09-timeseries-basics.html#three-types-of-basic-time-series",
    "title": "09-Time Series Basics",
    "section": "Three types of basic time series",
    "text": "Three types of basic time series\n\n\\(x_t = \\beta_0 + \\varepsilon_t\\)\n\n\n\n\n\n\n\n\n\n\n“level stationary process”\nmean, variance and covariance constant\nexample: white noise\n\\(=\\) stationary\n\n\n\\(x_t = \\beta_0 + \\beta_1 t + \\varepsilon_t\\)\n\n\n\n\n\n\n\n\n\n\n“trend stationary process”\nlinear regression model\ncan be made stationary by detrending\n\\(\\rightarrow\\) non-stationary\n\n\n\\(x_t = x_{t-1} + c + \\varepsilon_t\\)\n\n\n\n\n\n\n\n\n\n\n“difference stationary process” (random walk)\ncan be made stationary by differencing\n\\(\\rightarrow\\) non-stationary",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#simulated-data",
    "href": "slides/09-timeseries-basics.html#simulated-data",
    "title": "09-Time Series Basics",
    "section": "Simulated data",
    "text": "Simulated data\n\n\nShow the code\nset.seed(1237)\ntime &lt;- 1:100\n\nLSP &lt;- 4 + rnorm(time)\n\nTSP &lt;- 4 + 0.2 * time + rnorm(time)\n\nDSP &lt;- numeric(length(time))\nDSP[1] &lt;- rnorm(1)\nfor (tt in time[-1])  DSP[tt] &lt;- DSP[tt-1] + 0.2 + rnorm(1)\n\npar(mfrow=c(1,3))\n\nplot(ts(LSP), ylim=c(0, 8), main=\"LSP\", col = \"forestgreen\")\nplot(ts(TSP), main=\"TSP\", col = \"red\")\nplot(ts(DSP), main=\"DSP\", col = \"red\")\n\n\n\nThe introductory examples showed additional types of time series, e.g. with seasonality.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#why-is-this-important",
    "href": "slides/09-timeseries-basics.html#why-is-this-important",
    "title": "09-Time Series Basics",
    "section": "Why is this important?",
    "text": "Why is this important?\n\nSimple linear models require “independence”, or, more precisely: independence of residuals.\n\nBut\n\ntime series observations can be serially dependent\ndependent data have “less value” (contain less information than independent data)\nimportant for calculation of degrees of freedom for the sigificance tests\n\nApproaches\n\nmeasure serial dependency (autocorrelation)\nif yes, handle autocorrelation\n\nremove autocorrelation, or\nmodel autocorrelation",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#autocorrelation-1",
    "href": "slides/09-timeseries-basics.html#autocorrelation-1",
    "title": "09-Time Series Basics",
    "section": "Autocorrelation",
    "text": "Autocorrelation\n\nA time series\n\n\n\n\n\nx_t\n1\n3\n2\n4\n5\n4\n3\n2\n8\n5\n4\n3\n6\n7\n\n\n\n\n\n\n\n\nShifting to the right\n\n\n\n\n\nx_t\n1\n3\n2\n4\n5\n4\n3\n2\n8\n5\n4\n3\n6\n7\n\n\n\n\nx_t+1\n\n1\n3\n2\n4\n5\n4\n3\n2\n8\n5\n4\n3\n6\n7\n\n\n\nx_t+2\n\n\n1\n3\n2\n4\n5\n4\n3\n2\n8\n5\n4\n3\n6\n7\n\n\n\n\n\n\nCorrelation between \\(x_t\\) and \\(x_{t+1}\\) and \\(x_{t+2}\\):\n\ncor(df, use = \"pairwise.complete.obs\")[1,]\n\n       x_t      x_t+1      x_t+2 \n 1.0000000  0.1847131 -0.1411872 \n\n\nIn practice, autocorrelation is calculated using the acf function of R. The algorithm is somewhat different, especially the treatment of missing values, so results differ, especially for small samples.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#autocorrelation-and-partial-autocorrelation-acf-pacf",
    "href": "slides/09-timeseries-basics.html#autocorrelation-and-partial-autocorrelation-acf-pacf",
    "title": "09-Time Series Basics",
    "section": "Autocorrelation and partial autocorrelation (ACF, PACF)",
    "text": "Autocorrelation and partial autocorrelation (ACF, PACF)",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#autocorrelation-function-acf-autocorrelogram",
    "href": "slides/09-timeseries-basics.html#autocorrelation-function-acf-autocorrelogram",
    "title": "09-Time Series Basics",
    "section": "Autocorrelation function (ACF): autocorrelogram",
    "text": "Autocorrelation function (ACF): autocorrelogram\n\n… correlation between time series and its time-shifted (= lagged) versions\nACF considers both, direct and indirect dependency\nPACF considers only direct effects it is called partial autocorrelation, PACF.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#delay-between-air-and-water-temperature-in-a-lake",
    "href": "slides/09-timeseries-basics.html#delay-between-air-and-water-temperature-in-a-lake",
    "title": "09-Time Series Basics",
    "section": "Delay between air and water temperature in a lake",
    "text": "Delay between air and water temperature in a lake\n\n\nair temperature, station Dresden-Klotsche, data from German Weather Service\nwater temperature in Saidenbach Reservoir, 5m below the surface (Data from Ecological Station Neunzehnhain), Horn et al. (2006), Paul et al. (2020)",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#cross-correlation-between-air-and-water-temperature",
    "href": "slides/09-timeseries-basics.html#cross-correlation-between-air-and-water-temperature",
    "title": "09-Time Series Basics",
    "section": "Cross correlation between air and water temperature",
    "text": "Cross correlation between air and water temperature",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#typical-patterns-of-acf",
    "href": "slides/09-timeseries-basics.html#typical-patterns-of-acf",
    "title": "09-Time Series Basics",
    "section": "Typical patterns of ACF",
    "text": "Typical patterns of ACF\n\n\nShow the code\npar(mfrow=c(2, 2))\nacf(LSP, main = \"level stationary\")\nacf(TSP, main = \"trend stationary\")\nacf(DSP, main = \"difference stationary\")\n\ntime &lt;- seq(0, 4 * pi, length.out = 100)\nperiodic &lt;- ts(cos(time) + rnorm(100, sd = 0.1))\nacf(periodic, lag = 100, main = \"periodic\")",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#removal-of-autocorrelation",
    "href": "slides/09-timeseries-basics.html#removal-of-autocorrelation",
    "title": "09-Time Series Basics",
    "section": "Removal of autocorrelation",
    "text": "Removal of autocorrelation\n\n\nTrend stationary series: trend removal\n\nExample:\n\nm &lt;- lm(TSP ~ time(TSP))\nresiduals(m)\n\n\n\nDifference stationary series: differencing\n\nExample:\n\nx &lt;- c(2, 3, 4, 3, 6, 4, 3, 8, 5, 9)\ndiff(x)\n\n[1]  1  1 -1  3 -2 -1  5 -3  4\n\n\n\n\nPeriodic series: identification and removal of seasonal components\nMethods: spectral analysis, seasonal decomposition, averaging …",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#removal-of-autocorrelation-ii",
    "href": "slides/09-timeseries-basics.html#removal-of-autocorrelation-ii",
    "title": "09-Time Series Basics",
    "section": "Removal of autocorrelation II",
    "text": "Removal of autocorrelation II\n\n\n\nACF plots of TSP and DSP\nstrong autocorrelation \\(\\rightarrow\\) non-stationary\n\n\n\ndifferencing\ndifferences of DSP \\(\\rightarrow\\) stationary\n\n\n\ndetrending\nresiduals of TSP \\(\\rightarrow\\) stationary",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#test-of-stationarity",
    "href": "slides/09-timeseries-basics.html#test-of-stationarity",
    "title": "09-Time Series Basics",
    "section": "Test of stationarity",
    "text": "Test of stationarity\n\n\nKwiatkowski-Phillips-Schmidt-Shin test (KPSS test)\nhas built-in trend removal (option null = \"trend\")\n\n\n\n\nkpss.test(TSP)\n\n\n    KPSS Test for Level Stationarity\n\ndata:  TSP\nKPSS Level = 2.0877, Truncation lag parameter = 4, p-value = 0.01\n\n\n\\(\\rightarrow\\) non-stationary\n\n\nkpss.test(TSP, null = \"Trend\")\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  TSP\nKPSS Trend = 0.10306, Truncation lag parameter = 4, p-value = 0.1\n\n\n\\(\\rightarrow\\) stationary after trend removal",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#kpss-test-with-a-difference-stationary-process-dsp",
    "href": "slides/09-timeseries-basics.html#kpss-test-with-a-difference-stationary-process-dsp",
    "title": "09-Time Series Basics",
    "section": "KPSS test with a difference stationary process (DSP)",
    "text": "KPSS test with a difference stationary process (DSP)\n\n\n\n\nkpss.test(DSP)\n\n\n    KPSS Test for Level Stationarity\n\ndata:  DSP\nKPSS Level = 1.5869, Truncation lag parameter = 4, p-value = 0.01\n\n\n\\(\\rightarrow\\) non-stationary\n\n\nkpss.test(DSP, null = \"Trend\")\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  DSP\nKPSS Trend = 0.30063, Truncation lag parameter = 4, p-value = 0.01\n\n\n\\(\\rightarrow\\) trend removal does not cure non-stationarity\n\nTask: Check if a DSP series can be made stationary by differencing.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#trend-tests",
    "href": "slides/09-timeseries-basics.html#trend-tests",
    "title": "09-Time Series Basics",
    "section": "Trend Tests",
    "text": "Trend Tests\nMann-Kendall trend test\n\nnonparametric test\npopular in environmental sciences\nrobust in case of weak autocorrelation\n\nLinear regression analysis\n\nparametric test for linear trend\nslope gives direct information about effect size\nsensitive against autocorrelation\n\nOther methods\n\nSen’s slope, Pettitt’s Test, Cox-Stuart Test, …\n\nIn general\n\nTrend tests are not adequate for if residuals are autocorrelated.\n\\(\\Rightarrow\\) check stationarity (or autocorrelation) of residuals after trend removal !!!",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#mann-kendall-trend-test",
    "href": "slides/09-timeseries-basics.html#mann-kendall-trend-test",
    "title": "09-Time Series Basics",
    "section": "Mann-Kendall-Trend-Test",
    "text": "Mann-Kendall-Trend-Test\n\n\nlibrary(\"Kendall\")\nMannKendall(TSP)\n\ntau = 0.906, 2-sided pvalue =&lt; 2.22e-16\n\n\n\\(\\rightarrow\\) significant trend\n\n\nMannKendall(DSP)\n\ntau = 0.682, 2-sided pvalue =&lt; 2.22e-16\n\n\n\\(\\rightarrow\\) indicates significant trend, but as process is DSP, interpretation is wrong.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#application-trend-of-air-temperature",
    "href": "slides/09-timeseries-basics.html#application-trend-of-air-temperature",
    "title": "09-Time Series Basics",
    "section": "Application: trend of air temperature",
    "text": "Application: trend of air temperature\n\n\ntemp &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/airtemp_april.csv\")\ntemp &lt;- ts(temp$T, start=temp$Year[1])\nplot(temp)\n\n\nData: Air temperature in April, station Dresden-Klotsche (Germany), data source: Deutscher Wetterdienst (https://www.dwd.de), data aggregated and modified.",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#check-of-stationarity-of-residuals-and-trend-test",
    "href": "slides/09-timeseries-basics.html#check-of-stationarity-of-residuals-and-trend-test",
    "title": "09-Time Series Basics",
    "section": "Check of stationarity of residuals and trend test",
    "text": "Check of stationarity of residuals and trend test\n\nCheck for stationarity\n\nkpss.test(temp, null = \"Trend\")\n\n\n    KPSS Test for Trend Stationarity\n\ndata:  temp\nKPSS Trend = 0.097853, Truncation lag parameter = 3, p-value = 0.1\n\n\n\nMann-Kendall-Trend-test\n\nlibrary(Kendall)\nMannKendall(temp)\n\ntau = 0.449, 2-sided pvalue =1.5855e-05\n\n\n\n\nKPSS test does not reject H0 of trend stationarity\nMann-Kendall is significant \\(\\rightarrow\\) monotonous trend",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#estimate-slope-of-linear-trend",
    "href": "slides/09-timeseries-basics.html#estimate-slope-of-linear-trend",
    "title": "09-Time Series Basics",
    "section": "Estimate slope of linear trend",
    "text": "Estimate slope of linear trend\n\n\nShow the code\nplot(temp)\nm &lt;- lm(temp ~ time(temp))\nabline(m, col=\"red\")\n\n\n\n\nsummary(m)\n\n\nCall:\nlm(formula = temp ~ time(temp))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.2733 -0.9369 -0.0816  0.6197  2.9386 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -150.03413   31.88781  -4.705 2.64e-05 ***\ntime(temp)     0.07972    0.01603   4.973 1.11e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.397 on 43 degrees of freedom\nMultiple R-squared:  0.3651,    Adjusted R-squared:  0.3504 \nF-statistic: 24.73 on 1 and 43 DF,  p-value: 1.106e-05",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#a-simulation-experiment",
    "href": "slides/09-timeseries-basics.html#a-simulation-experiment",
    "title": "09-Time Series Basics",
    "section": "A simulation experiment",
    "text": "A simulation experiment\n\nDemonstration that application of a simple linear model to a difference stationary time series leads to an increase of false positives.\n\nTools\n\nDefine two functions to generate time series with specific properties:\n\n\ngenTSP &lt;- function(time, beta0, beta1)\n  as.ts(beta0 + beta1 * time + rnorm(time))\n\ngenDSP &lt;- function(time, c) {\n  DSP &lt;- numeric(length(time))\n  DSP[1] &lt;- rnorm(1)\n  for (tt in time[-1]) DSP[tt] &lt;- DSP[tt-1] + c + rnorm(1)\n  ts(DSP)\n}",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#count-significant-tests",
    "href": "slides/09-timeseries-basics.html#count-significant-tests",
    "title": "09-Time Series Basics",
    "section": "Count significant tests",
    "text": "Count significant tests\n\ncount.signif &lt;- function(N, time, FUN, ...) {\n  a &lt;- 0\n  for (i in 1:N) {\n    x &lt;- FUN(time, ...)\n    m &lt;- summary(lm(x  ~ time(x)))\n    f &lt;- m$fstatistic\n    p.value &lt;- pf(f[1], f[2], f[3], lower=FALSE)\n    # cat(\"p.value\", p.value, \"\\n\")\n    if (p.value &lt; 0.05) a &lt;- a + 1\n  }\n  a\n}\n\nRun the experiment\n\ncount number of significant linear increase (using the F-value)\nif trend parameters (beta1 an c) are set to zero, count.signif counts false positives\nrun loop 100 (or better: 1000) times for both types of time series\n\n\nNruns &lt;- 100 \ntime  &lt;- 1:100\ncount.signif(N=Nruns, time=time, FUN=genTSP, beta0=0, beta1=0) / Nruns\n\n[1] 0.05\n\ncount.signif(N=Nruns, time=time, FUN=genDSP, c=0) / Nruns\n\n[1] 0.91",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#unit-root-test",
    "href": "slides/09-timeseries-basics.html#unit-root-test",
    "title": "09-Time Series Basics",
    "section": "Unit root test",
    "text": "Unit root test\n\n\nanother method for stationarity testing\nchecks whether a time series is of type “DSP”.\nADF-test (augmented Dickey-Fuller test), contained in R-package tseries.\nremember: a “TSP” can be made stationary by subtracting a trend.\n\\(\\rightarrow\\) This is done automatically by the test.\nImportant: in the ADF test, stationarity is the alternative hypothesis!\n\n\n\nlibrary(tseries)\nadf.test(TSP)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  TSP\nDickey-Fuller = -3.87, Lag order = 4, p-value = 0.01831\nalternative hypothesis: stationary\n\nadf.test(DSP)\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  DSP\nDickey-Fuller = -2.1883, Lag order = 4, p-value = 0.4986\nalternative hypothesis: stationary",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#dsp-presence-of-a-unit-root",
    "href": "slides/09-timeseries-basics.html#dsp-presence-of-a-unit-root",
    "title": "09-Time Series Basics",
    "section": "DSP: Presence of a “unit root”",
    "text": "DSP: Presence of a “unit root”\n\n\nDSP series: presence of a unit root cannot be rejected,\n\\(\\rightarrow\\) non-stationary.\nBut, after differencing, there are no objections against stationarity:\n\n\nadf.test(diff(DSP))\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  diff(DSP)\nDickey-Fuller = -3.62, Lag order = 4, p-value = 0.03504\nalternative hypothesis: stationary",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/09-timeseries-basics.html#references",
    "href": "slides/09-timeseries-basics.html#references",
    "title": "09-Time Series Basics",
    "section": "References",
    "text": "References\n\n\n\n\n\nHorn, H., Horn, W., Paul, L., Uhlmann, D., & Röske, I. (2006). Drei Jahrzehnte kontinuierliche Untersuchungen an der Talsperre Saidenbach: Fakten, Zusammenhänge, Trends. Abschlussbericht zum Projekt \"Langzeitstabilität der biologischen Struktur von Talsperren-Ökosystemen\" der Arbeitsgruppe \"Limnologie von Talsperren\" der Sächsischen Akademie der Wissenschaften zu Leipzig (pp. 1–178). Verlag Dr. Uwe Miersch, Ossling.\n\n\nKeeling, C. D., Piper, S. C., Bacastow, R. B., Wahlen, M., Whorf, T. P., Heimann, M., & Meijer, H. A. (2001). Exchanges of atmospheric CO2 and 13CO2 with the terrestrial biosphere and oceans from 1978 to 2000. I. Global aspects (p. 88). Scripps Institution of Oceanography, San Diego.\n\n\nKleiber, C., & Zeileis, A. (2008). Applied econometrics with R. Springer.\n\n\nKronenberg, F., R. (2021). Das regionale klimainformationssystem ReKIS – eine gemeinsame plattform für sachsen, sachsen-anhalt und thüringen. https://rekis.hydro.tu-dresden.de/\n\n\nPaul, L., Horn, H., & W., H. (2020). Saidenbach reservoir in situ and Saidenbach reservoir inlets. Data packages 20, 23 and 177 from the IGB freshwater research and environmental database (FRED). Available according to the ODB CC-BY license. https://fred.igb-berlin.de\n\n\nShumway, R. H., & Stoffer, D. S. (2000). Time series analysis and its applications (Vol. 3). Springer.\n\n\nShumway, R. H., & Stoffer, D. S. (2019). Time series: A data analysis approach using r. CRC Press.\n\n\nTans, P., & Keeling, C. D. (2023). Trends in atmospheric carbon dioxide, Mauna Loa CO2 monthly mean data. NOAA Earth System Research Laboratories, Global Monitoring Laboratory. https://gml.noaa.gov/ccgg/trends/data.html",
    "crumbs": [
      "Basic Statistics",
      "09-Time Series Basics"
    ]
  },
  {
    "objectID": "slides/07-anova.html#anova-analysis-of-variances",
    "href": "slides/07-anova.html#anova-analysis-of-variances",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA – Analysis of Variances",
    "text": "ANOVA – Analysis of Variances\n\n\nTesting of complex hypothesis as a whole, e.g.:\n\nmore than two samples (multiple test problem),\nseveral multiple factors (multiway ANOVA)\nelimination of covariates (ANCOVA)\nfixed and/or random effects (variance decomposition methods, mixed effects models)\n\nDifferent application scenarios:\n\nexplorative use: Which influence factors are important?\ndescriptive use: Fitting of models for process description and forecasting.\nsignificance tests.\n\nANOVA methods are (in most cases) based on linear models.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#a-practical-example",
    "href": "slides/07-anova.html#a-practical-example",
    "title": "07-One and two-way ANOVA",
    "section": "A practical example",
    "text": "A practical example\n\nFind a suitable medium for growth experiments with green algae\n\ncheap, easy to handle\nsuitable for students courses and classroom experiments\n\n\n\nIdea\n\nuse a commercial fertilizer with the main nutrients N and P\nmineral water with trace elements\ndoes non-sparkling mineral water contain enough \\(\\mathrm{CO_2}\\)?\ntest how to improve \\(\\mathrm{CO_2}\\) availability for photosynthesis",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#application",
    "href": "slides/07-anova.html#application",
    "title": "07-One and two-way ANOVA",
    "section": "Application",
    "text": "Application\n\n7 Different treatments\n\n\nfertilizer solution in closed bottles\nfertilizer solution in open bottles (\\(\\mathrm{CO_2}\\) from air)\nfertilizer + sugar (organic C source)\nfertilizer + additional \\(\\mathrm{HCO_3^-}\\) (add \\(\\mathrm{CaCO_3}\\) to sparkling mineral water)\na standard algae growth medium (“Basal medium”) for comparison\ndeionized (“destilled”) water and\ntap water for comparison",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#experimental-design",
    "href": "slides/07-anova.html#experimental-design",
    "title": "07-One and two-way ANOVA",
    "section": "Experimental design",
    "text": "Experimental design\n\n\neach treatment with 3 replicates\nrandomized placement on a shaker\n16:8 light:dark-cycle\nmeasurement directly in the bottles using a self-made turbidity meter",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#results",
    "href": "slides/07-anova.html#results",
    "title": "07-One and two-way ANOVA",
    "section": "Results",
    "text": "Results\n\n\n\n\n Fertilizer – Open Bottle – F. + Sugar – F. + CaCO3 – Basal medium – A. dest – Tap water",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#the-data-set",
    "href": "slides/07-anova.html#the-data-set",
    "title": "07-One and two-way ANOVA",
    "section": "The data set",
    "text": "The data set\n\n\n\n\n\nTable 1: Growth from day 2 to day 6 (relative units)\n\n\n\n\n\n\ntreat\nreplicate 1\nreplicate 2\nreplicate 3\n\n\n\n\nFertilizer\n0.020\n-0.217\n-0.273\n\n\nF. open\n0.940\n0.780\n0.555\n\n\nF.+sugar\n0.188\n-0.100\n0.020\n\n\nF.+CaCO3\n0.245\n0.236\n0.456\n\n\nBas.med.\n0.699\n0.727\n0.656\n\n\nA.dest\n-0.010\n0.000\n-0.010\n\n\nTap water\n0.030\n-0.070\nNA\n\n\n\n\n\n\n\n\n\n\n\nNA means “not available”, i.e. a missing value\nthe crosstable structure is compact and easy to read, but not ideal for data analysis\n\\(\\Rightarrow\\) convert it to long format",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#data-in-long-format",
    "href": "slides/07-anova.html#data-in-long-format",
    "title": "07-One and two-way ANOVA",
    "section": "Data in long format",
    "text": "Data in long format\n\n\nAdvantages\n\nlooks “stupid” but is better for data analysis\ndependent variable growth and explanation variable treat clearly visible\nmodel formula: growth ~ treat\neasily extensible to \\(&gt;1\\) explanation variable\n\n\n\n\n\n\n\ntreat\nrep\ngrowth\n\n\n\n\nFertilizer\n1\n0.020\n\n\nFertilizer\n2\n-0.217\n\n\nFertilizer\n3\n-0.273\n\n\nF. open\n1\n0.940\n\n\nF. open\n2\n0.780\n\n\nF. open\n3\n0.555\n\n\nF.+sugar\n1\n0.188\n\n\nF.+sugar\n2\n-0.100\n\n\nF.+sugar\n3\n0.020\n\n\nF.+CaCO3\n1\n0.245\n\n\nF.+CaCO3\n2\n0.236\n\n\nF.+CaCO3\n3\n0.456",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#the-data-in-r",
    "href": "slides/07-anova.html#the-data-in-r",
    "title": "07-One and two-way ANOVA",
    "section": "The data in R",
    "text": "The data in R\n\n\nalgae &lt;- data.frame(\n  treat  = factor(c(\"Fertilizer\", \"Fertilizer\", \"Fertilizer\", \n             \"F. open\", \"F. open\", \"F. open\", \n             \"F.+sugar\", \"F.+sugar\", \"F.+sugar\", \n             \"F.+CaCO3\", \"F.+CaCO3\", \"F.+CaCO3\", \n             \"Bas.med.\", \"Bas.med.\", \"Bas.med.\", \n             \"A.dest\", \"A.dest\", \"A.dest\", \n             \"Tap water\", \"Tap water\"),\n             levels=c(\"Fertilizer\", \"F. open\", \"F.+sugar\", \n                    \"F.+CaCO3\", \"Bas.med.\", \"A.dest\", \"Tap water\")),\n  rep   = c(1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2), \n  growth = c(0.02, -0.217, -0.273, 0.94, 0.78, 0.555, 0.188, -0.1, 0.02, \n             0.245, 0.236, 0.456, 0.699, 0.727, 0.656, -0.01, 0, -0.01, 0.03, -0.07)\n)\n\n… can be entered directly in the code. A csv-file in long format is also possible.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#boxplot",
    "href": "slides/07-anova.html#boxplot",
    "title": "07-One and two-way ANOVA",
    "section": "Boxplot",
    "text": "Boxplot\n\nboxplot(growth ~ treat, data = algae)\nabline(h = 0, lty = \"dashed\", col = \"grey\")",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#stripchart",
    "href": "slides/07-anova.html#stripchart",
    "title": "07-One and two-way ANOVA",
    "section": "Stripchart",
    "text": "Stripchart\n\nstripchart(growth ~ treat, data = algae, vertical = TRUE)\n\n\nBetter, because we have only 2-3 replicates. Boxplot needs more.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#turn-scientific-question-into-a-statistical-hypothesis",
    "href": "slides/07-anova.html#turn-scientific-question-into-a-statistical-hypothesis",
    "title": "07-One and two-way ANOVA",
    "section": "Turn scientific question into a statistical hypothesis",
    "text": "Turn scientific question into a statistical hypothesis\n\nScientific Questions\n\nAre the treatments different?\nWhich medium is the best?\nIs the best medium significantly better than the others?\n\n Statistical Hypotheses\n\n\\(H_0\\): growth is the same in all treatments\n\\(H_A\\): differences between media",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#why-cant-we-apply-just-several-t-tests",
    "href": "slides/07-anova.html#why-cant-we-apply-just-several-t-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Why can’t we apply just several t-tests?",
    "text": "Why can’t we apply just several t-tests?\n\n\nIf we have 7 treatments and want to test all against each other, we would need:\n\n\\[7 \\cdot (7 - 1) / 2 = 21 \\qquad\\text{tests.}\\]\n\nIf we set \\(\\alpha = 0.05\\) we get 5% false positives. \\(\\Rightarrow\\) One of 20 tests is on average a false positive\nIf we do \\(N\\) tests, we increase the overall \\(\\alpha\\) error to \\(N\\cdot\\alpha\\) in the worst case.\nThis is called alpha-error-inflation or the Bonferroni law:\n\n\\[\n\\alpha_{total} \\le \\sum_{i=1}^{N} \\alpha_i = N \\cdot \\alpha\n\\]\nIf we ignore the Bonferroni law, we end in statistical fishing and get spurious results just by chance.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#anova-analysis-of-variances-1",
    "href": "slides/07-anova.html#anova-analysis-of-variances-1",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA: Analysis of variances",
    "text": "ANOVA: Analysis of variances\n\nBasic Idea\n\nSplit the total variance into effect(s) and errors:\n\n\n\\[\ns_y^2 = s^2_\\mathrm{effect} + s^2_{\\varepsilon}\n\\]\n\n\nSomewhat surprising: we use variances to compare mean values.\nExplanation: differences of means contribute to the total variance of the whole sample.\nVariance components can be called variance within (\\(s^2_\\varepsilon\\)) and variance between samples.\nThe way how to separate variances is a linear model.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#example",
    "href": "slides/07-anova.html#example",
    "title": "07-One and two-way ANOVA",
    "section": "Example",
    "text": "Example\n\nTwo brands of Clementine fruits from a shop “E”, that we encode as “EB” and “EP”. We want to know whether the premium brand (“P”) and the basic brand (“B”) have a different weight.\n\nclem &lt;- data.frame(\n  brand = c(\"EP\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \"EB\", \n            \"EB\", \"EB\", \"EB\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EP\", \"EB\", \"EP\"),\n  weight = c(88, 96, 100, 96, 90, 100, 92, 92, 102, 99, 86, 89, 99, 89, 75, 80, \n             81, 96, 82, 98, 80, 107, 88))\n\n We encode one sample (“EB”) with 1 and the other sample (“EP”) with 2:\n\nclem$code &lt;- as.numeric(factor(clem$brand))\nclem$code\n\n [1] 2 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 1 2",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#then-we-fit-a-linear-regression",
    "href": "slides/07-anova.html#then-we-fit-a-linear-regression",
    "title": "07-One and two-way ANOVA",
    "section": "Then we fit a linear regression:",
    "text": "Then we fit a linear regression:\n\nplot(weight ~ code, data = clem, axes = FALSE)\nm &lt;- lm(weight ~ code, data = clem)\naxis(1, at = c(1,2), labels = c(\"EB\", \"EP\")); axis(2); box()\nabline(m, col = \"blue\")",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#variance-components",
    "href": "slides/07-anova.html#variance-components",
    "title": "07-One and two-way ANOVA",
    "section": "Variance components",
    "text": "Variance components\n\nWe fit a linear model and compare the variances:\n\nm &lt;- lm(weight ~ code, data = clem)\n\ntotal variance\n\n(var_tot &lt;- var(clem$weight))\n\n[1] 68.98814\n\n\nresidual variance (= within variance)\n\n(var_res &lt;- var(residuals(m)))\n\n[1] 43.25\n\n\nexplained variance (= between variance)\n\nvar_tot - var_res\n\n[1] 25.73814\n\n\nNow we can analyse whether the between variance is big enough to justify a significant effect.\nThis is called an ANOVA.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#anova",
    "href": "slides/07-anova.html#anova",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA",
    "text": "ANOVA\n\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: weight\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \ncode       1 566.24  566.24  12.497 0.001963 **\nResiduals 21 951.50   45.31                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nA t-test for comparison\n\nt.test(weight ~ code, data = clem, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by code\nt = 3.5351, df = 21, p-value = 0.001963\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n  4.185911 16.147423\nsample estimates:\nmean in group 1 mean in group 2 \n       95.50000        85.33333 \n\n\n\\(\\Rightarrow\\) the p-values are exactly the same.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#anova-with-more-than-2-samples",
    "href": "slides/07-anova.html#anova-with-more-than-2-samples",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA with more than 2 samples",
    "text": "ANOVA with more than 2 samples\nBack to the algae growth data. Let’s call the linear model m:\n\nm &lt;- lm(growth ~ treat, data = algae)\n\n\n\nWe can print the coefficients of the linear model with summary(m)\nBut we are interested in the overall effect and use anova\n\n\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: growth\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \ntreat      6 2.35441 0.39240  25.045 1.987e-06 ***\nResiduals 13 0.20368 0.01567                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe ANOVA table shows F-tests testing for significance of all factors.\nIn the table above, we have only one single factor.\n\n\\(\\Rightarrow\\) We see that the treatment had a significant effect.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#posthoc-tests",
    "href": "slides/07-anova.html#posthoc-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Posthoc tests",
    "text": "Posthoc tests\n\nThe test showed, that the factor “treatment” had a significant effect.\nWe don’t know yet, which factor levels were different.\n\nTukey HSD test is the most common.\n\ntk &lt;- TukeyHSD(aov(m))\ntk\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = m)\n\n$treat\n                            diff         lwr         upr     p adj\nF. open-Fertilizer    0.91500000  0.56202797  1.26797203 0.0000103\nF.+sugar-Fertilizer   0.19266667 -0.16030537  0.54563870 0.5211198\nF.+CaCO3-Fertilizer   0.46900000  0.11602797  0.82197203 0.0069447\nBas.med.-Fertilizer   0.85066667  0.49769463  1.20363870 0.0000231\nA.dest-Fertilizer     0.15000000 -0.20297203  0.50297203 0.7579063\nTap water-Fertilizer  0.13666667 -0.25796806  0.53130140 0.8837597\nF.+sugar-F. open     -0.72233333 -1.07530537 -0.36936130 0.0001312\nF.+CaCO3-F. open     -0.44600000 -0.79897203 -0.09302797 0.0102557\nBas.med.-F. open     -0.06433333 -0.41730537  0.28863870 0.9943994\nA.dest-F. open       -0.76500000 -1.11797203 -0.41202797 0.0000721\nTap water-F. open    -0.77833333 -1.17296806 -0.38369860 0.0001913\nF.+CaCO3-F.+sugar     0.27633333 -0.07663870  0.62930537 0.1727182\nBas.med.-F.+sugar     0.65800000  0.30502797  1.01097203 0.0003363\nA.dest-F.+sugar      -0.04266667 -0.39563870  0.31030537 0.9994197\nTap water-F.+sugar   -0.05600000 -0.45063473  0.33863473 0.9985686\nBas.med.-F.+CaCO3     0.38166667  0.02869463  0.73463870 0.0307459\nA.dest-F.+CaCO3      -0.31900000 -0.67197203  0.03397203 0.0879106\nTap water-F.+CaCO3   -0.33233333 -0.72696806  0.06230140 0.1247914\nA.dest-Bas.med.      -0.70066667 -1.05363870 -0.34769463 0.0001792\nTap water-Bas.med.   -0.71400000 -1.10863473 -0.31936527 0.0004507\nTap water-A.dest     -0.01333333 -0.40796806  0.38130140 0.9999997",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#graphical-output",
    "href": "slides/07-anova.html#graphical-output",
    "title": "07-One and two-way ANOVA",
    "section": "Graphical output",
    "text": "Graphical output\n\npar(las = 1)             # las = 1 make y annotation horizontal\npar(mar = c(4, 10, 3, 1)) # more space at the left for axis annotation\nplot(tk)",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#anova-assumptions-and-diagnostics",
    "href": "slides/07-anova.html#anova-assumptions-and-diagnostics",
    "title": "07-One and two-way ANOVA",
    "section": "ANOVA assumptions and diagnostics",
    "text": "ANOVA assumptions and diagnostics\n\nANOVA has same assumptions as the linear model.\n\n\nIndependence of errors\nVariance homogeneity\nApproximate normality of errors\n\nGraphical checks are preferred.\n\n\npar(mfrow=c(2, 2))\nplot(m)",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#numerical-tests",
    "href": "slides/07-anova.html#numerical-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Numerical tests",
    "text": "Numerical tests\n\n\nTest variance homogeneity\n\nF-test compares only two variances.\nSeveral tests for multiple variances, e.g. Bartlett, Levene, Fligner-Killeen\nRecommended: Fligner-Killeen-test\n\n\nfligner.test(growth ~ treat, \n             data = algae)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  growth by treat\nFligner-Killeen:med chi-squared = 4.2095, df = 6, p-value = 0.6483\n\n\n\nTest of normal distribution\n\nThe Shapiro-Wilks test can be misleading.\nUse a graphical method!\n\n\nqqnorm(residuals(m))\nqqline(residuals(m))",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#one-way-anova-with-heterogeneous-variances",
    "href": "slides/07-anova.html#one-way-anova-with-heterogeneous-variances",
    "title": "07-One and two-way ANOVA",
    "section": "One-way ANOVA with heterogeneous variances",
    "text": "One-way ANOVA with heterogeneous variances\n\n\nextension of the Welch test for \\(\\ge 2\\) samples\nin R called oneway.test\n\n\noneway.test(growth ~ treat, data = algae)\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  growth and treat\nF = 115.09, num df = 6.0000, denom df = 4.6224, p-value = 6.57e-05",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#two-way-anova",
    "href": "slides/07-anova.html#two-way-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Two-way ANOVA",
    "text": "Two-way ANOVA\n\nExample from a statistics text book (Crawley, 2002), applied to a new context\nEffects of fertilizer and light regime on growth of plant height in cm per time\n\n\n\n\nfertilizer\nhigh light\nlow light\n\n\n\n\nA\n8.3\n6.6\n\n\nA\n8.7\n7.2\n\n\nB\n8.1\n6.9\n\n\nB\n8.5\n8.3\n\n\nC\n9.1\n7.9\n\n\nC\n9.0\n9.2\n\n\n\n\n\n\n\n\n\nfactorial experiment (with replicates): each factor combination has more than one observation.\nwithout replication:\n\nno replicates per factor combination\nthis is possible, but does not allow identification of interactions",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#enter-data-in-long-format",
    "href": "slides/07-anova.html#enter-data-in-long-format",
    "title": "07-One and two-way ANOVA",
    "section": "Enter data in long format",
    "text": "Enter data in long format\n\n\n\nplants &lt;- data.frame(No = 1:12,\n                    growth = c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,\n                               8.3, 8.7, 8.1, 8.5, 9.1, 9.0),\n                    fert   = rep(c(\"A\", \"B\", \"C\"), each=2),\n                    light   = rep(c(\"low\", \"high\"), each=6)\n          )\n\n\n\n\n\n\n\nNo\ngrowth\nfert\nlight\n\n\n\n\n1\n6.6\nA\nlow\n\n\n2\n7.2\nA\nlow\n\n\n3\n6.9\nB\nlow\n\n\n4\n8.3\nB\nlow\n\n\n5\n7.9\nC\nlow\n\n\n6\n9.2\nC\nlow\n\n\n7\n8.3\nA\nhigh\n\n\n8\n8.7\nA\nhigh\n\n\n9\n8.1\nB\nhigh\n\n\n10\n8.5\nB\nhigh\n\n\n11\n9.1\nC\nhigh\n\n\n12\n9.0\nC\nhigh",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#model-formula-examples",
    "href": "slides/07-anova.html#model-formula-examples",
    "title": "07-One and two-way ANOVA",
    "section": "Model formula examples",
    "text": "Model formula examples\n\n\n\n\n\n\n\nModel Type\nFormula\n\n\n\n\nNull model\ny ~ 1\n\n\nSimple linear regression\ny ~ x\n\n\nLinear model without intercept\ny ~ x - 1\n\n\nMultiple regression, no interaction\ny ~ x1 + x2 + x3\n\n\nMultiple regression with interaction\ny ~ x1 * x2 * x3\n\n\nMultiple regression, no 3x interaction\ny ~ x1 * x2 * x3 - x1 : x2 : x3\n\n\nTransformed with ‘as is’ function\ny ~ x + I(x^2)\n\n\nOne way ANOVA\ny ~ f\n\n\nANOVA with interaction\ny ~ f1 * f2\n\n\nANCOVA with Interaction\ny ~ x * f\n\n\nNested ANOVA\ny ~ x + (1 | a / b)\n\n\nGAM with smoother s\ny ~ s(x) + f\n\n\n\n\n\n\n\n\ny = response variable (dependent, target)\nx = metric explanation variable (predictor, independent); f = factor variable (nominal)",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#linear-model-and-anova",
    "href": "slides/07-anova.html#linear-model-and-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Linear model and ANOVA",
    "text": "Linear model and ANOVA\n\nANOVA\n\nm &lt;- lm(growth ~ light * fert, data = plants)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: growth\n           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  \nlight       1 2.61333 2.61333  7.2258 0.03614 *\nfert        2 2.66000 1.33000  3.6774 0.09069 .\nlight:fert  2 0.68667 0.34333  0.9493 0.43833  \nResiduals   6 2.17000 0.36167                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#interaction-plot",
    "href": "slides/07-anova.html#interaction-plot",
    "title": "07-One and two-way ANOVA",
    "section": "Interaction plot",
    "text": "Interaction plot\n\nwith(plants, interaction.plot(fert, light, growth, \n                            col = c(\"orange\", \"brown\"), lty = 1, lwd = 2))",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#diagnostics",
    "href": "slides/07-anova.html#diagnostics",
    "title": "07-One and two-way ANOVA",
    "section": "Diagnostics",
    "text": "Diagnostics\nAssumptions\n\nIndependence of measurements (within samples)\nVariance homogeneity of residuals\nNormal distribution of residuals\n\n Test of assumptions needs residuals of the fitted model.  \\(\\Rightarrow\\) Fit the ANOVA model first, then check if it was correct!\n\nDiagnostic tools\n\nBox plot\nPlot of residuals vs. mean values\nQ-Q-plot of residuals\nFligner-Killeen test (alternative: some people recommend the Levene-Test)",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#diagnostics-ii",
    "href": "slides/07-anova.html#diagnostics-ii",
    "title": "07-One and two-way ANOVA",
    "section": "Diagnostics II",
    "text": "Diagnostics II\n\npar(mfrow=c(1, 2))\npar(cex=1.2, las=1)\nqqnorm(residuals(m))\nqqline(residuals(m))\n\nplot(residuals(m)~fitted(m))\nabline(h=0)\n\n\n\nfligner.test(growth ~ interaction(light, fert), data=plants)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  growth by interaction(light, fert)\nFligner-Killeen:med chi-squared = 10.788, df = 5, p-value = 0.05575\n\n\nResiduals: look ok and p-value of the Fligner test \\(&gt; 0.05\\), \\(\\rightarrow\\) looks fine.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#notes",
    "href": "slides/07-anova.html#notes",
    "title": "07-One and two-way ANOVA",
    "section": "Notes",
    "text": "Notes\nLinear regression or ANOVA?\n\nessentially the same\nindependent variables are metric: linear model\nindependent variables are nominal (= factor): ANOVA\nmix of metric and nominal variables: ANCOVA\n\nUse of pre-tests\nPre-tests are in general questionable for theoretical reasons:\n\nThe null hypotheses \\(H_0\\) can only be rejected and not ultimately confirmed.\nIf the sample size is large, normality of residuals is not required\nIf \\(p\\) is close to the threshold and sample size is small, we would be left in uncertainty.\n\nAll this can only be overcome with careful thinking and some experience.\nIt is always a good idea to discuss results with colleagues and supervisors.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#sequential-holm-bonferroni-method",
    "href": "slides/07-anova.html#sequential-holm-bonferroni-method",
    "title": "07-One and two-way ANOVA",
    "section": "Sequential Holm-Bonferroni method",
    "text": "Sequential Holm-Bonferroni method\n\nAlso called Holm procedure (Holm, 1979)\nEasy to use\nCan be applied to any multiple test problem\nLess conservative that ordinary Bonferroni correction, but …\n… still a very conservative approach\nsee also Wikipedia\n\nAlgorithm\n\nSelect smallest \\(p\\) out of all \\(n\\) \\(p\\)-values\nIf \\(p \\cdot n &lt; \\alpha\\) \\(\\Rightarrow\\) significant, else STOP\nSet \\(n − 1 \\rightarrow n\\), remove smallest \\(p\\) from the list and go to step 1.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#example-1",
    "href": "slides/07-anova.html#example-1",
    "title": "07-One and two-way ANOVA",
    "section": "Example",
    "text": "Example\n\nGrowth rate per day (\\(d^{-1}\\)) of blue-green algae cultures (Pseudanabaena) after adding toxic peptides from another blue-green algae (Microcystis).\nThe original hypothesis was that Microcystin LR (MCYST) or a derivative of it (Substance A) inhibits growth.\n\n\nmcyst &lt;-  data.frame(treat = factor(c(rep(\"Control\", 5),\n                                       rep(\"MCYST\", 5),\n                                       rep(\"Subst A\", 5)),\n                                levels=c(\"Control\", \"MCYST\", \"Subst A\")),\n                      mu   = c(0.086, 0.101, 0.086, 0.086, 0.099,\n                               0.092, 0.088, 0.093, 0.088, 0.086,\n                               0.095, 0.102, 0.106, 0.106, 0.106)\n                     )",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#approach-1-one-way-anova",
    "href": "slides/07-anova.html#approach-1-one-way-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Approach 1: one-way ANOVA",
    "text": "Approach 1: one-way ANOVA\n\npar(mar=c(4, 8, 2, 1), las=1)\nm &lt;- lm(mu ~ treat, data=mcyst)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: mu\n          Df     Sum Sq    Mean Sq F value   Pr(&gt;F)   \ntreat      2 0.00053293 2.6647e-04   8.775 0.004485 **\nResiduals 12 0.00036440 3.0367e-05                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(TukeyHSD(aov(m)))",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#approach-2-multiple-t-tests-with-sequential-bonferroni-correction",
    "href": "slides/07-anova.html#approach-2-multiple-t-tests-with-sequential-bonferroni-correction",
    "title": "07-One and two-way ANOVA",
    "section": "Approach 2: multiple t-tests with sequential Bonferroni correction",
    "text": "Approach 2: multiple t-tests with sequential Bonferroni correction\nWe separate the data set in single subsets:\n\nControl &lt;- mcyst$mu[mcyst$treat == \"Control\"]\nMCYST   &lt;- mcyst$mu[mcyst$treat == \"MCYST\"]\nSubstA  &lt;- mcyst$mu[mcyst$treat == \"Subst A\"]\n\nand perform 3 t-Tests:\n\np1 &lt;- t.test(Control, MCYST)$p.value\np2 &lt;- t.test(Control, SubstA)$p.value\np3 &lt;- t.test(MCYST, SubstA)$p.value\n\nThe following shows the raw p-values without correction:\n\nc(p1, p2, p3)\n\n[1] 0.576275261 0.027378832 0.001190592\n\n\n… and with Holm correction:\n\np.adjust(c(p1, p2, p3))\n\n[1] 0.576275261 0.054757664 0.003571775",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#conclusions",
    "href": "slides/07-anova.html#conclusions",
    "title": "07-One and two-way ANOVA",
    "section": "Conclusions",
    "text": "Conclusions\nStatistical methods\n\nIn case of Holm-corrected t-tests, only a single p-value (MCYST vs. Subst A) remains significant. This indicates that in this case, Holm’s method is more conservative than TukeyHSD (only one compared to two significant) effects.\nAn ANOVA with posthoc test is in general preferred,\nbut the sequential Holm-Bonferroni can be helpful in special cases.\nMoreover, it demonstrates clearly that massive multiple testing needs to be avoided.\n\n\\(\\Rightarrow\\) ANOVA is to be preferred, when possible.\nInterpretation\n\nRegarding our original hypothesis, we can see that MCYST and SubstA did not inhibit growth of Pseudanabaena. In fact SubstA stimulated growth.\nThis was contrary to our expectations – the biological reason was then found 10 years later.\n\nMore about this can be found in Jähnichen et al. (2001), Jähnichen et al. (2007), Jähnichen et al. (2011), Zilliges et al. (2011) or Dziallas & Grossart (2011).",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#ancova",
    "href": "slides/07-anova.html#ancova",
    "title": "07-One and two-way ANOVA",
    "section": "ANCOVA",
    "text": "ANCOVA\nStatistical question\n\nComparison of regression lines\nSimilar to ANOVA, but contains also metric variables (covariates)\n\nExample\nAnnette Dobson’s birthweight data. A data set from a statistics textbook (Dobson, 2013), birth weight of boys and girls in dependence of the pregnancy week.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#the-birthweight-data-set",
    "href": "slides/07-anova.html#the-birthweight-data-set",
    "title": "07-One and two-way ANOVA",
    "section": "The birthweight data set",
    "text": "The birthweight data set\n\nThe data set is found at different places on the internet and in different versions.\nHere the version that is found in an R demo: demo(lm.glm)\n\n## Birth Weight Data see stats/demo/lm.glm.R\ndobson &lt;- data.frame(\n  week = c(40, 38, 40, 35, 36, 37, 41, 40, 37, 38, 40, 38,\n     40, 36, 40, 38, 42, 39, 40, 37, 36, 38, 39, 40),\n  weight = c(2968, 2795, 3163, 2925, 2625, 2847, 3292, 3473, 2628, 3176,\n        3421, 2975, 3317, 2729, 2935, 2754, 3210, 2817, 3126, 2539,\n        2412, 2991, 2875, 3231),\n  gender = gl(2, 12, labels=c(\"M\", \"F\"))\n)\n\n\nNote: This is an artificial data set, not the reality.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#anette-dobsons-birthweight-data",
    "href": "slides/07-anova.html#anette-dobsons-birthweight-data",
    "title": "07-One and two-way ANOVA",
    "section": "Anette Dobson’s birthweight data",
    "text": "Anette Dobson’s birthweight data\nWhy not just using a t-test?\n\nboxplot(weight ~ gender,data = dobson, ylab = \"weight\")\n\nt.test(weight ~ gender, data = dobson, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  weight by gender\nt = 0.97747, df = 22, p-value = 0.339\nalternative hypothesis: true difference in means between group M and group F is not equal to 0\n95 percent confidence interval:\n -126.3753  351.7086\nsample estimates:\nmean in group M mean in group F \n       3024.000        2911.333 \n\n\nThe box plot shows much overlap and the difference is not significant, because the t-test ignores important information: the pregnancy week.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#ancova-makes-use-of-covariates",
    "href": "slides/07-anova.html#ancova-makes-use-of-covariates",
    "title": "07-One and two-way ANOVA",
    "section": "ANCOVA makes use of covariates",
    "text": "ANCOVA makes use of covariates\n\nm &lt;- lm(weight ~ week * gender, data = dobson)\nanova(m)\n\nAnalysis of Variance Table\n\nResponse: weight\n            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nweek         1 1013799 1013799 31.0779 1.862e-05 ***\ngender       1  157304  157304  4.8221   0.04006 *  \nweek:gender  1    6346    6346  0.1945   0.66389    \nResiduals   20  652425   32621                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#pitfalls-of-anova-and-ancova-described-so-far",
    "href": "slides/07-anova.html#pitfalls-of-anova-and-ancova-described-so-far",
    "title": "07-One and two-way ANOVA",
    "section": "Pitfalls of ANOVA and ANCOVA described so far",
    "text": "Pitfalls of ANOVA and ANCOVA described so far\n\n\nHeterogeneity of variance\n\np-values can be biased (i.e. misleading or wrong)\nuse of a one-way ANOVA for uneaqual variances (in R: oneway.test)\n\nUnbalanced case: unequal number of samples for each factor combination \\(\\rightarrow\\) ANOVA results depend on the order of factors in the model formula.\n\nClassical method: Type II or Type III ANOVA\nModern approach: model selection and likelihood ratio tests",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#type-ii-and-type-iii-anova",
    "href": "slides/07-anova.html#type-ii-and-type-iii-anova",
    "title": "07-One and two-way ANOVA",
    "section": "Type II and type III ANOVA",
    "text": "Type II and type III ANOVA\n\n\nfunction Anova (with upper case A) in package car\nHelp file of function Anova:\n\n\n“Type-II tests are calculated according to the principle of marginality, testing each term after all others, except ignoring the term’s higher-order relatives; so-called type-III tests violate marginality, testing each term in the model after all of the others.”\n\n\nConclusion: Use type II and not type III.\nDon’t try to interpret single terms in case of significant interactions.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#type-ii-anova-example",
    "href": "slides/07-anova.html#type-ii-anova-example",
    "title": "07-One and two-way ANOVA",
    "section": "Type II ANOVA: Example",
    "text": "Type II ANOVA: Example\n\n\nlibrary(\"car\")\nm &lt;- lm(growth ~ light * fert, data = plants)\nAnova(m, type=\"II\")\n\nAnova Table (Type II tests)\n\nResponse: growth\n            Sum Sq Df F value  Pr(&gt;F)  \nlight      2.61333  1  7.2258 0.03614 *\nfert       2.66000  2  3.6774 0.09069 .\nlight:fert 0.68667  2  0.9493 0.43833  \nResiduals  2.17000  6                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#selection-of-an-optimal-model-from-a-set-of-candidates",
    "href": "slides/07-anova.html#selection-of-an-optimal-model-from-a-set-of-candidates",
    "title": "07-One and two-way ANOVA",
    "section": "Selection of an optimal model from a set of candidates",
    "text": "Selection of an optimal model from a set of candidates\n\nProblem:\n\nIn complex ANOVA models, p-values depend on number (and sometimes of order) of included factors and interactions.\nThe \\(H_0\\)-based approach becomes confusing, e.g. because of contradictory p-values.\n\nAlternative approach:\n\nEmploys the principle of parsimony\n\nInstead of p-value based testing, comparison of different model candidates:\n\nModel with all potentiall effects → full model\nOmit single factors → reduced models (several!)\nNo influence factors (ony mean value) → null model\nWhich model is the best → minimal adequate model?",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#how-can-we-measure-which-model-is-the-best",
    "href": "slides/07-anova.html#how-can-we-measure-which-model-is-the-best",
    "title": "07-One and two-way ANOVA",
    "section": "How can we measure which model is the best?",
    "text": "How can we measure which model is the best?\n\nCompromise between model fit and model complexity (number of parameters, k).\n\nGoodness of fit: Likelihood L (measures how good the data match a given model).\nLog Likelihood: makes the criterion additive.\nAIC (Akaike Information Criterion):\n\n\\[AIC = −2 \\ln(L) + 2k\\]\n\nBIC (Bayesian Information Criterion), takes sample size into account (\\(n\\)):\n\n\\[BIC = −2 \\ln(L) + k · \\ln(n)\\]\nThe model with the smallest AIC (or BIC) is the minimal adequate (i.e. optimal) model.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#model-selection-and-likelihood-ratio-tests",
    "href": "slides/07-anova.html#model-selection-and-likelihood-ratio-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Model Selection and Likelihood Ratio Tests",
    "text": "Model Selection and Likelihood Ratio Tests\nApproach\n\nFit several models individually\nCompare the models pairwise with ANOVA (likelihood ratio test)\n\nData and example\n\nplants &lt;- data.frame(No=1:12,\n                   growth=c(6.6, 7.2, 6.9, 8.3, 7.9, 9.2,\n                            8.3, 8.7, 8.1, 8.5, 9.1, 9.0),\n                   fert= rep(c(\"A\", \"B\", \"C\"), each=2),\n                   light= rep(c(\"low\", \"high\"), each=6)\n                   )\n\n\nm3 &lt;- lm(growth ~ fert * light, data=plants)  # f1 + f2 + f1:f2\nm2 &lt;- lm(growth ~ fert + light, data=plants)  # f1 + f2\nanova(m3, m2)\n\nAnalysis of Variance Table\n\nModel 1: growth ~ fert * light\nModel 2: growth ~ fert + light\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1      6 2.1700                           \n2      8 2.8567 -2  -0.68667 0.9493 0.4383\n\n\n\nLikelihood ratio test compares two models (anova with &gt; 1 model)\nModel with interaction (m3) not significantly better than model without interaction (m2).",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#aic-based-model-selection",
    "href": "slides/07-anova.html#aic-based-model-selection",
    "title": "07-One and two-way ANOVA",
    "section": "AIC-based model selection",
    "text": "AIC-based model selection\n\npairwise model comparison is cumbersome, especially for large number of models.\nSolution: Create set of candidate models\nuse smallest AIC to select the minimal adequate model.\n\n\nm3  &lt;- lm(growth ~ light * fert, data = plants) # full model\nm2  &lt;- lm(growth ~ light + fert, data = plants)\nm1a &lt;- lm(growth ~ fert, data = plants)\nm1b &lt;- lm(growth ~ light, data = plants)\nm0  &lt;- lm(growth ~ 1, data = plants)            # null model\n\nAIC(m0, m1a, m1b, m2, m3)\n\n    df      AIC\nm0   2 33.38238\nm1a  4 32.62699\nm1b  3 30.72893\nm2   5 26.83151\nm3   7 27.53237\n\n\nNote\n\nAIC values are defined up to an additive constant\n\\(\\rightarrow\\) absolute values differ sometimes, depending on the applied method\n\\(\\Rightarrow\\) look at range of AIC and differences, don’t care of absolute values\nrule of thumb: the “AIC unit” is 2, differences \\(\\approx 2.0\\rightarrow\\) minor importance",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#stepwise-model-selection-automatic",
    "href": "slides/07-anova.html#stepwise-model-selection-automatic",
    "title": "07-One and two-way ANOVA",
    "section": "Stepwise model selection (automatic)",
    "text": "Stepwise model selection (automatic)\n\nThe full model is supplied to the step function:\n\n\n\nm1 &lt;- lm(growth ~ fert * light, data=plants)\nopt &lt;- step(m1)\n\nStart:  AIC=-8.52\ngrowth ~ fert * light\n\n             Df Sum of Sq    RSS     AIC\n- fert:light  2   0.68667 2.8567 -9.2230\n&lt;none&gt;                    2.1700 -8.5222\n\nStep:  AIC=-9.22\ngrowth ~ fert + light\n\n        Df Sum of Sq    RSS     AIC\n&lt;none&gt;               2.8567 -9.2230\n- fert   2    2.6600 5.5167 -5.3256\n- light  1    2.6133 5.4700 -3.4275\n\n\n\nModel with the smallest AIC \\(\\rightarrow\\) optimal model.\n\n\n\nanova(opt)\n\nAnalysis of Variance Table\n\nResponse: growth\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)  \nfert       2 2.6600 1.33000  3.7246 0.07190 .\nlight      1 2.6133 2.61333  7.3186 0.02685 *\nResiduals  8 2.8567 0.35708                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\np &lt; 0.05  \\(\\rightarrow\\) significant\n\nResults of the example:\n\noptimal model (m2, opt), includes both factors fert and light but no interaction.\n\\(\\Rightarrow\\) Model selection identified fert and light as necessary explanatory variables, in contrast to the classical ANOVA table where only light is significant.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#significance-tests",
    "href": "slides/07-anova.html#significance-tests",
    "title": "07-One and two-way ANOVA",
    "section": "Significance tests?",
    "text": "Significance tests?\n\n\nThe concept of model selection supercedes p-value-based statistics.\nSome authors generally discourage to use p-values in this context, others recommend a compromize.\nIf you want to get a p-value, compare the optimal model with further reduced models, but still interpret the p-values with care:\n\n\nanova(m2, m1a) # fert\nanova(m2, m1b) # light\n\n\nIn any case: focus on practical implications and don’t forget to report the effect sizes!",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#summary-of-the-anova-chapter",
    "href": "slides/07-anova.html#summary-of-the-anova-chapter",
    "title": "07-One and two-way ANOVA",
    "section": "Summary of the ANOVA chapter",
    "text": "Summary of the ANOVA chapter\n\nLinear models form the basis of many statistical methods\n\nLinear regression\nANOVA, ANCOVA, GLM, GAM, GLMM, . . .\nANOVA/ANCOVA instead of multiple testing\n\nANOVA is more powerful than multiple tests:\n\navoids \\(\\alpha\\)-error inflation\none big experiment needs less n than many small experiments\nidentification of interaction effects\nelimination of co-variates\n\nModel selection vs. p-value based testing\n\nparadigm shift in statistics: AIC instead of p-value\nmore reliable, especially for imbalanced or complex designs\nextensible to generalized, additive, and mixed models (GLM, GAM, LME, GLMM, …)\nbut: p-value based tests are sometimes easier to understand",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#avoid-p-value-hacking",
    "href": "slides/07-anova.html#avoid-p-value-hacking",
    "title": "07-One and two-way ANOVA",
    "section": "Avoid p-value hacking",
    "text": "Avoid p-value hacking\nDo NOT repeat experiments until a significant p-value is found.\nThe high-ranked journal “… Nature asked influential statisticians to recommend one change to improve science. The common theme? The problem is not our maths, but ourselves.” (Leek et al. (2017)):\nFive ways to fix statistics. Comment on Nature\n\nJeff Leek: Adjust for human cognition\nBlakeley B. McShane & Andrew Gelman: Abandon statistical significance\nDavid Colquhoun: State false-positive risk, too\nMichèle B. Nuijten: Share analysis plans and results\nSteven N. Goodman: Change norms from within\n\n\nSelf study\nRead the paper of Johnson & Omland (2004) to gain more understanding of the model selection paradigm.",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/07-anova.html#bibliography",
    "href": "slides/07-anova.html#bibliography",
    "title": "07-One and two-way ANOVA",
    "section": "Bibliography",
    "text": "Bibliography\n\n\n\n\nCrawley, M. J. (2002). Statistical computing. An introduction to data analysis using S-PLUS (pp. 1–761). Wiley. datasets: http://www.bio.ic.ac.uk/research/mjcraw/statcomp/data/\n\n\nDobson, A. J. (2013). Introduction to statistical modelling. Springer.\n\n\nDziallas, C., & Grossart, H.-P. (2011). Increasing Oxygen Radicals and Water Temperature Select for Toxic Microcystis sp. PLoS ONE, 6(9), e25569. https://doi.org/10.1371/journal.pone.0025569\n\n\nHolm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics, 65–70. https://www.jstor.org/stable/4615733\n\n\nJähnichen, S., Ihle, T., Petzoldt, T., & Benndorf, J. (2007). Impact of Inorganic Carbon Availability on Microcystin Production by Microcystis aeruginosa PCC 7806. Applied and Environmental Microbiology, 73(21), 6994–7002. https://doi.org/10.1128/AEM.01253-07\n\n\nJähnichen, S., Long, B. M., & Petzoldt, T. (2011). Microcystin production by Microcystis aeruginosa: Direct regulation by multiple environmental factors. Harmful Algae, 12, 95–104. https://doi.org/10.1016/j.hal.2011.09.002\n\n\nJähnichen, S., Petzoldt, T., & Benndorf, J. (2001). Evidence for control of microcystin dynamics in Bautzen Reservoir (Germany) by cyanobacterial population growth rates and dissolved inorganic carbon. Fundamental and Applied Limnology, 150(2), 177–196. https://doi.org/10.1127/archiv-hydrobiol/150/2001/177\n\n\nJohnson, G., Jerald, & Omland, K. S. (2004). Model Selection in Ecology and Evolution. Trends in Ecology and Evolution, 19(2), 101–108. https://doi.org/10.1016/j.tree.2003.10.013\n\n\nZilliges, Y., Kehr, J.-C., Meissner, S., Ishida, K., Mikkat, S., Hagemann, M., Kaplan, A., Börner, T., & Dittmann, E. (2011). The Cyanobacterial Hepatotoxin Microcystin Binds to Proteins and Increases the Fitness of Microcystis under Oxidative Stress Conditions. PLoS ONE, 6(3), e17615. https://doi.org/10.1371/journal.pone.0017615",
    "crumbs": [
      "Basic Statistics",
      "07-One and two-way ANOVA"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#statistical-test",
    "href": "slides/05-classtests.html#statistical-test",
    "title": "05-Classical Tests",
    "section": "Statistical test",
    "text": "Statistical test\n\nA statistical hypothesis test is a method of statistical inference.\n\nCommonly, two samples are compared, or a sample is compared against properties from an idealized model.\nA hypothesis \\(H_a\\) for the statistical relationship between the two data sets, is compared to an idealized null hypothesis \\(H_0\\) that proposes no relationship between two data sets.\nThe comparison is considered statistically significant if the relationship between the data sets would be an unlikely realization of the null hypothesis according to a threshold probability – the significance level.\n\nadapted from: https://en.wikipedia.org/wiki/Statistical hypothesis testing",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#effect-size-and-significance",
    "href": "slides/05-classtests.html#effect-size-and-significance",
    "title": "05-Classical Tests",
    "section": "Effect size and significance",
    "text": "Effect size and significance\n\nIn case of relative mean differences, the relative effect size is:\n\n\\[\n  \\delta = \\frac{\\bar{\\mu}_1-\\bar{\\mu}_2}{\\sigma}=\\frac{\\Delta}{\\sigma}\n\\]\n\nwith:\n\nmean values of two populations \\(\\mu_1, \\mu_2\\)\neffect size \\(\\Delta\\)\nrelative effect size \\(\\delta\\) (also called Cohen’s d)\nsignificance means that an observed effect is unlikely the result of pure random variation.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#null-hypothesis-and-alternative-hypothesis",
    "href": "slides/05-classtests.html#null-hypothesis-and-alternative-hypothesis",
    "title": "05-Classical Tests",
    "section": "Null hypothesis and alternative hypothesis",
    "text": "Null hypothesis and alternative hypothesis\n\n\\(H_0\\) null hypothesis: two populations are not different with respect to a certain property.\n\nAssumption: observed effect occured purely at random, true effect is zero.\n\n\\(H_a\\) alternative hypothesis (experimental hypothesis): existence of a certain effect.\n\nAn alternative hypothesis is never completely true or “proven”.\nAcceptance of \\(H_A\\) means only than \\(H_0\\) is unlikely.\n\n“Not significant” means either no effect or sample size too small!\n\nNote: Different meaning of significance (\\(H_0\\) unlikely) and relevance (effect large enough to play a role in practice).",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#the-p-value",
    "href": "slides/05-classtests.html#the-p-value",
    "title": "05-Classical Tests",
    "section": "The p-value",
    "text": "The p-value\n\nThe interpretation of the p-value was often confused in the past, even in statistics textbooks, so it is good to refer to a clear definition:\n\n\nThe p-value is defined as the probability of obtaining a result equal to or ‘more extreme’ than what was actually observed, when the null hypothesis is true.\n\n\n\n\nhttps://en.wikipedia.org/wiki/P-value:\nHubbard (2004) Alphabet Soup: Blurring the Distinctions Between p’s and a’s in Psychological Research, Theory Psychology 14(3), 295-327. DOI: 10.1177/0959354304043638",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#alpha-and-beta-errors",
    "href": "slides/05-classtests.html#alpha-and-beta-errors",
    "title": "05-Classical Tests",
    "section": "Alpha and beta errors",
    "text": "Alpha and beta errors\n\n\n\n\n\n\n\n\n\nReality\nDecision of the test\ncorrect?\nprobability\n\n\n\n\n\\(H_0\\) = true\nsignificant\nno\n\\(\\alpha\\)-error\n\n\n\\(H_0\\) = false\nnot significant\nno\n\\(\\beta\\)-error\n\n\n\\(H_0\\) = true\nnot significant\nyes\n\\(1-\\alpha\\)\n\n\n\\(H_0\\) = false\nsignificant\nyes\n\\(1-\\beta\\) (power)\n\n\n\n\n\n\n\n\n\n1.\\(H_0\\) falsely rejected (error of the first kind or \\(\\alpha\\)-error)\n\nwe claim an effect, that does not exist, e.g. a drug with no effect\n\n2.\\(H_0\\) falsely retained (error of the second kind or \\(\\beta\\)-error)\n\ntypical case in small studies, where effect was not enough to detect existing effects\n\nUse in practice\n\ncommon convention in environmental sciences: \\(\\alpha=0.05\\), must be set beforehand\n\\(\\beta=f(\\alpha, \\text{effectsize}, \\text{sample size}, \\text{kind of test})\\), should be \\(\\le 0.2\\)",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#significance-and-relevance",
    "href": "slides/05-classtests.html#significance-and-relevance",
    "title": "05-Classical Tests",
    "section": "Significance and relevance",
    "text": "Significance and relevance\n\nSignificance is not the only important. Focus also on effect size and relevance!\n\nStatistical significance means that the null hypothesis \\(H_0\\) is unlikely in a statistical sense.\nPractical relevance (sometimes called “practical significance”) means that the effect size is large enough to play a role in practice.\n\nThis means that whether an effect can be relevant or not depends on its effect size and the field of application.\nLet’s for example consider a vaccination. If a vaccine had a significant effect in a clinical test, but protected only 10 out of 1000 people, one would not consider this effect as relevant and not produce this vaccine.\nOn the other hand, even small effects can be relevant. So if a toxic substance would have an effect on 1 out of 1000 people to produce cancer, we would consider this as relevant. To detect this as a significant effect would need an epidemiological study with a large number of people. But as it is highly relevant, it is worth the effort.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#take-home-messages",
    "href": "slides/05-classtests.html#take-home-messages",
    "title": "05-Classical Tests",
    "section": "Take home messages",
    "text": "Take home messages\n\nA p-value measures the probability that a purely random effect would be equally or more extreme than an observed effect if the null hypothesis is true.\nSignificant means the results are unlikely if there were no real effect.\nNot significant doesn’t mean “no effect”.\nNon-significant results suggest the need for further research, e.g.:\n\nincrease sample size\nincrease experimental effect\nreduce experimental error\nconsider a more powerful statistical procedure\n\nDon’t focus on p-values alone. Never forget to report also sample size, effect size and relevance of your results.\nWith large datasets:\n\nstatistically significant results can easily be obtained even for very small and practically irrelevant effects.\n\\(\\rightarrow\\) effect size and relevance become more important than p-values.\n\n\nThe p-value remains an important tool in statistics, but misuse can lead to misinterpretation.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#one-sample-t-test",
    "href": "slides/05-classtests.html#one-sample-t-test",
    "title": "05-Classical Tests",
    "section": "One sample t-Test",
    "text": "One sample t-Test\n\n\ntests if a sample is from a population with given mean value \\(\\mu\\)\nbased on checking if the population mean \\(\\mu\\) is in the confidence interval of \\(\\bar{x}\\)\n\n\nLet’s assume a sample of size with \\(n=10, \\bar{x}=5.5, s=1\\) and \\(\\mu=5\\).\nEstimate the 95% confidence interval of \\(\\bar{x}\\):\n\n\\[\nCI = \\bar{x} \\pm t_{1-\\alpha/2, n-1} \\cdot s_{\\bar{x}}\n\\] with \\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\qquad \\text{(standard error)}\n\\]\nDifferent ways of calculation shown at the next slides",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#remember-standard-deviation-and-standard-error",
    "href": "slides/05-classtests.html#remember-standard-deviation-and-standard-error",
    "title": "05-Classical Tests",
    "section": "Remember: standard deviation and standard error",
    "text": "Remember: standard deviation and standard error\n\n Visualization of a one-sample t-test. Left: original distribution of the data measured by standard deviation, right: distribution of mean values, measured by its standard error. \n\\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}} \\qquad \\text{(standard error)}\n\\]\n\nstandard error &lt; standard deviation\nmeasures precision of the mean value\nCLT!\n\nThe test is based on the distribution of the means, not distribution of original data.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#method-1-is-mu-in-the-confidence-interval",
    "href": "slides/05-classtests.html#method-1-is-mu-in-the-confidence-interval",
    "title": "05-Classical Tests",
    "section": "Method 1: Is \\(\\mu\\) in the confidence interval?",
    "text": "Method 1: Is \\(\\mu\\) in the confidence interval?\n\n\nSample: \\(n=10, \\bar{x}=5.5, s=1\\) and \\(\\mu=5\\)\nLet \\(\\alpha = 0.05\\), we get a two-sided 95% confidence interval with:\n\n\\[\\bar{x} \\pm t_{0,975, n-1} \\cdot \\frac{s}{\\sqrt{n}}\\]\n\n\n5.5 + c(-1, 1) * qt(0.975, 10-1) * 1/sqrt(10)\n\n[1] 4.784643 6.215357\n\n\n\n\n\nCheck if \\(\\mu=5.0\\) is in this interval?\nYes, it is inside \\(\\Rightarrow\\) difference not significant.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#method-2-comparison-with-a-tabulated-t-value",
    "href": "slides/05-classtests.html#method-2-comparison-with-a-tabulated-t-value",
    "title": "05-Classical Tests",
    "section": "Method 2: Comparison with a tabulated t-value",
    "text": "Method 2: Comparison with a tabulated t-value\n\nRearrange the equation of the confidence interval, to calculate an observed \\(t_{obs}\\)\n\n\\[\nt_{obs} = |\\bar{x}-\\mu | \\cdot \\frac{1}{s_{\\bar{x}}} = \\frac{|\\bar{x}-\\mu |}{s} \\cdot \\sqrt{n} = \\frac{|5.5 -5.0|}{1.0} \\cdot \\sqrt{10}\n\\]\nWe can calculate this in R:\n\nt &lt;- abs(5.5 - 5.0) / 1.0 * sqrt(10)\nt\n\n[1] 1.581139\n\n\n\nCompare \\(t_{obs}\\) with a tabulated value\n\n\n“Old style”: find critical t-value in a table for given \\(\\alpha\\) and degrees of freedom (\\(n-1\\))\nFor \\(\\alpha=0.05\\) and two-sided, this is: \\(t_{1-\\alpha/2, n-1} = 2.26\\).\n\nComparison: \\(1.58 &lt; 2.26\\) \\(\\Rightarrow\\) no significant difference between \\(\\bar{x}\\) and \\(\\mu\\).",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#method-3-calculation-of-the-p-value-from-t_obs",
    "href": "slides/05-classtests.html#method-3-calculation-of-the-p-value-from-t_obs",
    "title": "05-Classical Tests",
    "section": "Method 3: Calculation of the p-value from \\(t_{obs}\\)",
    "text": "Method 3: Calculation of the p-value from \\(t_{obs}\\)\n\n\nuse computerized probability function (pt) instead of table lookup\n\\(t = t_{obs}\\) and the degrees of freedom (\\(n-1\\)):\n\n\n\n2 * (1 - pt(t, df = 10 - 1)) # 2 * (1 - p) is re-arranged from 1-alpha/2\n\n[1] 0.1483047\n\n\nThis p-value = 0.1483047 is greater than \\(0.05\\) so we consider the difference as not significant.\n\nFAQ: less than or greater than?\n\n\n\n\n\n\n\n\n\n\np-value\n\\(\\text{p-value} &lt; \\alpha\\)\nnull hypothesis unlikely\nsignificant\n\n\ntest statistic\n\\(t_{obs} &gt; t_{1-\\alpha/2, n-1}\\)\neffect exceeds confint.\nsignificant",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#method-4-built-in-t-test-function-in-r",
    "href": "slides/05-classtests.html#method-4-built-in-t-test-function-in-r",
    "title": "05-Classical Tests",
    "section": "Method 4: Built-in t-test function in R",
    "text": "Method 4: Built-in t-test function in R\n\nThe same can be done much easier with the computer in R.\nLet’s assume we have a sample with \\(\\bar{x}=5, s=1\\):\n\n## define sample\nx &lt;- c(5.5, 3.5, 5.4, 5.3, 6, 7.2, 5.4, 6.3, 4.5, 5.9)\n\n## perform one-sample t-test\nt.test(x, mu=5)\n\n\n    One Sample t-test\n\ndata:  x\nt = 1.5811, df = 9, p-value = 0.1483\nalternative hypothesis: true mean is not equal to 5\n95 percent confidence interval:\n 4.784643 6.215357\nsample estimates:\nmean of x \n      5.5 \n\n\nThe test returns the observed t-value, the 95% confidence interval and the p-value.\nAn important difference is, that this method needs the original data, while the other methods need only mean, standard deviation and sample size.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#two-sample-t-test",
    "href": "slides/05-classtests.html#two-sample-t-test",
    "title": "05-Classical Tests",
    "section": "Two sample t-test",
    "text": "Two sample t-test\n\nThe two-sample t-test compares two independent samples:\n\nx1 &lt;- c(5.3, 6.0, 7.1, 6.4, 5.7, 4.9, 5.0, 4.6, 5.7, 4.0, 4.5, 6.5)\nx2 &lt;- c(5.8, 7.1, 5.8, 7.0, 6.7, 7.7, 9.2, 6.0, 7.2, 7.8, 7.8, 5.7)\nt.test(x1, x2)\n\n\n    Welch Two Sample t-test\n\ndata:  x1 and x2\nt = -3.7185, df = 21.611, p-value = 0.001224\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.3504462 -0.6662205\nsample estimates:\nmean of x mean of y \n 5.475000  6.983333 \n\n\n\n\n\\(\\rightarrow\\) both samples differ significantly (\\(p &lt; 0.05\\))\nNote: R has not performed the “ordinary” t-test but the Welch test (= heteroscedastic t-test)\nwhere variances of both samples don’t need to be identical.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#hypothesis-and-formula-of-the-two-sample-t-test",
    "href": "slides/05-classtests.html#hypothesis-and-formula-of-the-two-sample-t-test",
    "title": "05-Classical Tests",
    "section": "Hypothesis and formula of the two-sample t-test",
    "text": "Hypothesis and formula of the two-sample t-test\n\n\n\\(H_0\\) \\(\\mu_1 = \\mu_2\\)\n\\(H_a\\) the two means are different\ntest criterion\n\\[\nt_{obs} =\\frac{|\\bar{x}_1-\\bar{x}_2|}{s_{tot}} \\cdot \\sqrt{\\frac{n_1 n_2}{n_1+n_2}}\n\\]\n\n\n\n\n\n\n\n\n\n\npooled standard deviation\n\\[\ns_{tot} = \\sqrt{{({n}_1 - 1)\\cdot s_1^2 + ({n}_2 - 1)\\cdot s_2^2\n\\over ({n}_1 + {n}_2 - 2)}}\n\\]\nassumptions: independence, equal variances, approximate normal distribution",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#the-welch-test",
    "href": "slides/05-classtests.html#the-welch-test",
    "title": "05-Classical Tests",
    "section": "The Welch test",
    "text": "The Welch test\n\nKnown as t-test for samples with unequal variance, works also for equal variance!\n\nTest criterion:\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{s^2_{\\bar{x}_1} + s^2_{\\bar{x}_2}}}\n\\]\nStandard error of each sample:\n\\[\ns_{\\bar{x}_i} = \\frac{s_i}{\\sqrt{n_i}}\n\\] Corrected degrees of freedom:\n\\[\n\\text{df} = \\frac{\\frac{s^2_1}{n_1} + \\frac{s^2_2}{n_2}}{\\frac{s^4_1}{n^2_1(n_1-1)} + \\frac{s^4_2}{n^2_2(n_2-1)}}\n\\]",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#equality-of-variance-f-test",
    "href": "slides/05-classtests.html#equality-of-variance-f-test",
    "title": "05-Classical Tests",
    "section": "Equality of variance: F-test",
    "text": "Equality of variance: F-test\n\n\\(H_0\\): \\(\\sigma_1^2 = \\sigma_2^2\\)\n\\(H_a\\): variances unequal\nTest criterion:\n\\[F = \\frac{s_1^2}{s_2^2} \\]\n\nlarger of the two variances in the enumerator \\((s^2_1 &gt;  s^2_2)\\)\nseparate degrees of freedom (\\(n-1\\))\n\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\\(s_1=1\\), \\(s_2 =2\\), \\(n_1=5, n_2=10, F=\\frac{2^2}{1^2}=4\\)\ndeg. of freedom: \\(9 \\atop 4\\)\n\n\\(\\Rightarrow\\) \\(F_{9, 4, \\alpha=0.975} = 8.9 &gt; 4 \\quad\\rightarrow\\) not significant",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#homogeneity-of-variances-with-2-samples",
    "href": "slides/05-classtests.html#homogeneity-of-variances-with-2-samples",
    "title": "05-Classical Tests",
    "section": "Homogeneity of variances with > 2 samples",
    "text": "Homogeneity of variances with &gt; 2 samples\n\n\nBartlett’s test:\n\nbartlett.test(list(x1, x2, x3))\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  list(x1, x2, x3)\nBartlett's K-squared = 7.7136, df = 2, p-value = 0.02114\n\n\n Fligner-Killeen test (recommended):\n\nfligner.test(list(x1, x2, x3))\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  list(x1, x2, x3)\nFligner-Killeen:med chi-squared = 2.2486, df = 2, p-value = 0.3249\n\n\n\n\n\n\n\n\n\n\n\n\n\ntests are often used to check assumptions of the ANOVA",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#recommendation-for-two-sample-t-tests",
    "href": "slides/05-classtests.html#recommendation-for-two-sample-t-tests",
    "title": "05-Classical Tests",
    "section": "Recommendation for two sample t-tests",
    "text": "Recommendation for two sample t-tests\n\nTraditional procedure:\n\nTest for equal variances using the F-test: var.test(x, y)\nIf variances are equal: t.test(x, y, var.equal=TRUE)\notherwise, use t.test(x, y) (= Welch test)\nCheck if both samples follow a normal distribution.\n\n\nModern recommendation (preferred):\n\nDon’t use pre-tests!\nAlways use the Welch test: t.test(x, y)\nCheck approximate normal distribution with box- or QQ-plots. Less important if \\(n\\) is large.\n\nsee Zimmerman (2004) or Wikipedia.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#paired-t-test",
    "href": "slides/05-classtests.html#paired-t-test",
    "title": "05-Classical Tests",
    "section": "Paired t-Test",
    "text": "Paired t-Test\n\nsometimes also called “t-test of dependent samples”\n\nthe term “dependent” can be misleading, “paired” is clearer\nvalues within samples must still be independent\n\nexamples: left arm / right arm; before / after\nis essentially a one-sample t-test of pairwise differences against \\(\\mu=0\\)\n\nreduces the influence of individual differences (“covariates”) by focusing on the change within each pair\n\n\n\nx1 &lt;- c(2, 3, 4, 5, 6)\nx2 &lt;- c(3, 4, 7, 6, 8)\nt.test(x1, x2, var.equal=TRUE)\n\n\n    Two Sample t-test\n\ndata:  x1 and x2\nt = -1.372, df = 8, p-value = 0.2073\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -4.28924  1.08924\nsample estimates:\nmean of x mean of y \n      4.0       5.6 \n\n\np=0.20, not significant\n\n\nx1 &lt;- c(2, 3, 4, 5, 6)\nx2 &lt;- c(3, 4, 7, 6, 8)\nt.test(x1, x2, paired=TRUE)\n\n\n    Paired t-test\n\ndata:  x1 and x2\nt = -4, df = 4, p-value = 0.01613\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.710578 -0.489422\nsample estimates:\nmean difference \n           -1.6 \n\n\np=0.016, significant\nIt can be seen that the paired t-test has a greater discriminatory power in this case.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney-and-wilcoxon-test",
    "href": "slides/05-classtests.html#mann-whitney-and-wilcoxon-test",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney and Wilcoxon-test",
    "text": "Mann-Whitney and Wilcoxon-test\n\n\nNon-parametric tests:\n\nNo assumptions about shape and parameters of distribution, but\ndistributions should be similar, otherwise test may be misleading.\n\nBased on Ranks: Tests compare the ranks of the data.\nUse Mann-Whitney for independent samples, Wilcoxon for paired samples.\n\n Basic principle: Count of so-called “inversions” of ranks, where samples overlap\n\nSample A: 1, 3, 4, 5, 7\nSample B: 6, 8, 9, 10, 11\nBoth samples ordered together: 1, 3, 4, 5, 6, 7, 8, 9, 10, 11\nInversions: \\(\\rightarrow\\) \\(U = 1\\)",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney-test-procedure-in-practice",
    "href": "slides/05-classtests.html#mann-whitney-test-procedure-in-practice",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney test procedure in practice",
    "text": "Mann-Whitney test procedure in practice\n\n\nAssign ranks \\(R_A\\) and \\(R_B\\) to both samples \\(A\\), and \\(B\\) with sample size \\(m\\) and \\(n\\).\nCalculate number of inversions \\(U\\):\n\n\\[\\begin{align*}\n     U_A &= m \\cdot n + \\frac{m (m + 1)}{2} - \\sum_{i=1}^m R_A \\\\\n     U_B &= m \\cdot n + \\frac{n (n + 1)}{2} - \\sum_{i=1}^n R_B \\\\\n     U   &= \\min(U_A, U_B)\n\\end{align*}\\]\n\nCritical values of \\(U\\) can be found in common statistics text books.\nNot necessary in R, p-value directly printed.\nNote: Use special version wilcox.exact with correction if sample has ties.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#mann-whitney---wilcoxon-test-in-r",
    "href": "slides/05-classtests.html#mann-whitney---wilcoxon-test-in-r",
    "title": "05-Classical Tests",
    "section": "Mann-Whitney - Wilcoxon-test in R",
    "text": "Mann-Whitney - Wilcoxon-test in R\n\n\nA &lt;- c(1, 3, 4, 5, 7)\nB &lt;- c(6, 8, 9, 10, 11)\n\nwilcox.test(A, B) # use optional argument `paired = TRUE` for paired data.\n\n\n    Wilcoxon rank sum exact test\n\ndata:  A and B\nW = 1, p-value = 0.01587\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nMann-Whitney - Wilcoxon-test with tie correction\n\napplied if the rank differences contain duplicated values\n\n\nA &lt;- c(1, 3, 4, 5, 7)\nB &lt;- c(6, 8, 9, 10, 11)\n\n\nlibrary(\"exactRankTests\")\nwilcox.exact(A, B, paired=TRUE)\n\n\n    Exact Wilcoxon signed rank test\n\ndata:  A and B\nV = 0, p-value = 0.0625\nalternative hypothesis: true mu is not equal to 0",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#permutation-methods",
    "href": "slides/05-classtests.html#permutation-methods",
    "title": "05-Classical Tests",
    "section": "Permutation methods",
    "text": "Permutation methods\n\nBasic principle: Estimation of a test statistic \\(\\xi_{obs}\\) from sample,\nResampling: Simulate many \\(\\xi_{i, sim}\\) from randomly permuted data set (\\(n = 999\\) or more)\nWhere does \\(\\xi_{est}\\) appear within the ordered series of simulated values \\(\\xi_{i, sim}\\)?\n\n\nLet \\(\\xi_{obs}\\) be \\(4.5\\) in our example, then \\(\\Rightarrow\\) \\(p= 0.97\\).",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#determining-the-power-of-statistical-tests",
    "href": "slides/05-classtests.html#determining-the-power-of-statistical-tests",
    "title": "05-Classical Tests",
    "section": "Determining the power of statistical tests",
    "text": "Determining the power of statistical tests\n\nHow many replicates will I need?\n\nDepends on:\n\nthe relative effect size \\(\\frac{\\mathrm{effect}}{\\mathrm{standard ~ deviation}}\\)\n\n\\[\\delta=\\frac{(\\bar{x}_1-\\bar{x}_2)}{s}\\]\n\nthe sample size \\(n\\)\nand the pre-defined significance level \\(\\alpha\\)\nand the applied method\n\nThe smaller \\(\\alpha\\), \\(n\\) and \\(\\delta\\), the bigger the type II (\\(\\beta\\)) error.\nThe \\(\\beta\\)-error is the probability to overlook effects despite of their existence.\nPower (\\(1-\\beta\\)) is the probability that a test is significant if an effect exists.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#power-analysis-1",
    "href": "slides/05-classtests.html#power-analysis-1",
    "title": "05-Classical Tests",
    "section": "Power analysis",
    "text": "Power analysis\n\nFormula for minimum sample size in the one-sample case:\n\\[\nn = \\bigg(\\frac{z_\\alpha + z_{1-\\beta}}{\\delta}\\bigg)^2\n\\]\n\n\\(z\\): the quantiles (qnorm) of the standard normal distribution for \\(\\alpha\\) and for \\(1-\\beta\\)\n\\(\\delta=\\Delta / s\\): relative effect size.\n\nExample\nTwo-tailed test with \\(\\alpha=0.025\\) and \\(\\beta=0.2\\)\n\\(\\rightarrow\\) \\(z_\\alpha = 1.96\\), \\(z_\\beta=0.84\\), then:\n\\[\nn= (1.96 + 0.84)^2 \\cdot 1/\\delta^2 \\approx 8 /\\delta^2\n\\]\n\\(\\delta = 1.0\\cdot \\sigma\\) \\(\\qquad\\Rightarrow\\) n &gt; 8\n\\(\\delta = 0.5\\cdot \\sigma\\) \\(\\qquad\\Rightarrow\\) n &gt; 32",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#power-of-the-t-test",
    "href": "slides/05-classtests.html#power-of-the-t-test",
    "title": "05-Classical Tests",
    "section": "Power of the t-test",
    "text": "Power of the t-test\nThe power of a t-test, or the minimum sample size, can be calculated with: power.t.test():\n\npower.t.test(n=5, delta=0.5, sig.level=0.05)\n\n\n     Two-sample t test power calculation \n\n              n = 5\n          delta = 0.5\n             sd = 1\n      sig.level = 0.05\n          power = 0.1038399\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\n\\(\\rightarrow\\) power = 0.10\n\nFor \\(n=5\\) an existing effect of \\(0.5\\sigma\\) is only detected in 1 out of 10 cases.\nFor a power of 80% at \\(n=5\\) we need an effect size of at least \\(2\\sigma\\):\n\n\npower.t.test(n=5, power=0.8, sig.level=0.05)\n\nFor a weak effect of \\(0.5\\sigma\\) we need a sample size of \\(n\\ge64\\) in each group:\n\npower.t.test(delta=0.5,power=0.8,sig.level=0.05)\n\n\\(\\Rightarrow\\) we need either a large sample size or a strong effect.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#simulated-power-of-a-t-test",
    "href": "slides/05-classtests.html#simulated-power-of-a-t-test",
    "title": "05-Classical Tests",
    "section": "Simulated power of a t-test",
    "text": "Simulated power of a t-test\n\n# population parameters\nn      &lt;- 10\nxmean1 &lt;- 50; xmean2 &lt;- 55\nxsd1   &lt;- xsd2 &lt;- 10\nalpha  &lt;- 0.05\n\nnn &lt;- 1000   # number of test runs in the simulation\na &lt;- b &lt;- 0  # initialize counters\nfor (i in 1:nn) {\n  # create random numbers\n  x1 &lt;- rnorm(n, xmean1, xsd1)\n  x2 &lt;- rnorm(n, xmean2, xsd2)\n  # results of the t-test\n  p &lt;- t.test(x1,x2,var.equal = TRUE)$p.value \n  if (p &lt; alpha) {\n     a &lt;- a+1\n   } else {\n     b &lt;- b+1\n  }\n}\nprint(paste(\"a=\", a, \", b=\", b, \", a/n=\", a/nn, \", b/n=\", b/nn))",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#testing-for-distributions",
    "href": "slides/05-classtests.html#testing-for-distributions",
    "title": "05-Classical Tests",
    "section": "Testing for distributions",
    "text": "Testing for distributions\nNominal variables\n\n\\(\\chi^2\\)-test\nFisher’s exact test\n\nOrdinal variables\n\nCramér-von-Mises-Test\n\\(\\rightarrow\\) more powerful than \\(\\chi^2\\) or KS-test\n\nMetric scales\n\nKolmogorov-Smirnov-Test (KS-test)\nShapiro-Wilks-Test (for normal distribution)\nGraphical checks",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#contingency-tables-for-nominal-variables",
    "href": "slides/05-classtests.html#contingency-tables-for-nominal-variables",
    "title": "05-Classical Tests",
    "section": "Contingency tables for nominal variables",
    "text": "Contingency tables for nominal variables\n\nused for nominal (i.e. categorical or qualitative) data\nexamples: eye and hair color, medical treatment and the number of cured/not cured\nimportant: use absolute mesurements (true numbers!), not percentages or other calculated data (e.g. not something like biomass per area)\n\nExample: Occurence of Daphnia (water flea) in a lake:\n\n\n\nClone\nUpper layer\nDeep layer\n\n\n\n\nA\n50\n87\n\n\nB\n37\n78\n\n\nC\n72\n45\n\n\n\n\nfood algae in the deep water, that was poor of oxygen\ngenetically evolved clones with higher haemoglobin content can dive into deep water",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#calculation-of-the-chi2-test",
    "href": "slides/05-classtests.html#calculation-of-the-chi2-test",
    "title": "05-Classical Tests",
    "section": "Calculation of the \\(\\chi^2\\)-test",
    "text": "Calculation of the \\(\\chi^2\\)-test\n\nObserved frequencies \\(O_{ij}\\)\n\n\n\n\n\nClone A\nClone B\nClone C\nSum \\(s_i\\)\n\n\n\n\nUpper layer\n50\n37\n72\n159\n\n\nLower layer\n87\n78\n45\n210\n\n\nSum \\(s_j\\)\n137\n115\n117\n\\(n=369\\)\n\n\n\n\n\n\n\n\n\n\n\nExpected frequencies \\(E_{ij} = s_i \\cdot s_j / n\\) (balanced distribution = null hypothesis)\n\n\n\n\n\nClone A\nClone B\nClone C\nSum \\(s_i\\)\n\n\n\n\nUpper layer\n59.0\n49.6\n50.4\n159\n\n\nLower layer\n78.0\n65.4\n66.6\n210\n\n\nSum \\(s_j\\)\n137\n115\n117\n\\(n=369\\)\n\n\n\n\n\n\n\n\n\n\n\nTest statistic \\(\\hat{\\chi}^2 = \\sum_{i, j} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\)\nCompare with critical \\(\\chi^2\\) from table with \\((n_{row} - 1) \\cdot (n_{col} - 1)\\) df.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#the-chi2-test-in-r",
    "href": "slides/05-classtests.html#the-chi2-test-in-r",
    "title": "05-Classical Tests",
    "section": "The \\(\\chi^2\\)-test in R",
    "text": "The \\(\\chi^2\\)-test in R\n\nOrganize data in a matrix with 3 rows (for the clones) and 2 columns (for the depths):\n\n\nx &lt;- matrix(c(50, 37, 72, 87, 78, 45), ncol=2)\nx\n\n     [,1] [,2]\n[1,]   50   87\n[2,]   37   78\n[3,]   72   45\n\n\n\n\n\nchisq.test(x)\n\n\n    Pearson's Chi-squared test\n\ndata:  x\nX-squared = 24.255, df = 2, p-value = 5.408e-06\n\n\n\n\nNote: The results are only reliable if all observed frequencies are \\(\\geq 5\\).\nFor smaller samples, use Fisher’s exact test.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#fishers-exact-test",
    "href": "slides/05-classtests.html#fishers-exact-test",
    "title": "05-Classical Tests",
    "section": "Fisher’s exact test",
    "text": "Fisher’s exact test\n\n\n\nx &lt;- matrix(c(50, 37, 72, 87, 78, 45), ncol=2)\nx\n\n     [,1] [,2]\n[1,]   50   87\n[2,]   37   78\n[3,]   72   45\n\n\n\nfisher.test(x)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  x\np-value = 5.807e-06\nalternative hypothesis: two.sided\n\n\n\n\n\\(\\rightarrow\\) significant correlation between the clones and vertical distribution in the lake.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#favorite-numbers-of-hse-students",
    "href": "slides/05-classtests.html#favorite-numbers-of-hse-students",
    "title": "05-Classical Tests",
    "section": "Favorite numbers of HSE students",
    "text": "Favorite numbers of HSE students\n\n\nNumbers from 1..9, \\(n=34\\)\n\\(H_0\\): equal probability of all numbers \\(1/9\\) (discrete uniform distribution)\n\\(H_A\\): some numbers are favored \\(\\rightarrow\\) departure from discrete uniform",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#chisquare-test",
    "href": "slides/05-classtests.html#chisquare-test",
    "title": "05-Classical Tests",
    "section": "Chisquare test",
    "text": "Chisquare test\n\n\n\nobsfreq &lt;- c(1, 1, 6, 2, 2, 5, 8, 6, 3)\nchisq.test(obsfreq)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obsfreq\nX-squared = 13.647, df = 8, p-value = 0.09144\n\nchisq.test(obsfreq, simulate.p.value=TRUE, B=1000)\n\n\n    Chi-squared test for given probabilities with simulated p-value (based\n    on 1000 replicates)\n\ndata:  obsfreq\nX-squared = 13.647, df = NA, p-value = 0.0969\n\n\n\n\n\none-sample \\(\\chi^2\\)-test. It tests for equality of frequency in all classes.\nThe simulation-based version of the test (with 1000 replicates) is slightly more precise than the standard \\(\\chi^2\\)-test, but both are not significant.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#cramér-von-mises-test",
    "href": "slides/05-classtests.html#cramér-von-mises-test",
    "title": "05-Classical Tests",
    "section": "Cramér-von-Mises-Test",
    "text": "Cramér-von-Mises-Test\n\n\\[\nT = n \\omega^2 = \\frac{1}{12n} + \\sum_{i=1}^n \\left[ \\frac{2i-1}{2n}-F(x_i) \\right]^2\n\\]",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#cramér-von-mises-test-in-r",
    "href": "slides/05-classtests.html#cramér-von-mises-test-in-r",
    "title": "05-Classical Tests",
    "section": "Cramér-von-Mises-Test in R",
    "text": "Cramér-von-Mises-Test in R\n\nlibrary(dgof)\nobsfreq &lt;- c(1, 1, 6, 2, 2, 5, 8, 6, 3)\n\n## CvM-test needs individual values, not class frequencies\nx &lt;- rep(1:length(obsfreq), obsfreq)\nx\n\n [1] 1 2 3 3 3 3 3 3 4 4 5 5 6 6 6 6 6 7 7 7 7 7 7 7 7 8 8 8 8 8 8 9 9 9\n\n\n\n\n## create a cumulative function with equal probability of all cases\ncdf &lt;- stepfun(1:9, cumsum(c(0, rep(1/9, 9))))\ncdf &lt;- ecdf(1:9)\n\n## perform the test\ncvm.test(x, cdf)\n\n\n    Cramer-von Mises - W2\n\ndata:  x\nW2 = 0.51658, p-value = 0.03665\nalternative hypothesis: Two.sided\n\n\n\nThe Cramér-von-Mises-test works with the original, unbinned values\nUse of cumulative distribution function respects order of classes \\(\\rightarrow\\) more powerful, than \\(\\chi^2\\)-test.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#testing-or-checking",
    "href": "slides/05-classtests.html#testing-or-checking",
    "title": "05-Classical Tests",
    "section": "Testing or checking?",
    "text": "Testing or checking?\n\nPhilosophical problem: We want to keep the \\(H_0\\)!\n\nEquality cannot be tested\nTherefore: better to say “checking normality”.\n\nThink first\n\nDoes normal distribution “makes sense” for the data?\nAre the data metric (continuous)?\nWhat is the data generating process? \\(\\rightarrow\\) Contextual understanding!\n\nInherent non-normality\n\nSome types of data, such as count data (e.g., number of occurrences) and binary data (e.g., yes/no), are inherently non-normal.\n\n\nBinary data: use methods for Binomial distribution with raw data instead of percentages\nCount data: use methods designed for Poisson distribution",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#shapiro-wilks-w-test",
    "href": "slides/05-classtests.html#shapiro-wilks-w-test",
    "title": "05-Classical Tests",
    "section": "Shapiro-Wilks-W-Test ?",
    "text": "Shapiro-Wilks-W-Test ?\n\\(\\rightarrow\\) Aim: tests if a sample conforms to a normal distribution\n\nx &lt;- rnorm(100)\nshapiro.test(x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99064, p-value = 0.7165\n\n\n\n\\(\\rightarrow\\) the \\(p\\)-value is greater than 0.05, so we would keep \\(H_0\\) and conclude that nothing speaks against acceptance of the normal\n\nInterpration of the Shapiro-Wilks-test needs to be done with care:\n\nfor small \\(n\\), the test is not sensitive enough\nfar large \\(n\\), it is over-sensitive\nusing Shapiro-Wilks to check normality for t-test and ANOVA is not anymore recommended\n\n\n\nSimilarly, the \\(\\chi^2\\) (Chi-squared) or Kolmogorov-Smirnov-tests are not anymore recommended for normality testing, but still important for other test problems.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#alternative-use-graphical-methods",
    "href": "slides/05-classtests.html#alternative-use-graphical-methods",
    "title": "05-Classical Tests",
    "section": "Alternative: use graphical methods",
    "text": "Alternative: use graphical methods\n\n\n\nhistogram, boxplot, QQ-plot (=quantile-quantile plot)\nsee also: Box-Cox method",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#graphical-checks-of-normality",
    "href": "slides/05-classtests.html#graphical-checks-of-normality",
    "title": "05-Classical Tests",
    "section": "Graphical checks of normality",
    "text": "Graphical checks of normality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\): theoretical quantiles where a value should be found if the distribution is normal\n\\(y\\): normalized and ordered measured values (\\(z\\)-scores)\nscaled in the unit of standard deviations\nnormal distribution if the points follow a straight line",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#transformations",
    "href": "slides/05-classtests.html#transformations",
    "title": "05-Classical Tests",
    "section": "Transformations",
    "text": "Transformations\n\nallows to apply methods designed for normally distributed data to non-normal cases\nvery common in the in the past, still sometimes useful\nmodern methods (e.g. generalized linear models, GLM) can handle certain distributions directly, such as the Binomial, Gamma, or Poisson distribution.\n\nTransformations for right-skewed data\n\n\\(x'=\\log(x)\\)\n\\(x'=\\log(x + a)\\)\n\\(x'=(x+a)^c\\) (\\(a\\) between 0.5 and 1)\n\\(x'=1/x\\) (“very powerful”, i.e. to extreme in most cases)\n\\(x'=a - 1/\\sqrt{x}\\) (to make scale more convenient)\n\\(x'=1/\\sqrt{x}\\) (compromise between \\(\\ln\\) and \\(1/x\\))\n\\(x'=a+bx^c\\) (very general, includes powers and roots)",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#transformations-ii",
    "href": "slides/05-classtests.html#transformations-ii",
    "title": "05-Classical Tests",
    "section": "Transformations II",
    "text": "Transformations II\nTransformations for count data\n\n\\(x'=\\sqrt{3/8+x}\\) (counts: 1, 2, 3 \\(\\rightarrow\\) 0.61, 1.17, 1.54, 1.84, ) \n\\(x'=\\log(\\log(x))\\) for giant numbers\n\n\\(\\rightarrow\\) consider a GLM with family Poisson or quasi-Poisson instead\nRatios and percentages values between (0, 1)\n\n\\(x'=\\arcsin \\sqrt{x/n}\\)\n\\(x'=\\arcsin \\sqrt{\\frac{x+3/8}{n+3/4}}\\)\n\n\\(\\rightarrow\\) consider a GLM with family binomial instead",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#how-to-find-the-best-transformation",
    "href": "slides/05-classtests.html#how-to-find-the-best-transformation",
    "title": "05-Classical Tests",
    "section": "How to find the best transformation?",
    "text": "How to find the best transformation?\n\nExample: biovolumes of diatom algae cells (species Nitzschia acicularis).\n\n\n\n\ndat &lt;- read.csv(\"prk_nit.csv\")\n\nNit85 &lt;- dat$biovol[dat$group == \"nit85\"]\nNit90 &lt;- dat$biovol[dat$group == \"nit90\"]\n\nhist(Nit85, xlab=\"Biovolume (mm^3)\")\nhist(Nit90, xlab=\"Biovolume (mm^3)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRight skewed distribution.\nBiovolume calculations used for foodweb studies and algae bloom prediction.\n\n\n\nMore about Nitzschia in Wikipedia\nData set prk_nit.csv and metadata prk_nit_info.txt available from https://github.com/tpetzoldt/datasets.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#box-cox-method",
    "href": "slides/05-classtests.html#box-cox-method",
    "title": "05-Classical Tests",
    "section": "Box-Cox method",
    "text": "Box-Cox method\n\\[\ny' =\n\\begin{cases}\ny^\\lambda & | & \\lambda \\ne 0\\\\\n\\log(y) & | & \\lambda =0\n\\end{cases}\n\\] * Estimate optimal transformation from the class of powers and logarithms\n\n\n\n\nlibrary(MASS)\n\nboxcox(Nit90 ~ 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArgument of boxcox is a so-called “model formula” or the outcome of a linear model (lm)\nMost basic form is the “null model” without explanation variables (~ 1).\nMore about model formulas, see the ANOVA chapter.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#box-cox-method-results",
    "href": "slides/05-classtests.html#box-cox-method-results",
    "title": "05-Classical Tests",
    "section": "Box Cox-method: Results",
    "text": "Box Cox-method: Results\n\n\nInterpretation\n\n\ndotted vertical lines and horizontal 95%-line show the confidence limits for possible transformations.\nNumbers are apporoximate \\(\\rightarrow\\) round it to one decimal.\nHere we can use either a log transformation (\\(\\lambda=0\\)) or a power of \\(\\approx 0.5\\).\n\n\n\n\nObtain the numerical value directly:\n\nbc &lt;- boxcox(Nit90 ~ 1)\n\n\n\n\n\n\n\nstr(bc)\n\nList of 2\n $ x: num [1:100] -2 -1.96 -1.92 -1.88 -1.84 ...\n $ y: num [1:100] -237 -233 -230 -226 -223 ...\n\nbc$x[bc$y == max(bc$y)]\n\n[1] 0.1818182",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#test-of-pooled-samples-with-different-mean",
    "href": "slides/05-classtests.html#test-of-pooled-samples-with-different-mean",
    "title": "05-Classical Tests",
    "section": "Test of pooled samples with different mean",
    "text": "Test of pooled samples with different mean\n\nboxcox(biovol ~ group, data = dat)\n\n\n\nTo test for joint distribution of all groups at once, specify explanatory variables at the right hand side of the model formula: biovol ~ group\nOptimal transformation for both samples together is log.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#correlation",
    "href": "slides/05-classtests.html#correlation",
    "title": "05-Classical Tests",
    "section": "Correlation",
    "text": "Correlation\n\nFrequencies of nominal variables\n\n\\(\\chi^2\\)-test\nFisher’s exact test\n\n⇒ dependence between plant society and soil type\n(see before)\nOrdinal variables\n\nSpearman-Correlation\n\n\\(\\rightarrow\\) rank numbers\nMetric scales\n\nPearson-correlation\nSpearman-correlation",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#variance-and-covariance",
    "href": "slides/05-classtests.html#variance-and-covariance",
    "title": "05-Classical Tests",
    "section": "Variance and Covariance",
    "text": "Variance and Covariance\n\n\nVariance\n\nmeasures variation of a single variable\n\n\\[\n  s^2_x = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\n\\]\nCovariance\n\nmeasures how two variables change together\n\n\\[\n  q_{x,y} = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})(y_i-\\bar{y})}{n-1}\n\\]\nCorrelation: scaled to \\((-1, +1)\\)\n\\[\n  r_{x,y} = \\frac{q_{x,y}}{s_x \\cdot s_y}\n\\]",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#correlation-coefficient-after-pearson",
    "href": "slides/05-classtests.html#correlation-coefficient-after-pearson",
    "title": "05-Classical Tests",
    "section": "Correlation coefficient after Pearson",
    "text": "Correlation coefficient after Pearson\n\n\n\nthe usual correlation coefficient that we all know\ntests for linear dependence\n\n\\[\nr_p=\\frac{\\sum{(x_i-\\bar{x})  (y_i-\\bar{y})}}\n       {\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n\\]\nOr:\n\\[\nr_p=\\frac {\\sum xy - \\sum y \\sum y / n}\n        {\\sqrt{(\\sum x^2-(\\sum x)^2/n)(\\sum y^2-(\\sum y)^2/n)}}\n\\] \nRange of values: \\(-1 \\le r_p \\le +1\\)\n\n\n\n\\(0\\)\nno interdependence\n\n\n\\(+1 \\,\\text{or}\\,-1\\)\nstrictly positive resp. negative dependence\n\n\n\\(0 &lt; |r_p| &lt; 1\\)\npositive resp. negative dependence",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#which-size-of-correlation-indicates-dependency",
    "href": "slides/05-classtests.html#which-size-of-correlation-indicates-dependency",
    "title": "05-Classical Tests",
    "section": "Which size of correlation indicates dependency?",
    "text": "Which size of correlation indicates dependency?\n\n\n\n\n\n\n\n\n\n\n\\(r=0.4, \\quad p=0.0039\\)\n\n\n\n\n\n\n\n\n\n\n\\(r=0.85, \\quad p=0.07\\)",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#significant-correlation",
    "href": "slides/05-classtests.html#significant-correlation",
    "title": "05-Classical Tests",
    "section": "Significant correlation?",
    "text": "Significant correlation?\n\\[\n\\hat{t}_{\\alpha/2;n-2} =\\frac{|r_p|\\sqrt{n-2}}{\\sqrt{1-r^2_p}}\n\\]\n\\(t=0.829 \\cdot \\sqrt{1000-2}/\\sqrt{1-0.829^2}=46.86, df=998\\)\n Quick test: critical values for \\(r_p\\)\n\n\n\n\\(n\\)\nd.f.\n\\(t\\)\n\\(r_{crit}\\)\n\n\n3\n1\n12.706\n0.997\n\n\n5\n3\n3.182\n0.878\n\n\n10\n8\n2.306\n0.633\n\n\n20\n18\n2.101\n0.445\n\n\n50\n48\n2.011\n0.280\n\n\n100\n98\n1.984\n0.197\n\n\n1000\n998\n1.962\n0.062",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#rank-correlation-according-to-spearman",
    "href": "slides/05-classtests.html#rank-correlation-according-to-spearman",
    "title": "05-Classical Tests",
    "section": "Rank-correlation according to Spearman",
    "text": "Rank-correlation according to Spearman\n\n\nmeasures monotonous (and not necessarily linear) dependence\nestimation from rank differences:\n\n\\[\nr_s=1-\\frac{6 \\sum d^2_i}{n(n^2-1)}\n\\]\n\nor, alternatively: Pearson-correlation of ranked data (necessary in case of ties).\nTest: for \\(n &lt; 10\\) \\(\\rightarrow\\) table of critical values\n\nfor \\(10 \\leq n\\) \\(\\rightarrow\\) \\(t\\)-distribution\n\\[\n   \\hat{t}_{1-\\frac{\\alpha}{2};n-2}\n      =\\frac{|r_s|}{\\sqrt{1-r^2_S}} \\sqrt{n-2}\n\\]\n\n\nComputer statistics packages use a special algorithm (algorithm AS 89 according to Best and Roberts, 1975).",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#example",
    "href": "slides/05-classtests.html#example",
    "title": "05-Classical Tests",
    "section": "Example",
    "text": "Example\n\n\n\n\n\\(x\\)\n\\(y\\)\n\\(R_x\\)\n\\(R_y\\)\n\\(d\\)\n\\(d^2\\)\n\n\n\n\n1\n2.7\n1\n1\n0\n0\n\n\n2\n7.4\n2\n2\n0\n0\n\n\n3\n20.1\n3\n3\n0\n0\n\n\n4\n500.0\n4\n5\n-1\n1\n\n\n5\n148.4\n5\n4\n+1\n1\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\\[\nr_s=1-\\frac{6 \\cdot 2}{5\\cdot (25-1)}=1-\\frac{12}{120}=0.9\n\\]\nFor comparison: \\(r_p=0.58\\)",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#application-of-spearmans-r_s",
    "href": "slides/05-classtests.html#application-of-spearmans-r_s",
    "title": "05-Classical Tests",
    "section": "Application of Spearman’s-\\(r_s\\)",
    "text": "Application of Spearman’s-\\(r_s\\)\n\nAdvantages\n\ndistribution free (does not require normal distribution),\ndetects any dependence,\nnot much affected by outliers.\n\nDisadvantages:\n\ncertain information loss due to ranking,\nno information about type of dependency,\nno direct relationship to coefficient of determination.\n\nConclusion: \\(r_s\\) is nevertheless highly recommended!",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#correlation-coefficients-in-r",
    "href": "slides/05-classtests.html#correlation-coefficients-in-r",
    "title": "05-Classical Tests",
    "section": "Correlation coefficients in R",
    "text": "Correlation coefficients in R\n\nPearson’s product-moment correlation coefficient\nSpearman’s rank correlation coefficient\n\n\nx &lt;- c(1, 2, 3, 5, 7,  9)\ny &lt;- c(3, 2, 5, 6, 8, 11)\ncor.test(x, y, method=\"pearson\")\n\n\n    Pearson's product-moment correlation\n\ndata:  x and y\nt = 7.969, df = 4, p-value = 0.001344\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.7439930 0.9968284\nsample estimates:\n      cor \n0.9699203 \n\n\nIf linearity or normality of residuals is doubtful, use a rank correlation\n\ncor.test(x, y, method=\"spearman\")\n\n\n    Spearman's rank correlation rho\n\ndata:  x and y\nS = 2, p-value = 0.01667\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.9428571",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#problematic-cases",
    "href": "slides/05-classtests.html#problematic-cases",
    "title": "05-Classical Tests",
    "section": "Problematic cases",
    "text": "Problematic cases",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#outlook-more-than-two-independent-variables",
    "href": "slides/05-classtests.html#outlook-more-than-two-independent-variables",
    "title": "05-Classical Tests",
    "section": "Outlook: More than two independent variables",
    "text": "Outlook: More than two independent variables\n\nMultiple correlation\n\nExample: Chl-a=\\(f(x_1, x_2, x_3, \\dots)\\), where \\(x_i\\) = biomass of the \\(i\\)th phytoplankton species.\nmultiple correlation coefficient\npartial correlation coefficient\nattractive method \\(\\leftrightarrow\\) but difficult in practice:\n\n“independent” variables may correlate with each other (multi-collinearity) \\(\\Rightarrow\\) bias of the multiple \\(r\\).\nnon-linearities are even more difficult to handle than in the two-sample case.\n\n\nRecommendation:\n\nUse multivariate methods (NMDS, PCA, …) for a first overview,\napply multiple regression with care and use process knowledge.",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/05-classtests.html#references",
    "href": "slides/05-classtests.html#references",
    "title": "05-Classical Tests",
    "section": "References",
    "text": "References\n\n\n\n\n\nHubbard, R. (2004). Alphabet soup: Blurring the distinctions between p’s and a’s in psychological research. Theory & Psychology, 14(3), 295–327. https://doi.org/10.1177/0959354304043638\n\n\nZimmerman, D. W. (2004). A note on preliminary tests of equality of variances. British Journal of Mathematical and Statistical Psychology, 57(1), 173–181. https://doi.org/10.1348/000711004849222",
    "crumbs": [
      "Basic Statistics",
      "05-Classical Tests"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#statistical-parameters",
    "href": "slides/03-statparams.html#statistical-parameters",
    "title": "03-Statistical Parameters",
    "section": "Statistical Parameters",
    "text": "Statistical Parameters\n\n\\(\\rightarrow\\) Remember: calculation of statistical parameters is called estimation\nProperties of statistical parameters\n\nUnbiasedness: the estimation converges towards the true value with increasing \\(n\\)\nEfficiency a relatively small \\(n\\) is sufficient for a good estimation\nRobustness the estimation is not much influenced by outliers or certain violations of statistical assumptions\n\nDepending on a particular question, different classes of parameters exist, especially measures of location (e.g. mean, median), variation (e.g. variance, standard deviation) or dependence (e.g. correlation).",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#measures-of-location-i",
    "href": "slides/03-statparams.html#measures-of-location-i",
    "title": "03-Statistical Parameters",
    "section": "Measures of location I",
    "text": "Measures of location I\n\nArithmetic mean\n\\[\n  \\bar{x} = \\frac{1}{n} \\cdot {\\sum_{i=1}^n x_i}\n\\]\n\nGeometric mean\n\\[\n  G = \\sqrt[n]{\\prod_{i=1}^n x_i}\n\\]\nmore practical: logarithmic form:\n\\[\n  G =\\exp\\Bigg(\\frac{1}{n} \\cdot {\\sum_{i=1}^n \\ln{x_i}}\\Bigg)\n\\]\navoids huge numbers that make problems for the computer.",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#measures-of-location-ii",
    "href": "slides/03-statparams.html#measures-of-location-ii",
    "title": "03-Statistical Parameters",
    "section": "Measures of location II",
    "text": "Measures of location II\nHarmonic mean\n\\[\n    \\frac{1}{H}=\\frac{1}{n}\\cdot \\sum_{i=1}^n \\frac{1}{x_i} \\quad; x_i&gt;0\n\\]\nExample:\nYou drive with 50km/h to the university and with 100km/h back home. What is the mean velocity?\nResult:\n1/((1/50 + 1/100)/2) = 1/((0.02 + 0.01)/2) = 1/0.015 = 66.67",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#median-central-value",
    "href": "slides/03-statparams.html#median-central-value",
    "title": "03-Statistical Parameters",
    "section": "Median (central value)",
    "text": "Median (central value)\n \\(n\\) uneven: sort data, take the middle value\n\\[\\tilde{x} = x_{(n+1)/2}\\] \\(n\\) even: sort data, take average of the two middle values\n\\[\\tilde{x} = \\frac{x_{n/2}+x_{n/2+1}}{2}\\]\nExample\n\n\n\n\n\n\n\nsample with 7 values\n2.9, 7.9, 4.1, 8.8, 9.4, 0.5, 5.3\n\n\nordered sample\n0.5, 2.9, 4.1, 5.3, 7.9, 8.8, 9.4\n\n\n\n\n\n\n\n\\(\\Rightarrow\\) median: \\(\\tilde{x} = 5.3\\)\n\\(\\Rightarrow\\) mean: \\(\\bar{x} = 5.5571429\\)",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#trimmed-mean",
    "href": "slides/03-statparams.html#trimmed-mean",
    "title": "03-Statistical Parameters",
    "section": "Trimmed mean",
    "text": "Trimmed mean\n\n\nalso called “truncated mean”\ncompromize between the arithmetic mean and median\nA certain percentage of smallest and biggest values is ignored (e.g. 10% or 25%) before calculating the arithmetic mean\nused also in sports\n\nExample: sample with 20 values, exclude 10% at both sides\n0.4, 0.5, 1, 2.5, 2.9, 3.3, 4.1, 4.5, 4.6, 5.3, 5.5, 5.7, 6.8, 7.9, 8.8, 8.9, 9, 9.4, 9.6, 46\n\\(\\rightarrow\\) arithmetic mean: \\(\\bar{x}=7.335\\) \\(\\rightarrow\\) trimmed mean: \\(\\bar{x}_{t, 0.1}=5.6375\\)\n\nmedian and trimmed mean are less influenced by outliers and skewnes \\(\\rightarrow\\) more robust\nbut somewhat less efficient",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#pseudomedian-hodges-lehmann-estimator",
    "href": "slides/03-statparams.html#pseudomedian-hodges-lehmann-estimator",
    "title": "03-Statistical Parameters",
    "section": "Pseudomedian (Hodges-Lehmann estimator)",
    "text": "Pseudomedian (Hodges-Lehmann estimator)\n\nThe pseudomedian (\\(\\tilde{x}^*\\)) is a robust and efficient estimator for the location parameter. It is calculated as the median of all possible means of two observations each, where the pairs include the self-pairs (\\(i=j\\)).\n\\[\\tilde{x}^* = \\text{median}\\left(M_{ij}\\right) \\text{ mit } M_{ij} = \\frac{x_i + x_j}{2} \\text{ für } 1 \\le i \\le j \\le n\\]\nExample\n\nlibrary(Hmisc) \n\n# Using the same sample as for the median\nset.seed(123)\nx &lt;- round(runif(7, max = 10), 1)\n\n# Sorting for better readability of the example (not part of the pMedian calculation)\nsort(x) \n\n[1] 0.5 2.9 4.1 5.3 7.9 8.8 9.4\n\n# arithmetic mean, median, pseudomedian\nc(mean(x), median(x), pMedian(x))\n\n[1] 5.557143 5.300000 5.625000\n\n\nNote: Strictly speaking, the population parameter is referred to as the ‘pseudomedian’ and the sample parameter as the ‘Hodges-Lehmann estimator’..",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#mode-modal-value",
    "href": "slides/03-statparams.html#mode-modal-value",
    "title": "03-Statistical Parameters",
    "section": "Mode (modal value)",
    "text": "Mode (modal value)\n\n\nmost frequent value of a sample\nstrict definition only valid for discrete (binary, nominal, ordinal) scales\nextension to continuous scale: binning or density estimation\n\nFirst guess: middle of most-frequent class.",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#mode-weighting-formula",
    "href": "slides/03-statparams.html#mode-weighting-formula",
    "title": "03-Statistical Parameters",
    "section": "Mode: weighting formula",
    "text": "Mode: weighting formula\n\n\\[\\begin{align}\n   D &= x_{lo}+\\frac{f_k-f_{k-1}}{2f_k-f_{k-1}-f_{k+1}}\\cdot w \\\\\n   D &= 18 + \\frac{29 - 15}{2 \\cdot 29 - 15 - 26} \\cdot 2 = 19.65\n\\end{align}\\]\n\\(f\\): class frequency, \\(w\\): class width\n\\(k\\): the index of the most abundant class, \\(x_{lo}\\) its lower limit.",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#mode-density-estimation",
    "href": "slides/03-statparams.html#mode-density-estimation",
    "title": "03-Statistical Parameters",
    "section": "Mode: density estimation",
    "text": "Mode: density estimation\n\nSomewhat more computer intensive, where the mode is the maximum of a kernel density estimate.\nThe mode from the density estimate is then \\(D=19.42\\).",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#multi-modal-distribution",
    "href": "slides/03-statparams.html#multi-modal-distribution",
    "title": "03-Statistical Parameters",
    "section": "Multi-modal distribution",
    "text": "Multi-modal distribution\n\nExample: fish population with several age classes, cohorts)",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#measures-of-variation",
    "href": "slides/03-statparams.html#measures-of-variation",
    "title": "03-Statistical Parameters",
    "section": "Measures of variation",
    "text": "Measures of variation\nVariance\n\\[\n  s^2_x = \\frac{SQ}{df}=\\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n-1}\n\\]\n\n\\(SQ\\): sum of squared differences from the mean \\(\\bar{x}\\)\n\\(df = n-1\\): degrees of freedom, \\(n\\): sample size\n\nStandard deviation\n\\[s=\\sqrt{s^2}\\] \\(\\rightarrow\\) same unit as the mean \\(\\bar{x}\\), so they can be directly compared.\n\n\nIn practice, \\(s^2\\) is often computed with:\n\\[\n  s^2_x = \\frac{\\sum{(x_i)^2}-(\\sum{x_i})^2/n}{n-1}\n\\]",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#coefficient-of-variation-cv",
    "href": "slides/03-statparams.html#coefficient-of-variation-cv",
    "title": "03-Statistical Parameters",
    "section": "Coefficient of variation (\\(cv\\))",
    "text": "Coefficient of variation (\\(cv\\))\nIs the relative standard deviation:\n\n\\[\n  cv=\\frac{s}{\\bar{x}}\n\\]\n\n\nuseful to compare variations of different variables, independent of their measurement unit\nonly applicable for data with ratio scale, i.e. with an absolute zero (like meters)\nnot for variables like Celsius temperature or pH.\n\nExample\nLet’s assume we have the discharge of two rivers, one with a \\(cv=0.3\\), another one with \\(cv=0.8\\). We see that the 2nd has more extreme variation.",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#range",
    "href": "slides/03-statparams.html#range",
    "title": "03-Statistical Parameters",
    "section": "Range",
    "text": "Range\n\nThe range measures the difference between maximum and minimum of a sample:\n\n\\[\n  r_x = x_{max}-x_{min}\n\\]\n\n\nDisadvantage: very sensitive against outliers.",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#interquartile-range",
    "href": "slides/03-statparams.html#interquartile-range",
    "title": "03-Statistical Parameters",
    "section": "Interquartile range",
    "text": "Interquartile range\n\n\nIQR or \\(I_{50}\\) omits smallest and biggest 25%\nsample size of at least 12 values recommended\n\n\\[\n  I_{50}=Q_3-Q_1=P_{75}-P_{25}\n\\]\nOrdered sample\n\n\\(Q_1\\), \\(Q_3\\): 1st and 3rd quartiles\n\\(P_{25}, P_{75}\\): 25th and 75th percentile\ntypically used in boxplots\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor normally distributed samples, fixed relationship between \\(I_{50}\\) and \\(s\\):\n\\[\n  \\sigma = E(I_{50}/(2\\Phi^{-1}(3/4))) \\approx E(I_{50}/1.394) % 2*qnorm(3/4))\n\\]\nwhere \\(\\Phi^{-1}\\) is the quantile function of the normal distribution.",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#median-absolute-deviation",
    "href": "slides/03-statparams.html#median-absolute-deviation",
    "title": "03-Statistical Parameters",
    "section": "Median absolute deviation",
    "text": "Median absolute deviation\n\nThe median of the absolute differences between median and values.\n\\[\n  MAD = \\text{median}(|\\text{median} - x_i|)\n\\]\n\nfrequently used in some communities, rarely used in our field\nsome programs rescale the MAD with a factor \\(1.4826\\) to approximate the standard deviation.\n\n\\(\\rightarrow\\) Be careful and check the software docs!",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/03-statparams.html#standard-error-of-the-mean",
    "href": "slides/03-statparams.html#standard-error-of-the-mean",
    "title": "03-Statistical Parameters",
    "section": "Standard error of the mean",
    "text": "Standard error of the mean\n\n\\[\n  s_{\\bar{x}}=\\frac{s}{\\sqrt{n}}\n\\]\n\n\nmeasures the accuracy of the mean\nplays a central role for calculation of confidence intervals and statistical tests\n\nRule of thumb for a sample size of about \\(n &gt; 30\\):\n\n“Two sigma rule”: the true mean is with 95% in the range of \\(\\bar{x} \\pm 2 s_\\bar{x}\\)",
    "crumbs": [
      "Basic Statistics",
      "03-Statistical Parameters"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#goals-of-the-course",
    "href": "slides/01-introduction.html#goals-of-the-course",
    "title": "01-Introduction",
    "section": "Goals of the course",
    "text": "Goals of the course\n\n\nIntroduction to “Data Science”\nStatistical concepts and selected methods\n\nStatistical parameters\nDistributions and probability\nStatistical tests\nModel selection\n\nPractical experience\n\nData strutures\nBasics of the R language\nApplications with real and simulated data sets\n\n\n\\(\\Rightarrow\\) Practical understanding and “statistical feeling”,\n\\(\\rightarrow\\) More important than facts learned by heart.",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#topics",
    "href": "slides/01-introduction.html#topics",
    "title": "01-Introduction",
    "section": "Topics",
    "text": "Topics\n\n\nBasic Concepts of Statistics\nAn Introduction to R\nStatistical Parameters and Distributions\nLinear Models\nAnalysis of Variance\nNonlinear Regression\nTime Series Analysis\n(Multivariate Statistics)",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#material",
    "href": "slides/01-introduction.html#material",
    "title": "01-Introduction",
    "section": "Material",
    "text": "Material\n\n\nSlides, Tutorials: tpetzoldt.github.io/elements\nExercises: tpetzoldt.github.io/element-labs\n\n\n\\(\\rightarrow\\) Slides and exercises are regularly updated, depending on the progress of the course. Comments are welcome.\n\n\n\nWritten exam at the end of the semester\n\\(\\rightarrow\\) &gt; 50% practical questions\n\\(\\rightarrow\\) Attend the labs!\n\n\n\nQuestions?",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#an-introductory-example",
    "href": "slides/01-introduction.html#an-introductory-example",
    "title": "01-Introduction",
    "section": "An introductory example",
    "text": "An introductory example\nDaily average discharge of River Elbe, pegel Dresden, river km 55.6\ndate,       discharge\n1806-01-01,  472\n1806-01-02, 1050\n1806-01-03, 1310\n1806-01-04, 1020\n1806-01-05,  767\n1806-01-06,  616\n...\n2020-10-11,  216\n2020-10-12,  204\n2020-10-13,  217\n2020-10-14,  288\n2020-10-15,  440\n2020-10-16,  601\n2020-10-17,  570\n2020-10-18,  516\n2020-10-19,  450\n2020-10-20,  422\n2020-10-21,  396\n2020-10-22,  372\n2020-10-23,  356\n2020-10-24,  357\n2020-10-25,  332\n2020-10-26,  303\n2020-10-27,  302\n2020-10-28,  316\n2020-10-29,  321\n2020-10-30,  331\n2020-10-31,  353\n2020-11-01,  395\n\\(&gt;\\) 70,000 measurements. How can we analyse this and what does it mean?\nData Source: Bundesanstalt für Gewässerkunde",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#plot-the-last-20-years",
    "href": "slides/01-introduction.html#plot-the-last-20-years",
    "title": "01-Introduction",
    "section": "Plot the last 20 years",
    "text": "Plot the last 20 years\n\nDischarge of the Elbe River, gauge station Dresden, data source BfG",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#what-do-these-data-tell-us",
    "href": "slides/01-introduction.html#what-do-these-data-tell-us",
    "title": "01-Introduction",
    "section": "What do these data tell us?",
    "text": "What do these data tell us?\n\n\nWhat is the average discharge? → mean values\nHow much variation is in the data? → variance\nHow likely are droughts or floods? → distribution\nHow precise are our forecasts? → confidence intervals\nWhich factors influence discharge? → correlations",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#how-to-start",
    "href": "slides/01-introduction.html#how-to-start",
    "title": "01-Introduction",
    "section": "How to start",
    "text": "How to start\n\n\nMean value: 224\nMedian value: 224\nStandard deviation: 253\nRange: 2, 4500\n\nWhich of these parameters are most appropriate?",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#graphics",
    "href": "slides/01-introduction.html#graphics",
    "title": "01-Introduction",
    "section": "Graphics",
    "text": "Graphics",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#boxplots",
    "href": "slides/01-introduction.html#boxplots",
    "title": "01-Introduction",
    "section": "Boxplots",
    "text": "Boxplots\n\n\nNote the log scale of y!\nIn the right version, whiskers extend to the most extreme data point which is no more than 1.5 times the interquartile range from the box.",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#three-ways-to-work-with-statistics",
    "href": "slides/01-introduction.html#three-ways-to-work-with-statistics",
    "title": "01-Introduction",
    "section": "Three ways to work with statistics",
    "text": "Three ways to work with statistics\nDescriptive statistics and graphics\n\nplots, like in the examples\nmean values, standard deviations, …\ninterpret raw data\n\nHypothesis testing\n\ndistinguish effects from random fluctuations\nmake results more convincing\n\nStatistical modelling\n\nmeasure size of effects (e.g. climate trends)\nbuild models that aggregate dependencies\nmachine learning",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#statistical-hypothesis-testing",
    "href": "slides/01-introduction.html#statistical-hypothesis-testing",
    "title": "01-Introduction",
    "section": "Statistical hypothesis testing",
    "text": "Statistical hypothesis testing\n\nHow likely is it, that our hypothesis is true?\n\n\nTurn scientific into statistical hypothesis\nEstimate probability (p value) of a given hypothesis\n\nExamples\n\nIs a medical treatment successful or not? → \\(\\chi^2\\)-test\nDoes a specific food diet increase yield of a fish farm? → t-test\nWhich factors (e.g. food, temperature, pH) of a combined treatment influence growth of aquatic animals? → ANOVA\n(How) does observed algal biomass depend on phosphorus?",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#statistical-modeling",
    "href": "slides/01-introduction.html#statistical-modeling",
    "title": "01-Introduction",
    "section": "Statistical modeling",
    "text": "Statistical modeling\n\nFit a statistical model to the data\n\nSelect proper modelling strategy\nDesign statistical models\nMeasure effect size\nSelect the optimum model between different model candidates\n\nExamples\n\nFit a distribution to annual discharge data to estimate the 100 year flood.\nFit an ANOVA model to experimental data to see which factors influence the result most.\nFit a multiple linear model to climate data to see how much climate trends differ between geographical location.",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#example-compare-two-mean-values",
    "href": "slides/01-introduction.html#example-compare-two-mean-values",
    "title": "01-Introduction",
    "section": "Example: Compare two mean values",
    "text": "Example: Compare two mean values\n\n\nA given data set (Dobson, 1983) contains the birth weight (in g) of 12 boys and 12 girls.\nHas the weight difference something to do with the gender of the babies or is it a purely random fluctuation?",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#example-correlation-and-regression",
    "href": "slides/01-introduction.html#example-correlation-and-regression",
    "title": "01-Introduction",
    "section": "Example: Correlation and regression",
    "text": "Example: Correlation and regression\n\n\n\n\n\n\n\n\n\n\nDependence of chlorophyll concentration in lakes on phosphorus, a regional data set from Koschel and Scheffler (1985) (left) and from Vollenweider and Kerekes (1980) (right).\nWhich of the two figures has greater predictive power? Why?\n\n\n\nThe parameter \\(r\\) is the Pearson correlation coefficient.",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#which-data-structure-is-better",
    "href": "slides/01-introduction.html#which-data-structure-is-better",
    "title": "01-Introduction",
    "section": "Which data structure is better?",
    "text": "Which data structure is better?\n\n\nWide format\n\n\n\n\n\n\nstation\n2021\n2022\n2023\n\n\n\n\nA\n3\n6\n16\n\n\nB\n12\n9\n20\n\n\nC\n20\n14\n14\n\n\nD\n10\n11\n14\n\n\nE\n15\n15\n3\n\n\n\n\n\n\nLong format\n\n\n\n\n\n\nyear\nstation\nvalue\n\n\n\n\n2021\nA\n3\n\n\n2021\nB\n12\n\n\n2021\nC\n20\n\n\n2021\nD\n10\n\n\n2021\nE\n15\n\n\n2022\nA\n6\n\n\n2022\nB\n9\n\n\n2022\nC\n14\n\n\n2022\nD\n11\n\n\n2022\nE\n15\n\n\n2023\nA\n16\n\n\n2023\nB\n20\n\n\n2023\nC\n14\n\n\n2023\nD\n14\n\n\n2023\nE\n3",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#example-an-algae-growth-experiment",
    "href": "slides/01-introduction.html#example-an-algae-growth-experiment",
    "title": "01-Introduction",
    "section": "Example: An algae growth experiment",
    "text": "Example: An algae growth experiment\n\nWide format\n\n\n\n\nTable 1: Growth of algae within 4 days (relative units)\n\n\n\n\n\n\ntreat\nreplicate 1\nreplicate 2\nreplicate 3\n\n\n\n\nFertilizer\n0.020\n-0.217\n-0.273\n\n\nF. open\n0.940\n0.780\n0.555\n\n\nF.+sugar\n0.188\n-0.100\n0.020\n\n\nF.+CaCO3\n0.245\n0.236\n0.456\n\n\nBas.med.\n0.699\n0.727\n0.656\n\n\nA.dest\n-0.010\n0.000\n-0.010\n\n\nTap water\n0.030\n-0.070\nNA\n\n\n\n\n\n\n\n\n\n\n\nNA means “not available”, i.e. a missing value",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#data-in-long-format",
    "href": "slides/01-introduction.html#data-in-long-format",
    "title": "01-Introduction",
    "section": "Data in long format",
    "text": "Data in long format\n\n\nAdvantages\n\nlooks “stupid” but is better for data analysis\ndependent variable growth and explanation variable treat clearly visible\nmodel formula: growth ~ treat\neasily extensible to \\(&gt;1\\) explanation variable\n\n\n\n\n\n\n\ntreat\nrep\ngrowth\n\n\n\n\nFertilizer\n1\n0.020\n\n\nFertilizer\n2\n-0.217\n\n\nFertilizer\n3\n-0.273\n\n\nF. open\n1\n0.940\n\n\nF. open\n2\n0.780\n\n\nF. open\n3\n0.555\n\n\nF.+sugar\n1\n0.188\n\n\nF.+sugar\n2\n-0.100\n\n\nF.+sugar\n3\n0.020\n\n\nF.+CaCO3\n1\n0.245\n\n\nF.+CaCO3\n2\n0.236\n\n\nF.+CaCO3\n3\n0.456",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#why-using-long-format",
    "href": "slides/01-introduction.html#why-using-long-format",
    "title": "01-Introduction",
    "section": "Why using long format?",
    "text": "Why using long format?\n\nAdvantages\n\nClear and consistent:\n\navoids duplications\ndata structure easier to understand\n\nFlexible:\n\nfor various statistical analyses, e.g. ANOVA, multiple regression, time series\neasy to transform to wide formats when necessary\n\nCompatibile:\n\nmodern data analysis tools like R and Python prefer long format\ncompatible with data base systems\n\n\nTherefore:\n\nTry to avoid wide format. It can lead to inconsistencies and complications in analysis.\nTidy data before doing the analysis and convert wide to long format.\n\n\n\\(\\rightarrow\\) Lab-exercise with Elbe River time series.",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#mathematics",
    "href": "slides/01-introduction.html#mathematics",
    "title": "01-Introduction",
    "section": "Mathematics",
    "text": "Mathematics\n\n\nLinear Algebra: The foundation for many statistical methods, especially matrices and vectors.\nCalculus: Optimization problems, deriving statistical formulas, understanding function behavior.\nNumerical Analysis: Implementation of statistical methods on computers, especially with large or complex datasets.\nProbability Theory: Sampling and modeling data, understanding statistical inference, developing algorithms.\nStatistical Modeling: Regression analysis, time series analysis, Bayesian modeling, machine learning.\n\n\n\\(\\rightarrow\\) Proper use of ready-made software packages requires fundamental understanding.",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#computing",
    "href": "slides/01-introduction.html#computing",
    "title": "01-Introduction",
    "section": "Computing",
    "text": "Computing\nRequired software\n\nA spreadsheet program, Excel or LibreOffice https://www.libreoffice.org/\nThe R system for data analysis and graphics https://www.r-project.org\nRStudio for making R more user-friendly https://posit.co/download/rstudio-desktop/",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#why-r",
    "href": "slides/01-introduction.html#why-r",
    "title": "01-Introduction",
    "section": "Why R?",
    "text": "Why R?\n\n\nStatisticians call it “lingua franca” in computational statistics.\n\nExtremely powerful\nNo other system has so much statistics\nUsed in statistical research\n\nFree (OpenSource)\n\nFree to use\nFree to modify\nFree to contribute\n\nLess complicated than its first appearance:\n\nYes, it needs command line programming\nbut: already a single line can do much\nhuge number of books and online scripts\n\n\n\n\nIn contrast to other systems Copy & Paste is allowed! – just cite it.",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/01-introduction.html#books",
    "href": "slides/01-introduction.html#books",
    "title": "01-Introduction",
    "section": "Books",
    "text": "Books\nStatistics\n\nWell-readable introductions\n\nDalgaard, P., 2008: Introductory Statistics with R. Springer, New York, 2nd edition. (fulltext of the 1st edition freely available)\nVerzani, J. (2019). Using R for introductory statistics. CRC press.\n\nA very well understandable introduction into many fields of statistics, especially regression and time series analysis:\n\nKleiber, C. and Zeileis, A., 2008: Applied Econometrics with R, Springer Verlag, New York. https://link.springer.com/book/10.1007/978-0-387-77318-6\n\n\nR Programming\n\nAn introduction to data science using the modern “tidyverse” approach:\n\nWickham, H., Çetinkaya-Rundel, M and Grolemund, G, 2023: R for Data Science. Free ebook: https://r4ds.hadley.nz/\n\n\n\n\n\n\nAnd lots of material available freely on the internet …",
    "crumbs": [
      "Basic Statistics",
      "01-Introduction"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#basic-principles-and-terminology",
    "href": "slides/02-terminology.html#basic-principles-and-terminology",
    "title": "02-Basic Terminology",
    "section": "Basic Principles and Terminology",
    "text": "Basic Principles and Terminology\n\n\nGoals of statistical analyses\nDescriptive and experimental research\nThe principle of parsimony\nTypes of variables\nProbability\nSample and Population\nRandom and systematic errors\nPopulation and sample parameters",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#goals-of-statistical-analyses",
    "href": "slides/02-terminology.html#goals-of-statistical-analyses",
    "title": "02-Basic Terminology",
    "section": "Goals of statistical analyses",
    "text": "Goals of statistical analyses\n\nSummarise, condense and describe data (descriptive statistics)\n\nwork efficiently with large data sets\nEstimate statistical parameters, mean values, variation, correlation\n\nCreate hypotheses from data (explorative statistics)\n\ndata mining and explorative statistics\ngraphical methods, multivariate statistics\n\nTest Hypotheses (statistical inference)\n\nclassical tests, ANOVA, correlation, . . .\nmodel selection\n\nPlan research (experimental design)\n\neffect size compared to random error\nexperimental layout and required sample size\n\nStatistical modelling\n\nmeasure effect size, find best explanation for a problem\npattern recognition, forecasting, machine learning",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#descriptive-or-experimental-research",
    "href": "slides/02-terminology.html#descriptive-or-experimental-research",
    "title": "02-Basic Terminology",
    "section": "Descriptive or experimental research",
    "text": "Descriptive or experimental research\nDescriptive Research\n\nFind effects and relationships between data.\n\nobservation, monitoring, correlations\nthe research subject is not manipulated\n\n\nExperimental Research\n\nCan an expected effect be reproduced?\n\nmanipulation of single conditions\nelimination of disturbances (controlled boundary conditions)\nexperimental design as simple as possible\n\n\nStrong inference requires clear hypothesis and experimental research.\nWeak inference derived from observations and data.\n\\(\\rightarrow\\) descriptive research delivers the data for creating the hypotheses.",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#the-principle-of-parsimony",
    "href": "slides/02-terminology.html#the-principle-of-parsimony",
    "title": "02-Basic Terminology",
    "section": "The principle of parsimony",
    "text": "The principle of parsimony\nAttributed to an English philosopher from the 14th century (“Occams razor”)\n\nWhen you have two competing theories that make exactly the same predictions, the simpler one is the better.\n\nIn the context ofstatistical analysis and modeling:\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments should rely on only few assumptions\nmodels should be simplified until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\nOne of the most important scientific principles\n\\(\\rightarrow\\) But nature is complex, over-simplification has to be avoided.\n\nneeds critical reflection and discussion",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#variables-and-parameters",
    "href": "slides/02-terminology.html#variables-and-parameters",
    "title": "02-Basic Terminology",
    "section": "Variables and parameters",
    "text": "Variables and parameters\n\n\ny = a + b \\(\\cdot\\) x\n\n\n\nvariables: everything that is measured or experimentally manipulated, e.g phosphorus concentration in a lake, air temperature, or abundance of animals.\nparameters: values that are estimated by a statistical model, e.g. mean, standard deviation, slope of a linear model.\n\nIndependent variables (explanation variables, predictors)\n\nare manually controlled or assumed to result from non-controllable factors\n\nDependent variables (response variables, target variables, predicted variables)\n\nthe variables of interest that we try to understand.",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#scales-of-variables",
    "href": "slides/02-terminology.html#scales-of-variables",
    "title": "02-Basic Terminology",
    "section": "Scales of variables",
    "text": "Scales of variables\n\n\nBinary (boolean variable): exactly two states: true/false, 1/0, present or absent.\nNominal: named entities, no order, {red, yellow, green}, list of species.\nOrdinal variables (ranks, ordered factors): values or terms with an order {1., 2., 3., …}; {oligotrophic, mesotrophic, eutrophic, polytrophic, hypertrophic}, but not “dystrophic”\nMetric: continuous (ideally without steps). Two sub-types:\n\nInterval scale: allows comparison and differences, but ratios make no sense. (20°C is 10 degrees warmer than als 10°C, but not double)\nRatio scale: data with an absolute zero, ratios make sense.\nA tree with 2m has double the hight of a tree with 1m.\n\n\nThe “level” of variables increases from binary to ratio scale. It is always possible to convert a higher to a lower level.",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#transformation-of-scales",
    "href": "slides/02-terminology.html#transformation-of-scales",
    "title": "02-Basic Terminology",
    "section": "Transformation of scales",
    "text": "Transformation of scales\n\nThe “level” of variables increases from binary to ratio scale. It is always possible to convert a higher to a lower level scale:\n\nmetric \\(\\rightarrow\\) ordinal: ranking\nmetric or ordinal \\(\\rightarrow\\) binary: threshold\nnominal \\(\\rightarrow\\) binary: assign to two groups\n\nTransformation to a lower scale results in a certain amount of information loss, but allows to use additional methods from the lower-level scale.\nExplanation: If we apply rank correlation to metric data, we essentially apply a method for the ordinal scale to metric data. In this case, we loose information about the differences between the values, but also decrease influence of extreme values and outliers.\nTransformation from metric to binary can be useful, if the metric data are not precise enough. So for example, counting animals (e.g. wolves) in a certain area may depend on too many factors (structure of the landscape, experience of people, season etc.) so that the exact numbers (abundances) are questionable. In such cases, transformation to a binary scale (present/absent) and using a respective test (e.g. logistic regression or Fisher’s exact test) will be more reliable.\nOther examples are the comparison of floods between different rivers, e.g. a large and a small ones, or occurrences of genes in a molecular biological analysis.",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#probability",
    "href": "slides/02-terminology.html#probability",
    "title": "02-Basic Terminology",
    "section": "Probability",
    "text": "Probability\n\nClassical definition\n\nprobability \\(p\\) is the chance of a specific event:\n\n\\[\np = \\frac{\\text{number of selected cases}}{\\text{number of all possible cases}}\n\\]\n\n1 or 6 on a dice \\(p=2/6\\)\nproblem if denominator becomes infinite\n\nAxiomatic definition\n\nAxiom I: \\(0 \\le p \\le 1\\)\nAxiom II: impossible events have \\(p=0\\), safe events have \\(p=1\\)\nAxiom III: for mutually exlusive events \\(A\\) and \\(B\\), i.e. in set theory \\(A \\bigcap B = \\emptyset\\) holds: \\(p(A \\bigcup B)= p(A) + p(B)\\)",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#sample-and-population",
    "href": "slides/02-terminology.html#sample-and-population",
    "title": "02-Basic Terminology",
    "section": "Sample and Population",
    "text": "Sample and Population\n\nSample\nSubjects, from which we have measurements or observations\n\nPopulation\nSet of all subjects that had the same chance to become part of the sample.\n\\(\\Rightarrow\\) The population is defined by the way how samples are taken\n\\(\\Rightarrow\\) Samples should be representative for our intended observational subject.",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#sampling-strategies",
    "href": "slides/02-terminology.html#sampling-strategies",
    "title": "02-Basic Terminology",
    "section": "Sampling strategies",
    "text": "Sampling strategies\n\nRandom sampling\n\nIndividuals are selected at random from a given population.\nExamples:\n\nRandom selection of sample sites on a grid.\nRandom placement of experimental units on a shelf.\n\n\n Stratified sampling\n\nThe population is subdivided into classes of similar subjects (strata).\nThe strata are separately analysed and then the the information is weighted and combined to infer about the population.\nStratified sampling requires information about the size and representativity of the strata.\nExamples: election forecasts, depth layers in a lake, age classes for animals.",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#random-and-systematic-errors",
    "href": "slides/02-terminology.html#random-and-systematic-errors",
    "title": "02-Basic Terminology",
    "section": "Random and systematic errors",
    "text": "Random and systematic errors\n\nRandom errors\n\ncan be estimated with statistical methods\nare eliminated if sample size is large\nin large samples, big and small errors average out\n\nSystematic Errors also called bias\n\ncan often not easily be estimated with statistical methods alone\nknowledge about the considered system\nelimination requires calibration using standards, blind values or pairing",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#population-and-sample-parameters",
    "href": "slides/02-terminology.html#population-and-sample-parameters",
    "title": "02-Basic Terminology",
    "section": "Population and sample parameters",
    "text": "Population and sample parameters\n\n“True” parameters of the population\n\nsymbolized with greek letters, (\\(\\mu, \\sigma, \\gamma\\, \\alpha, \\beta\\))\nusually unknown\nestimated from a sample\n\n“Calculated” parameters from a sample\n\nsymbolized with latin letters (\\(\\bar{x}\\), \\(s\\), \\(r^2\\), …)\nthe calculation is done from a sample\nstatisticians say “estimation” instead of “calculation”\nparameters can themselves be treated as a random variable",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/02-terminology.html#expected-value",
    "href": "slides/02-terminology.html#expected-value",
    "title": "02-Basic Terminology",
    "section": "Expected value",
    "text": "Expected value\n\nA single measurement \\(x_i\\) of a random variable \\(X\\) can be written as the sum of the expected value \\(\\mathbf{E}(X)\\) of the random variable and a random error \\(\\varepsilon_i\\).\n\\[\\begin{align}\n  x_i &= \\mathbf{E}(X) + \\varepsilon_i\\\\\n  \\mathbf{E}(\\varepsilon)&=0\n\\end{align}\\]\nExample:\n\nfor a fair dice with 6 eyes, true mean \\(\\mu\\) should be 3.5\nin reality it is not exactly known if the dice is a perfect cubus\n\nExample: 3 people with 5 trials:\n\n\n\nsample 1:  3 3 2 4 1  mean: 2.6\n\n\nsample 2:  6 1 1 6 1  mean: 3\n\n\nsample 3:  6 5 6 6 5  mean: 5.6\n\n\n\nOverall mean: \\(\\bar{x} = 3.73\\) is close to \\(\\mu = 3.5\\).",
    "crumbs": [
      "Basic Statistics",
      "02-Basic Terminology"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#probability-distributions",
    "href": "slides/04-distributions.html#probability-distributions",
    "title": "04-Distributions",
    "section": "Probability Distributions",
    "text": "Probability Distributions\n\nDefinition\n\na mathematical function\nprobabilities of occurrence of different possible outcomes for an experiment\n\n\\(\\rightarrow\\) https://en.wikipedia.org/wiki/Probability_distribution\n\nCharacteristics\n\na specific shape (distribution type, a mathematical formula)\ncan be described by its parameters (e.g. mean \\(\\mu\\) and standard deviation \\(\\sigma\\)).\n\nProbability distributions are one of the core concepts in statistics and many statistics courses start with coin tossing1 or dice rolls. We begin with a small classroom experiment.\nWe expect that the chances are 50:50. Researchers found out that there is a very small deviation, see \\(\\rightarrow\\) youtube video",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#what-is-your-favorite-number",
    "href": "slides/04-distributions.html#what-is-your-favorite-number",
    "title": "04-Distributions",
    "section": "What is your favorite number?",
    "text": "What is your favorite number?\nIn a classroom experiment, students of an international course were asked for their favorite number from 1 to 9.\n\n\n\n\n\nnumber\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nfreqency\n0\n1\n5\n5\n6\n4\n12\n3\n3\n\n\n\n\n\n\nThe resulting distribution is:\n\nempirical: data from an experiment\ndiscrete: only discrete numbers (1, 2, 3 …, 9) possible, no fractions",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#computer-simulations",
    "href": "slides/04-distributions.html#computer-simulations",
    "title": "04-Distributions",
    "section": "Computer simulations",
    "text": "Computer simulations\n\nInstead of real-world experiments, we can also use simulated random numbers.\n\nadvantage: we can simulate data from distributions with known properties\nchallenge: somewhat abstract\n\nPurpose\n\nget a feeling about randomness, how a sample following a given “theory” can look like\nexplore and test statistical methods and train understanding\na tool for experimental design\ntesting application and power of an analysis beforehand\n\n\n\\(\\rightarrow\\) Simulation: important tool for statistical method development and understanding!",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#continuos-uniform-distribution-mathbfu0-1",
    "href": "slides/04-distributions.html#continuos-uniform-distribution-mathbfu0-1",
    "title": "04-Distributions",
    "section": "Continuos uniform distribution \\(\\mathbf{U}(0, 1)\\)",
    "text": "Continuos uniform distribution \\(\\mathbf{U}(0, 1)\\)\n\n\nsame probability of occurence in a given interval\ne.g. \\([0, 1]\\)\nin R: runif, random, uniform\n\n\nrunif(10)\n\n [1] 0.7543490 0.3742862 0.7793269 0.5062845 0.7014560 0.9209131 0.3238158\n [8] 0.5593060 0.1426554 0.5665669\n\n\n \n\nbinning: subdivide values into classes",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#density-function-of-mathbfux_min-x_max",
    "href": "slides/04-distributions.html#density-function-of-mathbfux_min-x_max",
    "title": "04-Distributions",
    "section": "Density function of \\(\\mathbf{U}(x_{min}, x_{max})\\)",
    "text": "Density function of \\(\\mathbf{U}(x_{min}, x_{max})\\)\n\n\ndensity \\(f(X)\\), sometimes abbreviated as “pdf” (probability density function):\n\n\\[\nf(x) = \\begin{cases}\n         \\frac{1}{x_{max}-x_{min}} & \\text{for } x \\in [x_{min},x_{max}] \\\\\n         0                     & \\text{otherwise}\n       \\end{cases}\n\\]\n\narea under the curve (i.e. the integral) = 1.0\n100% of the events are between \\(-\\infty\\) and \\(+\\infty\\) and for \\(\\mathbf{U}(x_{min}, x_{max})\\) in the interval \\([x_{min}, y_{max}]\\)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#cumulative-distribution-function-of-mathbfux_min-x_max",
    "href": "slides/04-distributions.html#cumulative-distribution-function-of-mathbfux_min-x_max",
    "title": "04-Distributions",
    "section": "Cumulative distribution function of \\(\\mathbf{U}(x_{min}, x_{max})\\)",
    "text": "Cumulative distribution function of \\(\\mathbf{U}(x_{min}, x_{max})\\)\n\n\nThe cdf is the integral of the density function:\n\\[\nF(x) =\\int_{-\\infty}^{x} f(x) dx\n\\] The total area (total probability) is \\(1.0\\):\n\\[\nF(x) =\\int_{-\\infty}^{+\\infty} f(x) dx = 1\n\\]\nFor the uniform distribution, it is:\n\\[\nF(x) = \\begin{cases}\n         0                     & \\text{for } x &lt; x_{min} \\\\\n         \\frac{x-x_{min}}{x_{max}-x_{min}} & \\text{for } x \\in [x_{min},x_{max}] \\\\\n         1                     & \\text{for } x &gt; x_{max}\n       \\end{cases}\n\\]",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#quantile-function",
    "href": "slides/04-distributions.html#quantile-function",
    "title": "04-Distributions",
    "section": "Quantile function",
    "text": "Quantile function\n\n… the inverse of the cumulative distribution function.\n\n\n\n\n\n\n\n\n\n\nCumulative distribution function\n\n\n\n\n\n\n\n\n\n\nQuantile function\nExample: In which range can we find 95% of a uniform distribution \\(\\mathbf{U}(40,60)\\)?",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#summary-uniform-distribution",
    "href": "slides/04-distributions.html#summary-uniform-distribution",
    "title": "04-Distributions",
    "section": "Summary: Uniform distribution",
    "text": "Summary: Uniform distribution",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#the-normal-distribution-mathbfnmu-sigma",
    "href": "slides/04-distributions.html#the-normal-distribution-mathbfnmu-sigma",
    "title": "04-Distributions",
    "section": "The normal distribution \\(\\mathbf{N}(\\mu, \\sigma)\\)",
    "text": "The normal distribution \\(\\mathbf{N}(\\mu, \\sigma)\\)\n\nof high theoretical importance due to the central limit theorem (CLT)\nresults from adding a large number of random values of same order of magnitude.\n\nThe density function of the normal distribution is mathematically beautiful.\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\, \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\]\n\nC.F. Gauss, Gauss curve and formula on a German DM banknote from 1991–2001 (Wikipedia, CC0)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#the-central-limit-theorem-clt",
    "href": "slides/04-distributions.html#the-central-limit-theorem-clt",
    "title": "04-Distributions",
    "section": "The central limit theorem (CLT)",
    "text": "The central limit theorem (CLT)\n\n\nSums of a large number \\(n\\) of independent and identically distributed random values are normally distributed, independently on the type of the original distribution.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#a-simulation-experiment",
    "href": "slides/04-distributions.html#a-simulation-experiment",
    "title": "04-Distributions",
    "section": "A Simulation experiment",
    "text": "A Simulation experiment\n\n\n\ngenerate a matrix with 100 rows and 25 columns of uniformly distributed random numbers\ncompute the row sums\n\n\npar(mfrow=c(2, 1), las=1)\nset.seed(42)\nx  &lt;- matrix(runif(25 * 100), ncol = 25)\n\n# View(x) # uncomment this to show the matrix\n\nx_sums &lt;- rowSums(x)\nhist(x)\nhist(x_sums)\n\n\\(\\rightarrow\\) row sums are approximately normal distributed",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#random-numbers-and-density-function",
    "href": "slides/04-distributions.html#random-numbers-and-density-function",
    "title": "04-Distributions",
    "section": "Random numbers and density function",
    "text": "Random numbers and density function",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#density-and-quantiles-of-the-standard-normal",
    "href": "slides/04-distributions.html#density-and-quantiles-of-the-standard-normal",
    "title": "04-Distributions",
    "section": "Density and quantiles of the standard normal",
    "text": "Density and quantiles of the standard normal\n\n\nin theory, 50% of the values are below and 50% above the mean value\n95% are between \\(\\pm 2 \\sigma\\)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#density-and-quantiles-of-the-standard-normal-1",
    "href": "slides/04-distributions.html#density-and-quantiles-of-the-standard-normal-1",
    "title": "04-Distributions",
    "section": "Density and quantiles of the standard normal",
    "text": "Density and quantiles of the standard normal",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#cumulative-distribution-function-quantile-function",
    "href": "slides/04-distributions.html#cumulative-distribution-function-quantile-function",
    "title": "04-Distributions",
    "section": "Cumulative distribution function – Quantile function",
    "text": "Cumulative distribution function – Quantile function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuantile\n1\n1.64\n1.96\n2.0\n2.33\n2.57\n3\n\\(\\mu \\pm z\\cdot \\sigma\\)\n\n\n\n\none-sided\n\n0.95\n0.975\n0.977\n0.99\n0.995\n0.9986\n\\(1-\\alpha\\)\n\n\ntwo-sided\n0.68\n0.90\n0.95\n0.955\n0.98\n0.99\n0.997\n\\(1-\\alpha/2\\)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#standard-normal-scaling-and-shifting",
    "href": "slides/04-distributions.html#standard-normal-scaling-and-shifting",
    "title": "04-Distributions",
    "section": "Standard normal, scaling and shifting",
    "text": "Standard normal, scaling and shifting\n\n\n\\(\\mu\\) is the shift parameter that moves the whole bell shaped curve along the \\(x\\) axis\n\\(\\sigma\\) is the scale parameter to stretch or compress in the direction of \\(x\\)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#standardization-z-transformation",
    "href": "slides/04-distributions.html#standardization-z-transformation",
    "title": "04-Distributions",
    "section": "Standardization (\\(z\\)-transformation)",
    "text": "Standardization (\\(z\\)-transformation)\n\nAny normal distribution can be shifted scaled to form a standard normal with \\(\\mu=0, \\sigma=1\\)\n\n\nNormal distribution\n\n\n\n\n\n\n\n\n\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\, \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n\\]\n\n \\[\nz = \\frac{x-\\mu}{\\sigma}\n\\] \\(\\longrightarrow\\) \\(\\longrightarrow\\) \\(\\longrightarrow\\)\n\nStandard normal distribution\n\n\n\n\n\n\n\n\n\n\\[\nf(x) = \\frac{1}{\\sqrt{2\\pi}} \\, \\mathrm{e}^{-\\frac{1}{2}x^2}\n\\]",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#t-distribution-mathbftx-df",
    "href": "slides/04-distributions.html#t-distribution-mathbftx-df",
    "title": "04-Distributions",
    "section": "t-Distribution \\(\\mathbf{t}(x, df)\\)",
    "text": "t-Distribution \\(\\mathbf{t}(x, df)\\)\n\n\nadditional parameter “degrees of freedom” (df)\nused for confidence intervals and statistical tests\nconverges to the normal distribution for \\(df \\rightarrow \\infty\\)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#dependency-of-the-t-value-on-the-number-of-df",
    "href": "slides/04-distributions.html#dependency-of-the-t-value-on-the-number-of-df",
    "title": "04-Distributions",
    "section": "Dependency of the t-value on the number of df",
    "text": "Dependency of the t-value on the number of df\n\n\n\n\n\n\ndf\n1.00\n4.00\n9.00\n19.00\n29.00\n99.00\n999.00\n\n\nt\n12.71\n2.78\n2.26\n2.09\n2.05\n1.98\n1.96",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#logarithmic-normal-distribution-lognormal",
    "href": "slides/04-distributions.html#logarithmic-normal-distribution-lognormal",
    "title": "04-Distributions",
    "section": "Logarithmic normal distribution (lognormal)",
    "text": "Logarithmic normal distribution (lognormal)\n\n\nmany processes in nature do not follow a normal distribution\nlimited by zero on the left side\nlarge extreme values on the right side\n\nExamples: discharge of rivers, nutrient concentrations, algae biomass in a lakes",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#parent-distribution-of-the-lognormal",
    "href": "slides/04-distributions.html#parent-distribution-of-the-lognormal",
    "title": "04-Distributions",
    "section": "Parent distribution of the lognormal",
    "text": "Parent distribution of the lognormal\n\n\nlog from values of a lognormal distribution \\(\\rightarrow\\) normal parent distribution.\nlognormal distribution is described by parameters of log-transformed data \\(\\bar{x}_L\\) and \\(s_L\\)\nthe the antilog of \\(\\bar{x}_L\\) is the geometric mean",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#binomial-distribution",
    "href": "slides/04-distributions.html#binomial-distribution",
    "title": "04-Distributions",
    "section": "Binomial distribution",
    "text": "Binomial distribution\n\n\nnumber of successful trials out of \\(n\\) total trials with success probability \\(p\\).\nHow many “6” with probability \\(1/6\\) in 3 trials?\nmedicine, toxicology, comparison of percent numbers\nsimilar, but without replacement: hypergeometric distribution in lottery",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#poisson-distribution",
    "href": "slides/04-distributions.html#poisson-distribution",
    "title": "04-Distributions",
    "section": "Poisson distribution",
    "text": "Poisson distribution\n\n\ndistribution of rare events, a discrete distribution\nmean and variance are equal (\\(\\mu = \\sigma^2\\)), resulting parameter “lambda” (\\(\\lambda\\))\nExamples: bacteria counting on a grid, waiting queues, failure models\n\nQuasi-poisson if \\(\\mu \\neq \\sigma^2\\)\n\nIf \\(s^2 &gt; \\bar{x}\\): overdispersion\nif \\(s^2 &lt; \\bar{x}\\): underdispersion",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#confidence-interval",
    "href": "slides/04-distributions.html#confidence-interval",
    "title": "04-Distributions",
    "section": "Confidence interval",
    "text": "Confidence interval\n– depends only on \\(\\lambda\\) resp. the number of counted units (\\(k\\))\n\nTypical error of cell counting: 95% confidence interval\n\n\n\n\n\ncounts\n2\n3\n5\n10\n50\n100\n200\n400\n1000\n\n\nlower\n0\n1\n2\n5\n37\n81\n173\n362\n939\n\n\nupper\n7\n9\n12\n18\n66\n122\n230\n441\n1064",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#testing-for-distribution",
    "href": "slides/04-distributions.html#testing-for-distribution",
    "title": "04-Distributions",
    "section": "Testing for distribution",
    "text": "Testing for distribution\n\nSometimes we want to know whether a data set belongs to a specific type of distribution. Though this sounds easy, it appears quite difficult for theoretical reasons:\n\nstatistical tests check for deviations from the null hypothesis\nbut here we want to test the opposite, if \\(H_0\\) is true\n\nThis is in fact impossible, because “not significant” means only that a potential effect is either not existent or just too small to be detected. On the opposite, “significantly different” includes a certain probability of false positives.\nHowever, most statistical tests do not require perfect agreement with a certain distribution:\n\nt-test and ANOVA assume normality of residuals\ndue to the CLT, the distribution of sums and mean values converges to normal",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#shapiro-wilks-w-test",
    "href": "slides/04-distributions.html#shapiro-wilks-w-test",
    "title": "04-Distributions",
    "section": "Shapiro-Wilks-W-Test ?",
    "text": "Shapiro-Wilks-W-Test ?\n\\(\\rightarrow\\) Aim: tests if a sample conforms to a normal distribution\n\nx &lt;- rnorm(100)\nshapiro.test(x)\n\n\n    Shapiro-Wilk normality test\n\ndata:  x\nW = 0.99064, p-value = 0.7165\n\n\n\n\\(\\rightarrow\\) the \\(p\\)-value is greater than 0.05, so we would keep \\(H_0\\) and conclude that nothing speaks against acceptance of the normal\n\nInterpration of the Shapiro-Wilks-test needs to be done with care:\n\nfor small \\(n\\), the tes is not sensitive enough\nfar large \\(n\\) it is over-sensitive\nusing Shapiro-Wilks to check normality for t-test and ANOVA is not anymore recommended\n\n\n\nSimilarly, the \\(\\chi^2\\) (Chi-squared) or Kolmogorov-Smirnov-tests are not anymore recommended for normality testing, but still important for other test problems.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#graphical-examination-of-normality",
    "href": "slides/04-distributions.html#graphical-examination-of-normality",
    "title": "04-Distributions",
    "section": "Graphical examination of normality",
    "text": "Graphical examination of normality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(x\\): theoretical quantiles where a value should be found if the distribution is normal\n\\(y\\): normalized and ordered measured values (\\(z\\)-scores)\nscaled in the unit of standard deviations\nnormal distribution if the points follow a straight line\n\nRecommendation: Use graphical checks. Don’t trust the Shapiro Wilks!",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#transformation",
    "href": "slides/04-distributions.html#transformation",
    "title": "04-Distributions",
    "section": "Transformation",
    "text": "Transformation\n\nallows to apply methods designed for normally distributed data\nmodern methods can handle specific distributions directly, such as binomial, gamma, or Poisson\n\nTransformations for right-skewed data\n\n\\(x'=\\log(x)\\)\n\\(x'=\\log(x + a)\\)\n\\(x'=(x+a)^c\\) (\\(a\\) between 0.5 and 1)\n\\(x'=1/x\\) (“very powerful”, i.e. to extreme in most cases)\n\\(x'=a - 1/\\sqrt{x}\\) (to make scale more convenient)\n\\(x'=1/\\sqrt{x}\\) (compromise between \\(\\ln\\) and \\(1/x\\))\n\\(x'=a+bx^c\\) (very general, includes powers and roots)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#transformations-ii",
    "href": "slides/04-distributions.html#transformations-ii",
    "title": "04-Distributions",
    "section": "Transformations II",
    "text": "Transformations II\nTransformations for count data\n\n\\(x'=\\sqrt{3/8+x}\\) (counts: 1, 2, 3 \\(\\rightarrow\\) 0.61, 1.17, 1.54, 1.84, )\n\\(x'=\\lg(x+3/8)\\)\n\\(x'=\\log(\\log(x))\\) for giant numbers\n\n\\(\\rightarrow\\) consider a GLM with family Poisson or quasi-Poisson instead\nRatios and percentages\n\n\\(x'=\\arcsin \\sqrt{x/n}\\)\n\\(x'=\\arcsin \\sqrt{\\frac{x+3/8}{n+3/4}}\\)\n\n\\(\\rightarrow\\) consider a GLM with family binomial instead",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#rank-transformation",
    "href": "slides/04-distributions.html#rank-transformation",
    "title": "04-Distributions",
    "section": "Rank transformation",
    "text": "Rank transformation\nExample: Spearman correlation\n\n Data set\n\nx &lt;- c(1, 2, 3, 5, 4, 5 ,6,  7)\ny &lt;- c(1, 2, 4, 3, 4, 6, 8, 20)\n\nRanks\n\nrank(x)\n\n[1] 1.0 2.0 3.0 5.5 4.0 5.5 7.0 8.0\n\nrank(y)\n\n[1] 1.0 2.0 4.5 3.0 4.5 6.0 7.0 8.0\n\n\nTwo ways of calculation\n\ncor(x, y, method = \"spearman\")\n\n[1] 0.8915663\n\ncor(rank(x), rank(y))\n\n[1] 0.8915663",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#remember-the-central-limit-theorem-clt",
    "href": "slides/04-distributions.html#remember-the-central-limit-theorem-clt",
    "title": "04-Distributions",
    "section": "Remember: The central limit theorem (CLT)",
    "text": "Remember: The central limit theorem (CLT)\n\nSums of a large number \\(n\\) of independent and identically distributed random values are normally distributed, independently on the type of the original distribution.\n\n\nwe can use methods assuming normal normal distribution for non-normal data\n\nif we have a large data set\nif the original distribution is not “too skewed”\n\nrequired number \\(n\\) depends on the skewness of the original distribution\n\n\n\nReason: Methods like t-test or ANOVA are based on mean values.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#confidence-intervals-of-the-mean",
    "href": "slides/04-distributions.html#confidence-intervals-of-the-mean",
    "title": "04-Distributions",
    "section": "Confidence intervals of the mean",
    "text": "Confidence intervals of the mean\n\nStandard error\n\\[\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n\\]\n\nvariability of the mean is half, if we increase the sample size four times (\\(2^2\\))\n\nEstimation of the 95% confidence interval:\n\\[\nCI_{95\\%} = \\bigg(\\bar{x} - z_{0.975} \\cdot \\frac{s}{\\sqrt{n}},\n                 \\bar{x} + z_{0.975} \\cdot \\frac{s}{\\sqrt{n}}\\bigg)\n\\]\nwith \\(z_{1-\\alpha/2} = z_{0.975} =\\) \\(1.96\\).\n\n\\(\\rightarrow\\) \\(2\\sigma\\) rule\n\n\ninterval in which the true mean is found with 95% probability",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#difference-between-sample-and-confidence-intervals",
    "href": "slides/04-distributions.html#difference-between-sample-and-confidence-intervals",
    "title": "04-Distributions",
    "section": "Difference between sample and confidence intervals",
    "text": "Difference between sample and confidence intervals\n\n\nprediction interval: characterizes the distribution of the data from the parameters of the sample (e.g. mean, standard deviation). It estimates the range where a single, future observation will likely fall.\nstandard deviation \\(s_x\\) measures the variability of the original data\nreconstruct the original distribution if its type is known (e.g. normal, lognormal)\n\n\n\nconfidence interval: characterizes the precision of a statistical parameter, based on its standard error\nUsing \\(\\bar{x}\\) and \\(s_\\bar{x}\\), estimate the interval where we find \\(\\mu\\) with a certain probability\nless dependent on the original distribution of the data due to the CLT",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#use-the-t-distribution-for-small-samples",
    "href": "slides/04-distributions.html#use-the-t-distribution-for-small-samples",
    "title": "04-Distributions",
    "section": "Use the t-distribution for small samples",
    "text": "Use the t-distribution for small samples\n\\[\nCI_{95\\%} = \\bigg(\\bar{x} - t_{0.975, n-1} \\cdot \\frac{s}{\\sqrt{n}},\n                 \\bar{x} + t_{0.975, n-1} \\cdot \\frac{s}{\\sqrt{n}}\\bigg)\n\\]\n\nnecessary for small samples: \\(n\\lessapprox 30\\), \\(n-1\\) degrees of freedom\ncan also be used for \\(n&gt;30\\)\n\\(t\\)-quantile can be found in tables or calculated with the qt()function in R.\n\nExample with \\(\\mu=50\\) and \\(\\sigma=10\\):\n\nset.seed(123)\nn &lt;- 10\nx &lt;- rnorm(n, 50, 10)\nm &lt;- mean(x); s &lt;- sd(x)\nse &lt;- s/sqrt(n)\n# lower and upper confidence limits\nm + qt(c(0.025, 0.975), n-1) * se\n\n[1] 43.92330 57.56922\n\n\n\\(\\rightarrow\\) the true mean (\\(\\mu\\)=50) is in the interval CI = (43.9, 57.6).",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#outliers",
    "href": "slides/04-distributions.html#outliers",
    "title": "04-Distributions",
    "section": "Outliers",
    "text": "Outliers\n\nextremely large or extremely small values are sometimes called “outliers”\nbut, potential outliers can be “extreme values” from a skewed distribution. Excluding them, can be scientific misconduct.\na “true” outlier is a value that is not from the population we want to analyze, e.g. a serious measurement error if someone forgot to add a chemical in an analysis.\nit can also be something interesting, e.g. the result of new phenomenon\n\n\\(\\Rightarrow\\) It can be wrong to exclude values only because they are “too big” or “too small”.\n\\(\\rightarrow\\) Try to find the reason, why values are extreme!\n\n\\(4 \\sigma\\)-rule\n\ncheck if a value is more that 4 standard deviations away from the mean value.\nsample size should be \\(n \\ge 10\\), \\(\\bar{x}\\) and \\(s\\) are calculated without the potential outlier.\nsimilar “rules of thumb” can be found in statistics textbooks.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#outlier-test-for-linear-models-with-bonferroni-correction",
    "href": "slides/04-distributions.html#outlier-test-for-linear-models-with-bonferroni-correction",
    "title": "04-Distributions",
    "section": "Outlier test for linear models with Bonferroni correction",
    "text": "Outlier test for linear models with Bonferroni correction\n\nFor linear models and GLMs we can use the Bonferroni outlier test from package car.\n\n\nlibrary(car)\nx &lt;- c(rnorm(20), 12) # the 21st value (=12) is an outlier\noutlierTest(lm(x~1))  # x ~ 1 is the null model\n\n   rstudent unadjusted p-value Bonferroni p\n21 11.66351         4.1822e-10   8.7826e-09\n\n\n\\(\\rightarrow\\) The 21st value is identified as an outlier:\n Alternative to outlier tests\n\nuse robust parameters and methods,\n\ne.g. median or trimmed mean instead of the arithmetic mean,\nrobust linear regression rlm instead of lm\nrank-based methods like Spearman correlation\n\nImportant outliers may be omitted in an analysis, but the the number and extent of outliers must be mentioned!",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#extreme-values-in-boxplots",
    "href": "slides/04-distributions.html#extreme-values-in-boxplots",
    "title": "04-Distributions",
    "section": "Extreme values in boxplots",
    "text": "Extreme values in boxplots\n\n\nextreme values outside the whiskers if more than 1.5 times distant from the box limits, compared to the width of the interquartile box.\nsometimes called “outliers”.\nI prefer the term “extreme value”, because they can be regular observations from a skewed or heavy tailed distribution.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#example",
    "href": "slides/04-distributions.html#example",
    "title": "04-Distributions",
    "section": "Example",
    "text": "Example\n\npar(mfrow=c(1, 3), las=1)\nelbe &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/elbe.csv\")\ndischarge &lt;- elbe$discharge\nboxplot(discharge, main=\"Boxplot of discharge\")\nhist(discharge)\nhist(log(discharge - 70))\n\n\nDischarge data of the Elbe River in Dresden in \\(\\mathrm m^3 s^{-1}\\), data source: Bundesanstalt für Gewässerkunde (BFG), see terms and conditions.\n\nleft: large number of extreme values, are these outliers?\nmiddle: distribution is right-skewed\nright: transformation (3-parametric lognormal) \\(\\rightarrow\\) symmetric distribution, no outliers!",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#more-in-the-labs",
    "href": "slides/04-distributions.html#more-in-the-labs",
    "title": "04-Distributions",
    "section": "More in the labs …",
    "text": "More in the labs …\n\nhttps://tpetzoldt.github.io/element-labs/",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/06-linear.html#the-linear-model",
    "href": "slides/06-linear.html#the-linear-model",
    "title": "06-Linear Regression",
    "section": "The linear model",
    "text": "The linear model\n\\[\ny_i = \\alpha + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_p x_{i,p} + \\varepsilon_i\n\\]\n\nFundamental for many statistical methods\n\nlinear regression including some (at a first look) “nonlinear” functions\nANOVA, ANCOVA, GLM (simultanaeous testing of multiple samples or multiple factors)\nmultivariate statistics (e.g. PCA)\ntime series analysis (e.g. ARIMA)\nimputation (estimation of missing values)",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#method-of-least-squares",
    "href": "slides/06-linear.html#method-of-least-squares",
    "title": "06-Linear Regression",
    "section": "Method of least squares",
    "text": "Method of least squares\n\\[\nRSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n  \\varepsilon^2 \\qquad \\text{(residual sum of squares)}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align}\n  \\text{total variance} &= \\text{explained variance} &+& \\text{residual variance}\\\\\n                    s^2_y &= s^2_{y|x}               &+& s^2_{\\varepsilon}\n\\end{align}\\]",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#the-coefficient-of-determination",
    "href": "slides/06-linear.html#the-coefficient-of-determination",
    "title": "06-Linear Regression",
    "section": "The coefficient of determination",
    "text": "The coefficient of determination\n\\[\\begin{align}\n     r^2 & = \\frac{\\text{explained variance}}{\\text{total variance}}\\\\\n         & = \\frac{s^2_{y|x}}{s^2_y}\\\\\n\\end{align}\\]\nIt can also be expressed as ratio of residual (RSS) and total (TSS) sum of squares:\n\\[\n    r^2 = 1-\\frac{s^2_{\\varepsilon}}{s^2_{y}} = 1-\\frac{RSS}{TSS} =  1- \\frac{\\sum(y_i -\\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\n\\]\n\nderived from “sum of squares”, scaled as relative variance\nidentical to the squared Pearson correlation \\(r^2\\) (in the linear case)\nvery useful interpretation: percentage of variance of the raw data explained by the model\n\nFor the example: \\(r^2= 1-\\) 15.3 \\(/\\) 40.8 \\(=\\) 0.625",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#minimization-of-rss",
    "href": "slides/06-linear.html#minimization-of-rss",
    "title": "06-Linear Regression",
    "section": "Minimization of RSS",
    "text": "Minimization of RSS\n\nAnalytical solution: minimize sum of squares (\\(\\sum \\varepsilon^2\\))\nLinear system of equations\nMinimum RSS \\(\\longleftarrow\\) partial 1st derivatives (\\(\\partial\\))\n\nFor \\(y=a \\cdot x + b\\) with 2 parameters: \\(\\frac{\\partial\\sum \\varepsilon^2}{\\partial{a}}=0\\), \\(\\frac{\\partial\\sum \\varepsilon^2}{\\partial{b}}=0\\):\n\n\\[\\begin{align}\n  \\frac{\\partial \\sum(\\hat{y_i} - y_i)^2}{\\partial a}     &= \\frac{\\partial \\sum(a + b \\cdot x_i - y_i)^2}{\\partial a} = 0\\\\\n      \\frac{\\partial \\sum(\\hat{y_i} - y_i)^2}{\\partial b} &= \\frac{\\partial \\sum(a + b \\cdot x_i - y_i)^2}{\\partial b} = 0\n\\end{align}\\]\nSolution of the linear system of equations:\n\\[\\begin{align}\nb &=\\frac {\\sum x_iy_i - \\frac{1}{n}(\\sum x_i \\sum y_i)} {\\sum x_i^2 - \\frac{1}{n}(\\sum x_i)^2}\\\\\na &=\\frac {\\sum y_i - b \\sum x_i}{n}\n\\end{align}\\]\n\nsolution for arbitrary number of parameters with matrix algebra",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#significance-of-the-regression",
    "href": "slides/06-linear.html#significance-of-the-regression",
    "title": "06-Linear Regression",
    "section": "Significance of the regression",
    "text": "Significance of the regression\n\n\\[\n\\hat{F}_{1;n-2;\\alpha}= \\frac{s^2_{explained}}{s^2_{residual}}\n                         = \\frac{r^2(n-2)}{1-r^2}\n\\] \nAssumptions\n\nValidity: the data maps to the research question\nAdditivity and linearity: \\(y = \\alpha + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots\\)\nIndependence of errors: residuals around the regression line are independent\nEqual variance of errors: residuals homogeneously distributed around the regression line\nNormality of errors: the “assumption that is generally least important”\n\nSee: Gelman & Hill (2007) : Data analysis using regression …",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#diagnostics",
    "href": "slides/06-linear.html#diagnostics",
    "title": "06-Linear Regression",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\n\nNo regression analysis without graphical diagnostics!\n\nx-y-plot with regression line: is the variance homogeneous?\nPlot of residuals vs. fitted: are there still any remaining patterns?\nQ-Q-plot, histogram, : is distribution of residuals approximately normal?\n\nUse graphical methods for normality, don’t trust the Shapiro-Wilks in that case.",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#confidence-intervals-of-the-parameters",
    "href": "slides/06-linear.html#confidence-intervals-of-the-parameters",
    "title": "06-Linear Regression",
    "section": "Confidence intervals of the parameters",
    "text": "Confidence intervals of the parameters\n\nbased on standard errors and the t-distribution, similar to CI of the mean\n\n\\[\\begin{align}\na & \\pm t_{1-\\alpha/2, n-2} \\cdot s_a\\\\\nb & \\pm t_{1-\\alpha/2, n-2} \\cdot s_b\n\\end{align}\\]\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4451 -1.0894 -0.4784  1.5065  3.1933 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.50740    0.87338   2.871   0.0102 *  \nx            2.04890    0.07427  27.589 3.51e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.885 on 18 degrees of freedom\nMultiple R-squared:  0.9769,    Adjusted R-squared:  0.9756 \nF-statistic: 761.1 on 1 and 18 DF,  p-value: 3.514e-16\n\n\n Example: CI of a: \\(a \\pm t_{1-\\alpha/2, n-2} \\cdot s_a = 2.5074 \\pm\n2.09 \\cdot 0.87338\\)",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#confidence-interval-and-prediction-interval",
    "href": "slides/06-linear.html#confidence-interval-and-prediction-interval",
    "title": "06-Linear Regression",
    "section": "Confidence interval and prediction interval",
    "text": "Confidence interval and prediction interval\n\n\nConfidence interval:\n\nShows the area where the “true regression line” is expected by 95%.\nWidth of this band decreses with increasing \\(n\\)\nanalogous to the standard error\n\nPrediction interval:\n\nShows the range, in which the prediction for a single value is expected (by 95%).\nWidth is independent of sample size \\(n\\)\nanalogous to the standard deviation",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#confidence-intervals-for-linear-regression-code",
    "href": "slides/06-linear.html#confidence-intervals-for-linear-regression-code",
    "title": "06-Linear Regression",
    "section": "Confidence intervals for linear regression: Code",
    "text": "Confidence intervals for linear regression: Code\n\n## generate example data\nx &lt;- 1:10\ny &lt;- 2 + 0.5 * x + 0.5 * rnorm(x)\n\n## fit model\nreg &lt;- lm(y ~ x)\nsummary(reg)\n\n## plot data and regression line\nplot(x,y, xlim = c(0, 10), ylim = c(0, 10), pch = 16)\nabline(reg, lwd = 2)\n\n## calcuate and plot intervals\nnewdata &lt;- data.frame(x=seq(-1, 11, length=100))\nconflim &lt;- predict(reg, newdata=newdata, interval = \"confidence\")\npredlim &lt;- predict(reg, newdata=newdata, interval = \"prediction\")\n\nlines(newdata$x, conflim[,2], col = \"blue\")\nlines(newdata$x, conflim[,3], col = \"blue\")\nlines(newdata$x, predlim[,2], col = \"red\")\nlines(newdata$x, predlim[,3], col = \"red\")\n\n\nThe variable newdata:\n\nspans the range of x values in small steps to get a smooth curve\nsingle column with exactly the same name x as in the model formula\nfor multiple regression, one column per explanation variable",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#problematic-cases",
    "href": "slides/06-linear.html#problematic-cases",
    "title": "06-Linear Regression",
    "section": "Problematic cases",
    "text": "Problematic cases",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#identification-and-treatment-of-problematic-cases",
    "href": "slides/06-linear.html#identification-and-treatment-of-problematic-cases",
    "title": "06-Linear Regression",
    "section": "Identification and treatment of problematic cases",
    "text": "Identification and treatment of problematic cases\n\nRainbow-Test (linearity)\n\n## generate test data\nx &lt;- 1:10\ny &lt;- 2 + 0.5 * x + 0.5 * rnorm(x)\n\nlibrary(lmtest)\nraintest(y~x)\n\n\n    Rainbow test\n\ndata:  y ~ x\nRain = 0.79952, df1 = 5, df2 = 3, p-value = 0.6153\n\n\n\nBreusch-Pagan-test (variance homogeneity)\n\nbptest(y~x)\n\n\n    studentized Breusch-Pagan test\n\ndata:  y ~ x\nBP = 3.3989, df = 1, p-value = 0.06524",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#non-normality-and-outliers",
    "href": "slides/06-linear.html#non-normality-and-outliers",
    "title": "06-Linear Regression",
    "section": "Non-normality and outliers",
    "text": "Non-normality and outliers\n\n\nNon-normality\n\nless important than many people think (due to the CLT)\ntransformations (e.g. Box-Cox), polynomials, periodic functions \nuse of GLM’s (generalized linear models)\n\n\n\n\nOutliers (depends on pattern)\n\nuse of transformations (e.g. double log)\nuse of outlier-tests, e.g. outlierTest from package car\nrobust regression with IWLS (iteratively re-weighted least squares) from package MASS",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#robust-regression-with-iwls",
    "href": "slides/06-linear.html#robust-regression-with-iwls",
    "title": "06-Linear Regression",
    "section": "Robust regression with IWLS",
    "text": "Robust regression with IWLS\n\n\nIWLS: iterated re-weighted least squares\nOLS (ordinary least squares) is “normal” linear regression\nM-estimation and MM-estimation are two different approaches, details in Venables & Ripley (2013)\nrobust regression is preferred over outlier exclusion",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#code-of-the-iwls-regression",
    "href": "slides/06-linear.html#code-of-the-iwls-regression",
    "title": "06-Linear Regression",
    "section": "Code of the IWLS regression",
    "text": "Code of the IWLS regression\n\nlibrary(\"MASS\")\n\n## test data with 2 \"outliers\"\nx &lt;- c(1, 2, 3, 3, 4, 5, 7, 7, 7, 8, 8, 9, 10, 14, 15, 15, 16, 17, 18, 18)\ny &lt;- c(8.1, 20, 10.9, 8.4, 9.6, 16.1, 17.3, 15.3, 16, 15.9, 19.3, \n       21.3, 24.8, 31.3, 4, 31.9, 33.7, 36.5, 42.4, 38.5)\n\n## fit the models\nssq    &lt;- lm(y ~ x)\niwls   &lt;- rlm(y ~ x)\niwlsmm &lt;- rlm(y ~ x, method = \"MM\")\n\n## plot the models\nplot(x, y, pch = 16, las = 1)\nabline(ssq, col = \"blue\", lty = \"dashed\")\nabline(iwls, col = \"red\")\nabline(iwlsmm, col = \"green\")\nlegend(\"topleft\", c(\"OLS\", \"IWLS-M\", \"IWLS-MM\"),\n       col = c(\"blue\", \"red\", \"green\"),\n       lty = c(\"dashed\", \"solid\", \"solid\"))",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/06-linear.html#references",
    "href": "slides/06-linear.html#references",
    "title": "06-Linear Regression",
    "section": "References",
    "text": "References\n\n\n\n\n\nFox, J., & Weisberg, S. (2018). An R companion to applied regression. Sage publications.\n\n\nGelman, A., & Hill, J. (2007). Data analysis using regression and multilevelhierarchical models (Vol. 1). Cambridge University Press New York, NY, USA.\n\n\nKleiber, C., & Zeileis, A. (2008). Applied econometrics with R. Springer.\n\n\nVenables, W. N., & Ripley, B. D. (2013). Modern applied statistics with S-PLUS (3rd ed.). Springer Science; Business Media.",
    "crumbs": [
      "Basic Statistics",
      "06-Linear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#nonlinear-regression",
    "href": "slides/08-nonlinear-regression.html#nonlinear-regression",
    "title": "08-Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\nMany phenomena in nature are non-linear!\nLinear or non-linear?\n\nSome non-linear problems can be solved with linear methods\ne.g., polynomials or periodic (sine and cosine) functions\nothers can be fitted by using transformations\n\nExample\n\\[y = a \\cdot x^b\\] can be transformed to:\n\\[\\ln(y) = \\ln(a) + b \\cdot \\ln(x)\\]\n\nbut: linearization transforms also the residuals\ntransformation is sometimes correct and sometimes wrong\nhomogeneity of variances",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#linearization-or-direct-nonlinear-fitting",
    "href": "slides/08-nonlinear-regression.html#linearization-or-direct-nonlinear-fitting",
    "title": "08-Nonlinear Regression",
    "section": "Linearization or direct nonlinear fitting?",
    "text": "Linearization or direct nonlinear fitting?\n\nLinearising transformations\n\nlog, square root, reciprocals …\ncan be applied if the residual variance remains (or is becoming) homogenous.\nbut in some cases: transformations lead to heteroscedasticity \\(\\Rightarrow\\) biased regression parameters.\n\nNonlinear regression\n\nvery flexible, user-defined functions,\nno transformation required\nbut: requires iterative optimization methods,\nin theory only local optima can be found,\nparameters are not in all cases identifiable.",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#nonlinear-regression-1",
    "href": "slides/08-nonlinear-regression.html#nonlinear-regression-1",
    "title": "08-Nonlinear Regression",
    "section": "Nonlinear regression",
    "text": "Nonlinear regression\n\nIterative search for the minimum of the sum of squares",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#how-to-find-the-global-minimum",
    "href": "slides/08-nonlinear-regression.html#how-to-find-the-global-minimum",
    "title": "08-Nonlinear Regression",
    "section": "How to find the global minimum?",
    "text": "How to find the global minimum?\n\nGoodness of fit: Residual sum of squared differences \\(RSS\\):\n\\[\nRSS = \\sum_{i=1}^n\\left(y_i- f(\\mathbf x_i, \\mathbf p)\\right)^2 = \\min !\n\\]\nwith \\(y\\): dependent var, \\(\\mathbf x\\): matrix of independent vars,\\(\\mathbf p\\): parameter vector, \\(n\\): sample size\nUse of an iterative solver \\(\\rightarrow\\) see next slides\nNonlinear coefficient of determination \\(R^2\\)\n\nnot related to the (linear) correlation coefficient\ncan be calculated from the residual and total variances\n\n\\[\nR^2 = 1 - \\frac{\\text{variance of the residuals}}{\\text{variance of the y-data}} = 1- \\frac{s^2_\\varepsilon}{s^2_y}\n\\]\n\nnonlinear \\(r^2\\) measures the fraction of the explained variance\n… other indices can be used in addition, e.g. MSE, RMSE, NSE, …",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#general-principle-of-optimization-algorithms",
    "href": "slides/08-nonlinear-regression.html#general-principle-of-optimization-algorithms",
    "title": "08-Nonlinear Regression",
    "section": "General principle of optimization algorithms",
    "text": "General principle of optimization algorithms\n\nThe minimum of the squared residuals (RSS) is searched by iteration:\n\n\nStart from an initial guess for the parameter set.\nTry to find a parameter set with lower RSS.\nIs the new parameter set better?\n\nNo: Reject the new parameters and goto 2\nYes: Accept the new parameters and goto 4\n\nIs the new parameter set good enough?\n\nNo: goto 2\nYes: goto 5\n\nParameter set found.",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#deterministic-methods",
    "href": "slides/08-nonlinear-regression.html#deterministic-methods",
    "title": "08-Nonlinear Regression",
    "section": "Deterministic Methods",
    "text": "Deterministic Methods\n\n\nGradient Descent\n\ngo one step into the direction of steepest descent\n\\(\\rightarrow\\) relatively simple\n\\(\\rightarrow\\) relatively robust for “more complicated’’ problems\n\nNewton’s Method\n\nquadratic approximation of the RSS function\ntry to estimate the minimum\n\\(\\rightarrow\\) takes curvature into account\n\\(\\rightarrow\\) faster for “well behaving” problems\nseveral versions: quasi-Newton, Gauss-Newton, L-BFGS, …\n\nLevenberg-Marquardt\n\ninterpolates between Gauss-Newton and gradient descent.\n\n\n  Newton method (red) uses curvature information to converge faster than gradient descent (green). Source: Wikipedia.",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#stochastic-methods",
    "href": "slides/08-nonlinear-regression.html#stochastic-methods",
    "title": "08-Nonlinear Regression",
    "section": "Stochastic Methods",
    "text": "Stochastic Methods\n\nUse statistical principles (random search)\nClassical methods\n\nSimulated annealing: simulates heating and cooling of matter \\(\\rightarrow\\) “Crystallisation process”.\nControlled random search Price (1983)\n\nEvolutionary Algorithms\n\nanalogy to genetics: mutation and selection\nfollows a “population” of several parameter sets in parallel\ninformation exchange (“crossover”) between parameter sets\n\\(\\rightarrow\\) for complicated problems with large number of parameters\nnowadays builtin in Microsoft Excel and LibreOffice Calc\n\n… and many more.",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#enzyme-kinetics",
    "href": "slides/08-nonlinear-regression.html#enzyme-kinetics",
    "title": "08-Nonlinear Regression",
    "section": "Enzyme Kinetics",
    "text": "Enzyme Kinetics\n\n… can be described with the well-known Michaelis-Menten function:\n\\[\nv = v_{max} \\frac{S}{k_m + S}\n\\]",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#linearization-vs.-true-nonlinear-regression",
    "href": "slides/08-nonlinear-regression.html#linearization-vs.-true-nonlinear-regression",
    "title": "08-Nonlinear Regression",
    "section": "Linearization vs. (true) nonlinear regression",
    "text": "Linearization vs. (true) nonlinear regression\n\nLinearizing transformation\n[&gt;] Appropriate if transformation improves homogeneity of variances  [+] Fast, simple and easy. [+] Analytical solution returns the global optimum. [-] Only a limited set of functions can be fitted. [-] Can lead to wrongly transformed error structure and biased results.\nNonlinear Regression\n[&gt;] Appropriate if error structure is already homogeneous and/or analytical solution does not exist. [+] Can be used to fit arbitrary functions, given that the parameters are identifiable. [-] Needs start values and considerable computation time. [-] Best solution (global optimum) is not guaranteed.",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#nonlinear-regression-in-r-simple-exponential",
    "href": "slides/08-nonlinear-regression.html#nonlinear-regression-in-r-simple-exponential",
    "title": "08-Nonlinear Regression",
    "section": "Nonlinear regression in R: simple exponential",
    "text": "Nonlinear regression in R: simple exponential\n\nFit model\n\n# example data\nx &lt;- 1:10\ny &lt;- c(1.6, 1.8, 2.1, 2.8, 3.5, \n       4.1, 5.1, 5.8, 7.1, 9.0)\n\n# initial parameters for the optimizer\npstart &lt;- c(a = 1, b = 1)\n\n# nonlinear least squares\nfit &lt;- nls(y ~ a * exp(b * x), start = pstart)\nsummary(fit)\n\n\nFormula: y ~ a * exp(b * x)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 1.263586   0.049902   25.32 6.34e-09 ***\nb 0.194659   0.004716   41.27 1.31e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1525 on 8 degrees of freedom\n\nNumber of iterations to convergence: 13 \nAchieved convergence tolerance: 5.956e-08\n\n\n\nPlot result\n\n# additional x-values to get a smooth curve\nx1 &lt;- seq(1, 10, 0.1)\ny1 &lt;- predict(fit, data.frame(x = x1))\nplot(x, y)\nlines(x1, y1, col = \"red\")",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#fitted-parameters",
    "href": "slides/08-nonlinear-regression.html#fitted-parameters",
    "title": "08-Nonlinear Regression",
    "section": "Fitted parameters",
    "text": "Fitted parameters\n\n\n\n\nFormula: y ~ a * exp(b * x)\n\nParameters:\n  Estimate Std. Error t value Pr(&gt;|t|)    \na 1.263586   0.049902   25.32 6.34e-09 ***\nb 0.194659   0.004716   41.27 1.31e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1525 on 8 degrees of freedom\n\nNumber of iterations to convergence: 13 \nAchieved convergence tolerance: 5.956e-08\n\n\n\nEstimate”: the fitted parameters\nStd. error: \\(s_{\\bar{x}}\\): indicates reliability of the estimate\nt- and p-values: no over-interpretation!\nin the non-linear world, “non-significant” parameters can be structurally necessary.\n\nCoefficient of determination \\(r^2 = 1-\\frac{s^2_\\varepsilon}{s^2_y}\\)\n\n(Rsquared &lt;- 1 - var(residuals(fit))/var(y))\n\n[1] 0.9965644",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics",
    "href": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics",
    "title": "08-Nonlinear Regression",
    "section": "Michaelis-Menten-Kinetics",
    "text": "Michaelis-Menten-Kinetics\n\n\nCode\n\n\nS &lt;-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)\nV &lt;-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,\n      0.0575, 0.0399, 0.0381, 0.017, 0.0253)\n\n## Michaelis-Menten kinetics\nf &lt;- function(S, Vm, K) { Vm * S/(K + S) }\n\npstart &lt;- c(Vm = 0.1, K = 5)\nmodel_fit &lt;- nls(V ~ f(S, Vm, K), start = pstart)\nsummary(model_fit)\n\nplot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))\nx1 &lt;-seq(0, 25, length = 100)\nlines(x1, predict(model_fit, data.frame(S = x1)), col = \"red\")\n\n\n Coefficient of determination\n\n(1 - var(residuals(model_fit))/var(V))\n\n[1] 0.9895147\n\n\n\nResults\n\n\n\nFormula: V ~ f(S, Vm, K)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nVm  0.11713    0.00381   30.74 1.36e-09 ***\nK   5.38277    0.46780   11.51 2.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.003053 on 8 degrees of freedom\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 6.678e-06",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics-with-selfstart",
    "href": "slides/08-nonlinear-regression.html#michaelis-menten-kinetics-with-selfstart",
    "title": "08-Nonlinear Regression",
    "section": "Michaelis-Menten-Kinetics with selfstart",
    "text": "Michaelis-Menten-Kinetics with selfstart\n\nFunction SSmicmen determines start parameters automatically.\nOnly few selfstart functions available in R\n\n\nCode\n\n\nS &lt;- c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)\nV &lt;- c(0.0998, 0.0948, 0.076, 0.0724, 0.0557,\n       0.0575, 0.0399, 0.0381, 0.017, 0.0253)\n\nmodel_fit &lt;- nls(V ~ SSmicmen(S, Vm, K))\n\nplot(S, V, xlim = c(0, max(S)), ylim = c(0, max(V)))\nx1 &lt;-seq(0, 25, length = 100)\nlines(x1, predict(model_fit, data.frame(S = x1)), col=\"red\")\n\n\nResults\n\n\nsummary(model_fit, correlation=TRUE)\n\n\nFormula: V ~ f(S, Vm, K)\n\nParameters:\n   Estimate Std. Error t value Pr(&gt;|t|)    \nVm  0.11713    0.00381   30.74 1.36e-09 ***\nK   5.38277    0.46780   11.51 2.95e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.003053 on 8 degrees of freedom\n\nCorrelation of Parameter Estimates:\n  Vm  \nK 0.88\n\nNumber of iterations to convergence: 3 \nAchieved convergence tolerance: 6.678e-06\n\n\n\n\n\n\nPlot\n\n\n\n\n\n\n\n\n\n\nNote: Correlation of parameters\n\nhigh absolute values of correlation indicate non-identifiability of parameters\ncritical value depends on the data\nsometimes, better start values or another optimization algorithm can help",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#practical-hints",
    "href": "slides/08-nonlinear-regression.html#practical-hints",
    "title": "08-Nonlinear Regression",
    "section": "Practical hints",
    "text": "Practical hints\n\n\nplot data\nfind good starting values by thinking about it or by trial and error\navoid very small and/or very large numbers \\(\\longrightarrow\\) rescale the problem to values between approx 0.001 to 1000\nstart with a simple function and add terms and parameters sequentially\nDon’t take significance of parameters too seriously. A non-significant parameter may be necessary for the structure of the model, removal of it will invalidate the whole model.",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#further-reading",
    "href": "slides/08-nonlinear-regression.html#further-reading",
    "title": "08-Nonlinear Regression",
    "section": "Further reading",
    "text": "Further reading\n\n\nPackage growthrates for growth curves: https://cran.r-project.org/package=growthrates\nPackage FME for more complex model fitting tasks (identifiability analysis, constrained optimization, multiple dependent variables and MCMC): (Soetaert & Petzoldt, 2010), https://cran.r-project.org/package=FME\nMore about optimization in R: https://cran.r-project.org/web/views/Optimization.html",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#lineweaver-burk-transformation-vs.-nonlinear-fit",
    "href": "slides/08-nonlinear-regression.html#lineweaver-burk-transformation-vs.-nonlinear-fit",
    "title": "08-Nonlinear Regression",
    "section": "Lineweaver-Burk transformation vs. nonlinear fit",
    "text": "Lineweaver-Burk transformation vs. nonlinear fit\n\nCode of the method comparison slide\n\n\nS &lt;-c(25, 25, 10, 10, 5, 5, 2.5, 2.5, 1.25, 1.25)\nV &lt;-c(0.0998, 0.0948, 0.076, 0.0724, 0.0557, \n      0.0575, 0.0399, 0.0381, 0.017, 0.0253)\nmodel_fit&lt;-nls(V ~ SSmicmen(S, Vm, K))\n\npar(mfrow=c(1,2), las=1, lwd=2)\n## Lineweaver-Burk\nx &lt;- 1/S; y &lt;- 1/V\n\nplot(x, y, xlab=\"1/S\", ylab=\"1/V\", xlim=c(-0.2,1), ylim=c(0, 80), pch=16,\n  main=\"Linearisation\\n(Lineweaver-Burk)\")\nabline(h=0, lwd=1, col=\"grey\")\nabline(v=0, lwd=1, col=\"grey\")\nm &lt;- lm(y ~ x)\nabline(m, col = \"red\")\nVm_l &lt;- 1/coef(m)[1]\nKm_l &lt;- coef(m)[2] * Vm_l\n#legend(\"topleft\", c(\"vmax = 1/intercept\", \"km = slope * vmax\"), \n#        box.lwd=1, bg=\"white\")\ntext(0.35, 75, \"1/V = 1/vmax + km / vmax * S\")\n\n## retransformed, both together\nplot(S, V, xlim = c(0, max(S)),ylim=c(0, max(V)), pch=16, main=\"No Transformation\")\nx1 &lt;-seq(0, 25, length=100)\nlines(x1, Vm_l * x1 / (Km_l + x1), col=\"red\")\nlines(x1, predict(model_fit, data.frame(S=x1)), col=\"blue\")\nlegend(\"bottomright\", legend=c(\"linearisation\", \"nonlinear\"), \n       box.lwd=1, lwd=2, col=c(\"red\", \"blue\"))\ntext(15, 0.04, \"V = S * vmax / (km + S)\")\n\n\nSee: https://en.wikipedia.org/wiki/Lineweaver-Burk_plot",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/08-nonlinear-regression.html#references",
    "href": "slides/08-nonlinear-regression.html#references",
    "title": "08-Nonlinear Regression",
    "section": "References",
    "text": "References\n\n\n\n\n\nPrice, W. L. (1977). A controlled random search procedure for global optimization. The Computer Journal, 20(4), 367–370.\n\n\nPrice, W. L. (1983). Global optimization by controlled random search. Journal of Optimization Theory and Applications, 40(3), 333–348.\n\n\nSoetaert, K., & Petzoldt, T. (2010). Inverse modelling, sensitivity and monte carlo analysis in R using package FME. Journal of Statistical Software, 33(3), 1–28. https://doi.org/10.18637/jss.v033.i03",
    "crumbs": [
      "Basic Statistics",
      "08-Nonlinear Regression"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#time-series-decomposition-1",
    "href": "slides/10-timeseries-outlook.html#time-series-decomposition-1",
    "title": "10-Time Series Outlook",
    "section": "Time series decomposition",
    "text": "Time series decomposition\n\nTraditional component model\nDecomposition of time series into:\n\ntrend component,\nseasonal or periodic component and\nstochastic component.\n\nAssumptions\n\nadditivity of components\napproximate normal distribution of residuals\nhomogeneous variance (of residuals)\n\nIf distribution is skewed, effects are “multiplicative”, or variance changes proportionally with trend \\(\\rightarrow\\), consider transformation.\nFor hydrological or biomass data with right-skewed distribution, log-transformation can be helpful.",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#smoothing-methods",
    "href": "slides/10-timeseries-outlook.html#smoothing-methods",
    "title": "10-Time Series Outlook",
    "section": "Smoothing methods",
    "text": "Smoothing methods\n\nalternative to parametric linear or non-linear regression vs. time\nsmoothers are based on moving averages\n\nExample: Annual precipitation 1900-1986, Great Lakes\n\nlibrary(Kendall)\ndata(PrecipGL)\nplot(PrecipGL, ylab=\"Precipitation (inches)\")\n# moving average with rectangular kernel\nkernel &lt;- rep(1, 10)  # change second value to play with bandwidth\nlines(stats::filter(PrecipGL, kernel/sum(kernel)), lwd = 2, col = \"blue\")",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#trend-corrected-series",
    "href": "slides/10-timeseries-outlook.html#trend-corrected-series",
    "title": "10-Time Series Outlook",
    "section": "Trend corrected series",
    "text": "Trend corrected series\n\nsmooth &lt;- stats::filter(PrecipGL, kernel/sum(kernel))\nresiduals &lt;- PrecipGL - smooth\nplot(residuals)\n\n\n\nTrend corrected series can be obtained by subtracting the trend.\nNote: use stats::filter to avoid potential conflict with dplyr::filter.",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#smoothing-of-data-with-12monthly-seasonality",
    "href": "slides/10-timeseries-outlook.html#smoothing-of-data-with-12monthly-seasonality",
    "title": "10-Time Series Outlook",
    "section": "Smoothing of data with 12monthly seasonality",
    "text": "Smoothing of data with 12monthly seasonality\n\nFor a seasonal time series with monthly values Kleiber & Zeileis (2008) recommend a filter with 13 coefficients.\nExample: water level of Rio Negro 18 km upstream from its confluence with the Amazon River.\nThe data set is contained in the package boot (Canty & Ripley, 2022):\n\n\nlibrary(boot)\ndata(manaus)\ntsp(manaus) # shows time series properties\n\n[1] 1903.000 1992.917   12.000\n\nplot(manaus)\nlines(stats::filter(manaus, c(0.5, rep(1, 11), 0.5)/12), lwd=2, col=\"blue\")",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#very-popular-method-the-lowess-filter",
    "href": "slides/10-timeseries-outlook.html#very-popular-method-the-lowess-filter",
    "title": "10-Time Series Outlook",
    "section": "Very popular method: The LOWESS filter",
    "text": "Very popular method: The LOWESS filter\n\n\nShow the code\nplot(PrecipGL)\nlines(lowess(time(PrecipGL), PrecipGL, f = 2/3), lwd = 3, col = \"blue\")\nlines(lowess(time(PrecipGL), PrecipGL, f = 1/3), lwd = 3, col = \"forestgreen\")\nlines(lowess(time(PrecipGL), PrecipGL, f = 0.1), lwd = 3, col = \"red\")\n\n\n\n\nlocally weighted polynomial regression (W. S. Cleveland, 1979)\nallows to adjust smoothness (smoother span f)\ndifferent functions available in R: lowess and loess",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#identification-of-seasonal-components",
    "href": "slides/10-timeseries-outlook.html#identification-of-seasonal-components",
    "title": "10-Time Series Outlook",
    "section": "Identification of seasonal components",
    "text": "Identification of seasonal components\n\nPeriodic phenomena are common in nature\n\nseasonal course of solar radiation and temperature\nseasonal development of vegetation\nheartbeat of animals.\n\nHarmonic analysis\n\nevery time series can be described as a sum of sine and a cosine functions with different periods (Fourier series)\n\n\\[\nx_t = a_0 + \\sum_{p=1}^{N/2-1} \\big(a_p \\cos(2 \\pi p t/N)\n                                 +    b_p \\sin(2 \\pi p t/N)\\big)\n           +a_{N/2} \\cos(\\pi t), \\qquad t=1 \\dots N\n\\]\nwith\n\\(x_t\\): equidistant time series, \\(t\\): time step, \\(N\\): number of data, \\(a_p, b_p\\): Fourier coefficients, \\(a_0 = \\bar{x}\\), \\(p\\): order of the periodic component",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#formulation-with-single-cosine-term-and-shift-phi",
    "href": "slides/10-timeseries-outlook.html#formulation-with-single-cosine-term-and-shift-phi",
    "title": "10-Time Series Outlook",
    "section": "Formulation with single cosine term and shift \\(\\Phi\\)",
    "text": "Formulation with single cosine term and shift \\(\\Phi\\)\n\n\nequation with sine and cosine terms can be transformed in an equation with cosine terms only\nuse of trigonometric addition formulas:\n\n\n\\[\nx_t = a_0 + \\sum(R_p \\cdot \\cos(2 \\pi p t / N + \\Phi_p)\n\\]\nwith:\n\namplitude: \\(R_p = \\sqrt{a_p^2 + b_p^2}\\)\nphase shift: \\(\\Phi_p = \\arctan(-b_p/a_p)\\)\n\n see: https://en.wikipedia.org/wiki/List_of_trigonometric_identities",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#different-methods-to-identify-seasonal-parameters",
    "href": "slides/10-timeseries-outlook.html#different-methods-to-identify-seasonal-parameters",
    "title": "10-Time Series Outlook",
    "section": "Different methods to identify seasonal parameters",
    "text": "Different methods to identify seasonal parameters\n\nlinear regression with the lm function, e.g. for a single periodic component: m &lt;- lm(x ~ sin(2 * pi * t / N) + cos(2 * pi * t / N))\nnonlinear regression with nls (allows to identify the period as nonlinear parameter)\nclassical Fourier analysis:\n\n\\[\\begin{align}\n     a_0 &= \\bar{x}\\\\\na_{N/2} &= \\sum_{t=1}^{N}(-1)^tx_t/N\\\\\na_p     &= 2 \\frac{\\sum_{t=1}^{N} x_t \\cos(2 \\pi p t/N)}{N}\\\\\nb_p     &= 2 \\frac{\\sum_{t=1}^{N} x_t \\sin(2 \\pi p t/N)}{N}\n\\end{align}\\]\n\nFast Fourer transform (FFT) using complex numbers used in signal processing (CD and MP3 players, mobile phones, …",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#fast-fourier-transform",
    "href": "slides/10-timeseries-outlook.html#fast-fourier-transform",
    "title": "10-Time Series Outlook",
    "section": "Fast Fourier transform",
    "text": "Fast Fourier transform\n\n\nShow the code\n# create an arbitrary time series\nset.seed(123)\nn   &lt;- 360\nt   &lt;- 0:(n-1)\nx   &lt;- sin(t*2*pi/n) + rnorm(n, sd = 0.2)\nplot(t, x, xlim = c(-10, 370))\n  \n# perform the FFT\np       &lt;- fft(x)\n\n# invert the FFT for the main period only (keep parameters 1..3)\np[4:n]  &lt;- 0 # set higher order parameters to zero\nx_sim      &lt;- fft(p, inverse = TRUE)\nlines(t, 2 * Re(x_sim)/n - Re(p[1])/n, col = \"blue\", lwd = 4)\n\n\n## perform FFT and invert it for all periods\np       &lt;- fft(x)\np[(n/2):n] &lt;- 0 # set only the \"redundant frequencies\" to zero\nx_sim         &lt;- fft(p, inverse = TRUE)\nlines(t, 2 * Re(x_sim)/n - Re(p[1])/n, col = \"red\", lwd = 1)",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#automatic-time-series-decomposition",
    "href": "slides/10-timeseries-outlook.html#automatic-time-series-decomposition",
    "title": "10-Time Series Outlook",
    "section": "Automatic time series decomposition",
    "text": "Automatic time series decomposition\nFunction decompose implements the classical approach with symmetric moving average.\n\nmanaus_dec &lt;- decompose(manaus)\nplot(manaus_dec)\n\n\n\nIn this data set the seasonal component possesses an additive character.\nIn case of a multiplicative components, use type = \"multiplicative\".",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#automatic-time-series-decomposition-ii",
    "href": "slides/10-timeseries-outlook.html#automatic-time-series-decomposition-ii",
    "title": "10-Time Series Outlook",
    "section": "Automatic time series decomposition II",
    "text": "Automatic time series decomposition II\n\nfunction stl (seasonal time series decomposition) (R. B. Cleveland et al., 1990)\nuses a LOESS filter\n\n\nmanaus_stl &lt;- stl(manaus, s.window=13)\nplot(manaus_stl)",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#example-sarima-model-for-co2-in-the-atmosphere",
    "href": "slides/10-timeseries-outlook.html#example-sarima-model-for-co2-in-the-atmosphere",
    "title": "10-Time Series Outlook",
    "section": "Example: SARIMA model for CO2 in the atmosphere",
    "text": "Example: SARIMA model for CO2 in the atmosphere\n\n\nShow the code\n## read data directly from NOAA\n#co2 &lt;- read_csv(\"https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv\", \n#                skip = 40, show_col_types = FALSE)\n\n## local version\nco2 &lt;- read_csv(\"../data/co2_mm_mlo.csv\", skip=40, show_col_types = FALSE)\n\n## year 1958 is incomplete, so let's start with 1959\nco2 &lt;- dplyr::filter(co2, year &gt; 1958) \n\nco2 |&gt; \n  mutate(date=as_date(paste(year, month, 15, sep=\".\"))) |&gt;\n  ggplot(aes(date, average)) + \n  geom_line() + ylab(expression(CO[2]~\"in the atmosphere (ppm)\")) + \n  ggtitle(\"Mauna Loa Observatory, Hawaii, Monthly Averages\") +\n  geom_smooth()",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#fit-sarima-model-to-co2-data",
    "href": "slides/10-timeseries-outlook.html#fit-sarima-model-to-co2-data",
    "title": "10-Time Series Outlook",
    "section": "Fit SARIMA model to CO2 data",
    "text": "Fit SARIMA model to CO2 data\n\n\nShow the code\nlibrary(astsa)\n\n## convert to a time series object\nco2ts &lt;- ts(co2$average, start=1959, frequency = 12)\n\n## fit a SARIMA model and show the results\nm &lt;- sarima(co2ts, p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)\n\n\n\nMathematical details can be found in Shumway & Stoffer (2019).",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#sarima-forecast-of-co2",
    "href": "slides/10-timeseries-outlook.html#sarima-forecast-of-co2",
    "title": "10-Time Series Outlook",
    "section": "SARIMA-forecast of CO2",
    "text": "SARIMA-forecast of CO2\n\n\nShow the code\nm &lt;- sarima.for(co2ts, n.ahead = 12 * 27, \n           p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12, \n           plot.all = TRUE)\n\n\n\nMathematical details can be found in Shumway & Stoffer (2019).",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#identification-of-structural-breaks-1",
    "href": "slides/10-timeseries-outlook.html#identification-of-structural-breaks-1",
    "title": "10-Time Series Outlook",
    "section": "Identification of structural breaks",
    "text": "Identification of structural breaks\n\nStructural break\nIf one or more statistical parameters are not constant over the whole length of a time series it is called a structural break. For instance a location parameter (e.g. the mean), a trend or another distribution parameter (such as variance or covariance) may change.\n\nTesting for structural breaks\nThe package strucchange implements a number of tests for identification of structural changes or parameter instability of time series. Generally, two approaches are available: fluctuation tests and F-based tests. Fluctuation tests try to detect the structural instability with cumulative or moving sums (CUSUMs and MOSUMs).",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#the-nile-data-set",
    "href": "slides/10-timeseries-outlook.html#the-nile-data-set",
    "title": "10-Time Series Outlook",
    "section": "The Nile data set",
    "text": "The Nile data set\nThe Nile data set contains measurements of annual discharge of the Nile at Aswan from 1871 to 1970 (see help page ?Nile for data source):\n\nlibrary(strucchange)\ndata(\"Nile\")\nplot(Nile)",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#test-for-existence-of-structural-break",
    "href": "slides/10-timeseries-outlook.html#test-for-existence-of-structural-break",
    "title": "10-Time Series Outlook",
    "section": "Test for existence of structural break",
    "text": "Test for existence of structural break\n\nAre there are periods with different flow?\nuse of OLS-CUSUM (ordinary least squares, cumulative sum) or MOSUM tests (moving average sum)\nefp = empirical fluctuation process,sctest = structural change test\n\n\nocus &lt;- efp(Nile ~ 1, type = \"OLS-CUSUM\")\nplot(ocus)\n\nsctest(ocus)\n\n\n    OLS-based CUSUM test\n\ndata:  ocus\nS0 = 2.9518, p-value = 5.409e-08",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#breakpoint-analysis",
    "href": "slides/10-timeseries-outlook.html#breakpoint-analysis",
    "title": "10-Time Series Outlook",
    "section": "Breakpoint analysis",
    "text": "Breakpoint analysis\n\nPurpose\n\nfind structural breaks with respect to a specified linear model\nidentify their number and location\nvery general approach, allows different linear models, covariates, …\nwe show the most simplest case here\n\nMethod\n\nemploys a BIC-based model selection technique\nWhich model is the best?\n\nSee also\n\nBai & Perron (2003), Zeileis et al. (2002), Zeileis et al. (2003).",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#remember-model-selection-techniques",
    "href": "slides/10-timeseries-outlook.html#remember-model-selection-techniques",
    "title": "10-Time Series Outlook",
    "section": "Remember: model selection techniques",
    "text": "Remember: model selection techniques\n\n\nmodel selection is a modern alternative to \\(p\\)-value based testing\nBased on the principle of parsimony:\n\nModels with more parameters fit better, but may contain unnecessary factors.\n\\(\\rightarrow\\) Find an optimal compromise between model fit and complexity.\nKeep it as simple as possible, but not simpler.\n\nMultiple model candidates for the same phenomenon:\n\nFull model: includes all potential factors.\nSeveral reduced models, derived from the full model.\nNull model without explanatory factors.\n\nSelect minimal adequate model (= “optimal”, “most parsimonious”).\n\nMore, see Johnson & Omland (2004)",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#model-selection-with-aic-and-bic",
    "href": "slides/10-timeseries-outlook.html#model-selection-with-aic-and-bic",
    "title": "10-Time Series Outlook",
    "section": "Model selection with AIC and BIC",
    "text": "Model selection with AIC and BIC\n\nModels with more parameters fit better \\(\\leftrightarrow\\) but more parameters (\\(k\\)) are “more complex”.\n\nGoodness of fit can be measured with likelihood \\(L\\) (how likely are the data given a specific model).\nLog likelihood: makes the problem additive.\nPenalty: penalises number of parameters (e.g. \\(2 k\\))\nAIC (Akaike Information Criterion):\n\n\\[\nAIC = - 2 \\ln(L) + 2 k\n\\]\n\nAlternatively: BIC (Bayesian Information Criterion), considers sample size (\\(n\\)):\n\n\\[\nBIC = - 2 \\ln(L) + k \\cdot \\ln(n)\n\\]\nThe model with smallest AIC (resp. BIC) is considered as “optimal” model.",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#breakpoint-analysis-1",
    "href": "slides/10-timeseries-outlook.html#breakpoint-analysis-1",
    "title": "10-Time Series Outlook",
    "section": "Breakpoint analysis",
    "text": "Breakpoint analysis\n\n\nbp.nile &lt;- breakpoints(Nile ~ 1)\nsummary(bp.nile)\n\n\n     Optimal (m+1)-segment partition: \n\nCall:\nbreakpoints.formula(formula = Nile ~ 1)\n\nBreakpoints at observation number:\n                      \nm = 1      28         \nm = 2      28       83\nm = 3      28    68 83\nm = 4      28 45 68 83\nm = 5   15 30 45 68 83\n\nCorresponding to breakdates:\n                                \nm = 1        1898               \nm = 2        1898           1953\nm = 3        1898      1938 1953\nm = 4        1898 1915 1938 1953\nm = 5   1885 1900 1915 1938 1953\n\nFit:\n                                                   \nm   0       1       2       3       4       5      \nRSS 2835157 1597457 1552924 1538097 1507888 1659994\nBIC    1318    1270    1276    1285    1292    1311\n\n\nNotes\n\ncomputationally intensive\nconsider to adapt model and parameter h, see help page",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#likelihood-profile-rightarrow-optimal-number-of-breaks",
    "href": "slides/10-timeseries-outlook.html#likelihood-profile-rightarrow-optimal-number-of-breaks",
    "title": "10-Time Series Outlook",
    "section": "Likelihood profile: \\(\\rightarrow\\) optimal number of breaks?",
    "text": "Likelihood profile: \\(\\rightarrow\\) optimal number of breaks?\n\nbp.nile &lt;- breakpoints(Nile ~ 1)\nplot(bp.nile)\n\n\n\nRSS (residual sum of squares) shows goodness of fit. The smaller the better.\nPenalty = 2 \\(\\cdot\\) number of breakpoints (not shown).\nMinimum BIC indicates optimal model.",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#plot-breakpoints-and-confidence-interval",
    "href": "slides/10-timeseries-outlook.html#plot-breakpoints-and-confidence-interval",
    "title": "10-Time Series Outlook",
    "section": "Plot breakpoints and confidence interval",
    "text": "Plot breakpoints and confidence interval\n\n## fit null hypothesis model and model with 1 breakpoint\nfm0 &lt;- lm(Nile ~ 1)\nfm1 &lt;- lm(Nile ~ breakfactor(bp.nile,  breaks = 1))\nplot(Nile)\nlines(ts(fitted(fm0),  start = 1871),  col = 3)\nlines(ts(fitted(fm1),  start = 1871),  col = 4)\nlines(bp.nile)\n\n## confidence interval\nci.nile &lt;- confint(bp.nile)\n#ci.nile  # output numerical results\nlines(ci.nile)",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#test-significance-of-breakpoints",
    "href": "slides/10-timeseries-outlook.html#test-significance-of-breakpoints",
    "title": "10-Time Series Outlook",
    "section": "Test significance of breakpoints",
    "text": "Test significance of breakpoints\n\nLikelihood ratio test\n\nanova(fm0, fm1)\n\nAnalysis of Variance Table\n\nModel 1: Nile ~ 1\nModel 2: Nile ~ breakfactor(bp.nile, breaks = 1)\n  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    \n1     99 2835157                                 \n2     98 1597457  1   1237700 75.93 7.439e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBIC based comparison\n\nBIC(fm0, fm1)\n\n    df      BIC\nfm0  2 1318.242\nfm1  3 1265.479\n\n\nAIC based comparison\n\nAIC(fm0, fm1)\n\n    df      AIC\nfm0  2 1313.031\nfm1  3 1257.663\n\n\n\\(\\rightarrow\\) model with structural break in 1898 is significantly better than the null model.",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#diagnostics",
    "href": "slides/10-timeseries-outlook.html#diagnostics",
    "title": "10-Time Series Outlook",
    "section": "Diagnostics",
    "text": "Diagnostics\n\n\npar(mfrow = c(2, 2))\nacf(residuals(fm0))     # model without breaks\nacf(residuals(fm1))     # model with 1 breakpoint\nqqnorm(residuals(fm0))\nqqnorm(residuals(fm1))",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/10-timeseries-outlook.html#references",
    "href": "slides/10-timeseries-outlook.html#references",
    "title": "10-Time Series Outlook",
    "section": "References",
    "text": "References\n\n\n\n\n\nBai, J., & Perron, P. (2003). Computation and analysis of multiple structural change models. J. Appl. Econ., 18:, 1–22.\n\n\nCanty, A., & Ripley, B. D. (2022). Boot: Bootstrap r (s-plus) functions.\n\n\nCleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I. (1990). STL: A seasonal-trend decomposition procedure based on loess. Journal of Official Statistics, 6, 3–73.\n\n\nCleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. Journal of the American Statistical Association, 74(368), 829–836. https://doi.org/10.1080/01621459.1979.10481038\n\n\nJohnson, G., Jerald, & Omland, K. S. (2004). Model Selection in Ecology and Evolution. Trends in Ecology and Evolution, 19(2), 101–108. https://doi.org/10.1016/j.tree.2003.10.013\n\n\nKleiber, C., & Zeileis, A. (2008). Applied econometrics with R. Springer.\n\n\nShumway, R. H., & Stoffer, D. S. (2019). Time series: A data analysis approach using r. CRC Press.\n\n\nZeileis, A., Kleiber, C., Krämer, W., & Hornik, K. (2003). Testing and dating of structural changes in practice. Computational Statistics and Data Analysis, 44(1–2), 109–123.\n\n\nZeileis, A., Leisch, F., Hornik, K., & Kleiber, C. (2002). Strucchange: An r package for testing for structural change in linear regression models. Journal of Statistical Software, 7(2), 1–38. http://www.jstatsoft.org/v07/i02/",
    "crumbs": [
      "Basic Statistics",
      "10-Time Series Outlook"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#data-sets-and-terms-of-use",
    "href": "slides/12-multivariate-2.html#data-sets-and-terms-of-use",
    "title": "12-Multivariate methods II",
    "section": "Data sets and terms of use",
    "text": "Data sets and terms of use\n\n\nThe “UBA-lakes” data set originates from the public data repository of the German Umweltbundesamt (Umweltbundesamt, 2021). The data set provided can be used freely according to the terms and conditions published at the UBA web site, that refer to § 12a EGovG with respect of the data, and to the Creative Commons CC-BY ND International License 4.0 with respect to other objects directly created by UBA.\nThe “gauernitz” data set contains simplified teaching versions from research data, of the study from Winkelmann et al. (2011)\nThe document itself, the codes and the ebedded images are own work and can be shared according to CC BY 4.0.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#a-taxonomic-table",
    "href": "slides/12-multivariate-2.html#a-taxonomic-table",
    "title": "12-Multivariate methods II",
    "section": "A Taxonomic Table",
    "text": "A Taxonomic Table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSite\nMollusca\nDiptera\nBaetis\nPlecoptera\nColeoptera\nTurbellaria\nHeptageniidae\nEphemeroptera\nGammarus\nTrichoptera\nAcari\nNematoda\nOligochaeta\n\n\n\n\nGP9\n3\n165\n91\n14\n6\n3\n9\n136\n256\n45\n6\n0\n11\n\n\nGR9\n31\n438\n728\n31\n728\n11\n31\n0\n65\n367\n3\n0\n503\n\n\nTP9\n0\n26\n3\n20\n9\n0\n3\n20\n119\n40\n0\n0\n23\n\n\nTR9\n0\n11\n6\n37\n11\n0\n0\n3\n68\n26\n0\n0\n23\n\n\nGP8\n23\n913\n31\n14\n3\n9\n6\n26\n901\n37\n3\n20\n0\n\n\nGR8\n225\n1066\n310\n199\n461\n48\n91\n23\n688\n600\n26\n0\n284\n\n\nTP8\n17\n2204\n54\n117\n11\n0\n11\n20\n2525\n77\n3\n3\n68\n\n\nTR8\n3\n520\n74\n762\n125\n20\n65\n0\n668\n173\n3\n0\n267\n\n\nGP7\n26\n247\n68\n6\n3\n3\n3\n20\n813\n9\n6\n0\n0\n\n\nGR7\n117\n509\n290\n63\n191\n6\n26\n11\n682\n117\n60\n9\n131\n\n\nTP7\n26\n3477\n17\n28\n0\n0\n3\n37\n2693\n17\n0\n0\n0\n\n\nTR7\n48\n429\n57\n412\n97\n9\n63\n9\n808\n102\n26\n6\n57\n\n\nGP10\n3\n159\n14\n31\n3\n0\n48\n91\n100\n23\n0\n0\n17\n\n\nGR10\n0\n68\n191\n26\n51\n3\n253\n0\n80\n233\n3\n0\n11\n\n\nTP10\n0\n51\n0\n6\n9\n0\n0\n6\n71\n0\n0\n0\n31\n\n\nTR10\n0\n28\n6\n40\n14\n0\n0\n0\n40\n54\n0\n0\n0\n\n\n\n\n\n\n\nAggregated part of taxa list from two small streams.\nMayfly splitted in most dominant taxa (Baetis and Heptageniidae) and the remaining Ephemeroptera for simplicity of the teaching example.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#bar-chart",
    "href": "slides/12-multivariate-2.html#bar-chart",
    "title": "12-Multivariate methods II",
    "section": "Bar chart",
    "text": "Bar chart\n\n\n\nlibrary(\"ggplot2\")\nlibrary(\"tidyr\")\n\nbenthos &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/gauernitz.csv\")\n\nbenthos_long &lt;-\n  benthos |&gt;\n  pivot_longer(cols=5:17, names_to=\"Taxon\", values_to = \"Abundance\")\n\nggplot(benthos_long, aes(x=Site, y=Abundance, fill=Taxon)) + geom_col()",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#how-to-analyse-this-kind-of-data",
    "href": "slides/12-multivariate-2.html#how-to-analyse-this-kind-of-data",
    "title": "12-Multivariate methods II",
    "section": "How to analyse this kind of data?",
    "text": "How to analyse this kind of data?\n\nDifferent approaches\n\nDirect interpretation\n\nraw data, mean values, …\ntables and plots\n\nCalculation of biodiversity indices\n\ngeneral-purpose indices (Richness, Simpson, Shannon, Eveness)\ndomain-specific indices (in stream ecology: sapropbic index, Perlodes, EPT)\n\nMultivariate statistics\n\nordination methods (CCA, NMDS, dbRDA)\ncluster analysis",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#diversity-indices",
    "href": "slides/12-multivariate-2.html#diversity-indices",
    "title": "12-Multivariate methods II",
    "section": "Diversity indices",
    "text": "Diversity indices\n\n\n\nSimpson index\n\\[\nD = \\sum_{i=1}^S p_i^2\n\\]\n\n\\(p_i\\): relative abundance of species\nin most cases, Simpson index is given as \\(\\tilde{D} = 1 - D\\) (large values – high diversity)\nalso possible: inverse Simpson index: \\(D' = 1 / D\\)\n\n\nShannon index\n\\[\nH = -\\sum_{i=1}^S p_i \\log_b p_i\n\\]\n\nin most cases log base \\(b=e\\) (natural log), some prefer \\(b=2\\) (information theory)\n\nEveness \n\\[\n  E = \\frac{H}{\\log(S)}\n  \\]\n\n\\(S\\): number of species\n\n\n\n\nmore indices: species richness, species deficit, Fisher’s \\(\\alpha\\) …",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#diversity-indices-1",
    "href": "slides/12-multivariate-2.html#diversity-indices-1",
    "title": "12-Multivariate methods II",
    "section": "Diversity indices",
    "text": "Diversity indices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSite\nHabitat\nStream\nFlood\nshannon\nsimpson\ninvsimpson\neveness\nfisher_alpha\n\n\n\n\nGP9\np\ng\nn\n1.75\n0.78\n4.55\n0.68\n2.03\n\n\nGR9\nr\ng\nn\n1.79\n0.81\n5.23\n0.70\n1.44\n\n\nTP9\np\nt\nn\n1.70\n0.74\n3.87\n0.66\n1.80\n\n\nTR9\nr\nt\nn\n1.74\n0.78\n4.57\n0.68\n1.70\n\n\nGP8\np\ng\nv\n1.11\n0.58\n2.39\n0.43\n1.70\n\n\nGR8\nr\ng\nv\n2.08\n0.85\n6.57\n0.81\n1.52\n\n\nTP8\np\nt\nv\n1.04\n0.57\n2.32\n0.41\n1.47\n\n\nTR8\nr\nt\nv\n1.81\n0.80\n5.04\n0.71\n1.46\n\n\nGP7\np\ng\nv\n1.04\n0.50\n1.99\n0.40\n1.67\n\n\nGR7\nr\ng\nv\n1.97\n0.82\n5.45\n0.77\n1.83\n\n\nTP7\np\nt\nv\n0.80\n0.51\n2.05\n0.31\n0.90\n\n\nTR7\nr\nt\nv\n1.80\n0.77\n4.33\n0.70\n1.84\n\n\nGP10\np\ng\nn\n1.83\n0.80\n5.00\n0.71\n1.78\n\n\nGR10\nr\ng\nn\n1.79\n0.80\n4.99\n0.70\n1.57\n\n\nTP10\np\nt\nn\n1.42\n0.71\n3.46\n0.55\n1.20\n\n\nTR10\nr\nt\nn\n1.62\n0.78\n4.64\n0.63\n1.19\n\n\n\n\n\n\n\naggregated data but which of the indices tells what?\n\\(\\rightarrow\\) information loss compared to the original list",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#problem",
    "href": "slides/12-multivariate-2.html#problem",
    "title": "12-Multivariate methods II",
    "section": "Problem",
    "text": "Problem\n\nThe PCA does not work well for this kind of data\n\nWithout standardization: most frequent taxa dominate the analysis, rare species under-represented\nWith standardization: rare species will given too much influence, result dominates much on sampling error\nsquare root transformation does not help, log-transformation is not possible because of zeros\n\nWhy?\n\nthe distance measure, used in PCA is the so-called Euclidean distance\nit works not well for species lists\n\nApproach\n\\(\\rightarrow\\) methods that support other distance and dissimilarity measures",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#euclidean-distance",
    "href": "slides/12-multivariate-2.html#euclidean-distance",
    "title": "12-Multivariate methods II",
    "section": "Euclidean distance",
    "text": "Euclidean distance\n\n\n\nPCA works with Euclidean distance\nTheorem of Pythagoras\n\n\\[\na^2 + b^2 = c^2 \\quad \\Rightarrow\\quad c = \\sqrt{a^2 + b^2} = \\sqrt{\\Delta x^2 + \\Delta y^2}\n\\]\n\\(\\rightarrow\\) but: Euclidean distance is not always the best option.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#distance-and-dissimilarity",
    "href": "slides/12-multivariate-2.html#distance-and-dissimilarity",
    "title": "12-Multivariate methods II",
    "section": "Distance and dissimilarity",
    "text": "Distance and dissimilarity\n\nAxiomatic definition\nMeasure of distance \\(d\\) between multidimensional points \\(x_i\\) and \\(x_j\\):\n\n\\(d(x_i, x_j) \\ge 0\\), distances are similar or equal to zero\n\\(d(x_i, x_j)=d(x_j,x_i)\\), the distance from A to B is the same as from B to A,\n\\(d(x_i, x_i)=0\\), the distance from a given point to itself is zero\n\nA distance measure is termed metric, if:\n\n\\(d=0\\) applies in the case of equality only, and\nthe triangle inequality applies. The indirect route is longer than the direct route\n\nIf one or both of the additional conditions are violated, we speak about nonmetric measures and use the term dissimilarity instead of distance.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#similarity",
    "href": "slides/12-multivariate-2.html#similarity",
    "title": "12-Multivariate methods II",
    "section": "Similarity",
    "text": "Similarity\n\nA measure of similarity \\(s\\) can be defined in a similar way:\n\n\\(s(x_i,x_j) \\le s_{max}\\)\n\\(s(x_i,x_j)=s(x_j,x_i)\\)\n\\(s(x_i,x_i)=s_{max}\\)\n\nit is metric, if:\n\n\\(s_{max}\\) applies only in the case of equality and\nthe triangle inequality applies",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#conversion-between-dissimilarity-and-similarity",
    "href": "slides/12-multivariate-2.html#conversion-between-dissimilarity-and-similarity",
    "title": "12-Multivariate methods II",
    "section": "Conversion between dissimilarity and similarity",
    "text": "Conversion between dissimilarity and similarity\n\n\n\n\n\n\nsimilarity\ndissimilarity\n\n\n\n\n\\(s=1-d/d_{max}\\)\n\\(d=1-s/s_{max}\\)\n\n\n\\(s=\\exp(-d)\\)\n\\(d= - \\ln(s-s_{min})\\)\n\n\n\n\n\n\ndistance goes from \\(0\\) to \\(\\infty\\)\ndifferent transformations, as long as the \\(\\Rightarrow\\) transformation is monotonic\nin most cases similarity \\(s\\) is limited between \\((0, 1)\\) or between 0 and 100%.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#common-distance-and-dissimilarity-measures",
    "href": "slides/12-multivariate-2.html#common-distance-and-dissimilarity-measures",
    "title": "12-Multivariate methods II",
    "section": "Common distance and dissimilarity measures",
    "text": "Common distance and dissimilarity measures\n\n\n\nEuclidean distance: shortest connection between 2 points in space\nManhattan distance: around the corner, as in Manhattans grid-like streets\nChi-square distance: for comparison of frequencies\nMahalanobis distance: takes covariance into account\nBray-Curtis dissimilarity: comparison of species lists in ecology\nJaccard index: for binary (presence-absence) data\nGower dissimilarity: used for mixed-type variables",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#distance-and-dissimilarity-of-metric-variables",
    "href": "slides/12-multivariate-2.html#distance-and-dissimilarity-of-metric-variables",
    "title": "12-Multivariate methods II",
    "section": "Distance and dissimilarity of metric variables",
    "text": "Distance and dissimilarity of metric variables\n\nEuclidean distance:\n\\[\nd_{jk} = \\sqrt{\\sum (x_{ij}-x_{ik})^2}\n\\]\nManhattan distance: \\[\nd_{jk} = \\sum |x_{ij}-x_{ik}|\n\\]\nGower distance: \\[\nd_{jk} = \\frac{1}{M} \\sum\\frac{|x_{ij}-x_{ik}|}{\\max(x_i)-\\min(x_i)}\n\\]\nBray-Curtis dissimilarity: \\[\nd_{jk} = \\frac{\\sum{|x_{ij}-x_{ik}|}}{\\sum{(x_{ij}+x_{ik})}}\n\\]\n\nwith \\(x_{ij}, x_{ik}\\) abundance of species \\(i\\) at sites (\\(j, k\\)).",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#distance-and-dissimilarity-of-binary-variables",
    "href": "slides/12-multivariate-2.html#distance-and-dissimilarity-of-binary-variables",
    "title": "12-Multivariate methods II",
    "section": "Distance and dissimilarity of binary variables",
    "text": "Distance and dissimilarity of binary variables\n\n\nEuclidean: \\(\\sqrt{A+B-2J}\\)\nManhattan: \\(A+B-2J\\)\nGower: \\(\\frac{A+B-2J}{M}\\)\nBray-Curtis: \\(\\frac{A+B-2J}{A+B}\\)\nJaccard: \\(\\frac{2b}{1+b}\\) with \\(b\\) = Bray-Curtis dissimilarity\n\nwhere:\n\n\\(A, B\\) = numbers of species on compared sites\n\\(J\\) = (joint) is the number of species that occur on both compared sites\n\\(M\\) = number of columns (excluding missing values)\n\n\nApplications\nAdditional distance measures and application suggestions in the vegdist help page.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#which-distances-are-supported-by-different-methods",
    "href": "slides/12-multivariate-2.html#which-distances-are-supported-by-different-methods",
    "title": "12-Multivariate methods II",
    "section": "Which distances are supported by different methods?",
    "text": "Which distances are supported by different methods?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatrices\nDistance\nR function\n\n\n\n\nPCA\nPrincipal Components Analysis\none matrix\neuclidean\nprcomp, rda\n\n\nRDA\nRedundancy Analysis\ntwo matices\neuclidean\nrda\n\n\nCA\nCorrespondence Analysis\none matrix\nchi square\ncca\n\n\nCCA\nCanonical Correspondence Analysis\ntwo matrices\nchi square\ncca\n\n\nPCO/MDS\nPrincipal Correspondence Analysis\none matrix\nany\ncmdscale, …\n\n\ndbRDA\ndistance-based Redundancy Analysis\ntwo matrices\nany\ndbrda, capscale\n\n\nPCoA\nPrincipal Coordinate Analysis\ntwo matrices\nany\nwcmdscale\n\n\nNMDS\nNon-metric Multidimensional Scaling\none matrix\nany\nmetaMDS\n\n\n…\n…\n…\n…\n…\n\n\n\nCluster analysis\none matrix\nany\nseveral packages\n\n\n\n\n\n\n\n\n\n\n\n\nmany different methods, not all are shown\none matrix methods: all variables depend on each other; optional matrix of explanation variables can be projected afterwards\ntwo matrix methods (= constrained methods): additional matrix of explanation variables, both matrices handled simultanaeously\nan additional third matrix is supported by so-called partial methods, e.g. pRDA, pCCA, pdbRDA",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#principal-coordinate-analysis-pco",
    "href": "slides/12-multivariate-2.html#principal-coordinate-analysis-pco",
    "title": "12-Multivariate methods II",
    "section": "Principal coordinate analysis (PCO)",
    "text": "Principal coordinate analysis (PCO)\n\n\n\nlibrary(\"vegan\")\nbenthos &lt;- read.csv(\"../data/gauernitz.csv\")\nrow.names(benthos) &lt;- benthos$Site\nspecies &lt;- benthos[-c(1:4)]\n\nd   &lt;- vegdist(species, method=\"bray\")\nord &lt;- cmdscale(d)\n\n\nplot(ord, type=\"n\", xlab=\"PCO 1\", ylab=\"PCO 2\")\ntext(ord, row.names(species))\n\n\\(\\rightarrow\\) distance matrix (d) used as input and not data matrix (species) directly\n\n\n\n\n\n\n\n\n\n\n\nworks with arbitrary distance measures, e.g. Bray-Curtis dissimilarity\nsupported by different packages, e.g. stats, vegan, labdsv, ecodist, ade4 and ape\nbasic version in package stats (base R), other packages with more specialized versions\nbasic version has no biplot, can be added separately",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#single-matrix-methods-i",
    "href": "slides/12-multivariate-2.html#single-matrix-methods-i",
    "title": "12-Multivariate methods II",
    "section": "Single matrix methods I",
    "text": "Single matrix methods I\n\nPCA: Principal Components analysis\n▶ input: raw data, covariance or correlation matrix\n(+) basic method, very easy to understand\n(+) biplot: common representation of objects and variables\n(–) only Euclidean distance, not suitable for taxa lists\n\nCA: Correspondence analysis\n▶ input: raw data (frequencies)\n(+) similar to PCA, but uses \\(\\chi^2\\) distance\n(+) better for taxa lists\n\nPCO: Principal Coordinates Analysis (metric MDS)\n▶ input: distance matrix\n(+) any distance measure can be used",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#single-matrix-methods-ii-nmds",
    "href": "slides/12-multivariate-2.html#single-matrix-methods-ii-nmds",
    "title": "12-Multivariate methods II",
    "section": "Single matrix methods II: NMDS",
    "text": "Single matrix methods II: NMDS\n\n\nnon-metric multidimensional scaling\nis an extension of the PCO (=MDS)\n\\(\\rightarrow\\) attempts to bring similarity structure better into 2 (or 3) dimensions\niterative procedure, minimize goodness of fit (called “stress”)\nseveral variants, mostly algorithm according to Kruskal\n\n▶ input: random configuration or PCO\n(+) popular, relatively easy to interpret\n(+/-) geometric distortion\n(–) results not always identical\n(–) computer intensive, especially for large data sets\n\nNote\n\nstress is sometimes given as ratio 0…1, sometimes in 0…100%\ndifferences between packages and statistics programs, e.g. SPSS",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#example",
    "href": "slides/12-multivariate-2.html#example",
    "title": "12-Multivariate methods II",
    "section": "Example",
    "text": "Example\n\nbenthos &lt;- read.csv(\"../data/gauernitz.csv\")\nrow.names(benthos) &lt;- benthos$Site\nenv &lt;- benthos[c(\"Habitat\", \"Stream\", \"Flood\")] # required later\n\nbio &lt;- benthos[c(\"Mollusca\", \"Diptera\", \"Baetis\", \"Plecoptera\", \"Coleoptera\", \n         \"Turbellaria\", \"Heptageniidae\", \"Ephemeroptera\", \"Gammarus\", \n         \"Trichoptera\", \"Acari\", \"Nematoda\", \"Oligochaeta\")]\n\nord &lt;- metaMDS(bio)\n\nSquare root transformation\nWisconsin double standardization\nRun 0 stress 0.1102424 \nRun 1 stress 0.1858757 \nRun 2 stress 0.1105543 \n... Procrustes: rmse 0.02427231  max resid 0.07210348 \nRun 3 stress 0.2491381 \nRun 4 stress 0.1105543 \n... Procrustes: rmse 0.02419506  max resid 0.07184901 \nRun 5 stress 0.1105543 \n... Procrustes: rmse 0.024259  max resid 0.07206191 \nRun 6 stress 0.1102424 \n... Procrustes: rmse 7.871569e-06  max resid 2.184699e-05 \n... Similar to previous best\nRun 7 stress 0.1727865 \nRun 8 stress 0.1105543 \n... Procrustes: rmse 0.02421122  max resid 0.07189791 \nRun 9 stress 0.1102424 \n... Procrustes: rmse 4.921984e-06  max resid 1.300433e-05 \n... Similar to previous best\nRun 10 stress 0.2540404 \nRun 11 stress 0.1855539 \nRun 12 stress 0.1105543 \n... Procrustes: rmse 0.02427195  max resid 0.07210317 \nRun 13 stress 0.1102424 \n... Procrustes: rmse 5.547713e-06  max resid 1.566575e-05 \n... Similar to previous best\nRun 14 stress 0.1102424 \n... Procrustes: rmse 1.137468e-05  max resid 3.180843e-05 \n... Similar to previous best\nRun 15 stress 0.1102424 \n... Procrustes: rmse 3.388606e-06  max resid 6.928114e-06 \n... Similar to previous best\nRun 16 stress 0.1102424 \n... Procrustes: rmse 1.977793e-05  max resid 5.454078e-05 \n... Similar to previous best\nRun 17 stress 0.2346072 \nRun 18 stress 0.1102424 \n... Procrustes: rmse 4.906897e-06  max resid 7.873003e-06 \n... Similar to previous best\nRun 19 stress 0.1105543 \n... Procrustes: rmse 0.02425855  max resid 0.07205888 \nRun 20 stress 0.1105543 \n... Procrustes: rmse 0.02421141  max resid 0.07189827 \n*** Best solution repeated 7 times",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#results-of-the-nmds",
    "href": "slides/12-multivariate-2.html#results-of-the-nmds",
    "title": "12-Multivariate methods II",
    "section": "Results of the NMDS",
    "text": "Results of the NMDS\n\nord\n\n\nCall:\nmetaMDS(comm = bio) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     wisconsin(sqrt(bio)) \nDistance: bray \n\nDimensions: 2 \nStress:     0.1102424 \nStress type 1, weak ties\nBest solution was repeated 7 times in 20 tries\nThe best solution was from try 0 (metric scaling or null solution)\nScaling: centring, PC rotation, halfchange scaling \nSpecies: expanded scores based on 'wisconsin(sqrt(bio))' \n\n\n\nmetaMDS runs a series of NMDS trials and outputs the best\nmakes automatic decisions about transformation, distance and scaling\n\nRecommendation\n\nspecify distance, scaling and transformation explicitly\nconsider to increase try and trymax for big and/or difficult data sets, e.g.:\n\n\nord &lt;- metaMDS(bio, distance=\"bray\", autotransform=FALSE, try=40)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#nmds-plot",
    "href": "slides/12-multivariate-2.html#nmds-plot",
    "title": "12-Multivariate methods II",
    "section": "NMDS Plot",
    "text": "NMDS Plot\n\nplot(ord, type=\"text\")\nabline(h=0, lty=\"dashed\", col=\"gray\")\nabline(v=0, lty=\"dashed\", col=\"gray\")\nmtext(paste(\"Stress=\", round(ord$stress,2)), side=3)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#stress",
    "href": "slides/12-multivariate-2.html#stress",
    "title": "12-Multivariate methods II",
    "section": "Stress",
    "text": "Stress\n\nCompares similarity of the ordination with original dissimilarity in all dimensions.\n\n\n\n\\(\\theta(d_{ij})\\): observed dissimilarity\n\\(\\tilde{d}_{ij}\\): ordination dissimilarity\n\n\\[\nS = \\sqrt{\\frac{\\sum_{i \\ne j} (\\theta(d_{ij}) - \\tilde{d}_{ij})^2}{\\sum_{i \\ne j}  \\tilde{d}_{ij}^2}}\n\\]\n\n\n\n\nQuality of ordination\nStress\n\n\n\n\npoor\n&gt; 0.2\n\n\nsufficient\n&lt; 0.1\n\n\ngood\n&lt;0.05\n\n\nexcellent\n&lt;0.025\n\n\nperfect\n0.0",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#stressplot-shepherd-plot",
    "href": "slides/12-multivariate-2.html#stressplot-shepherd-plot",
    "title": "12-Multivariate methods II",
    "section": "Stressplot (Shepherd plot)",
    "text": "Stressplot (Shepherd plot)\n\nstressplot(ord)\nmtext(paste(\"Stress =\", round(ord$stress,2)), side=3)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#stressplot-a-good-and-a-poor-example",
    "href": "slides/12-multivariate-2.html#stressplot-a-good-and-a-poor-example",
    "title": "12-Multivariate methods II",
    "section": "Stressplot: a good and a poor example",
    "text": "Stressplot: a good and a poor example\n\n\nGood\n\n\n\n\n\n\n\n\n\n\nPoor\n\n\n\n\n\n\n\n\n\n\nPoints should be close to the red line.\nPattern of stairs not important (at least not for now)\nThe \\(R^2\\) values are always big, ignore or at least don’t overinterpret it.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#environmental-fitting",
    "href": "slides/12-multivariate-2.html#environmental-fitting",
    "title": "12-Multivariate methods II",
    "section": "Environmental fitting",
    "text": "Environmental fitting\n\nord &lt;- metaMDS(bio, trace = FALSE) # trace: show or suppress intermediate output\nef &lt;- envfit(ord, env, permu = 1000)\nplot(ord, type = \"t\")\nplot(ef, p.max = 0.1)\n\n\n\nPlots arrows if explanation variables are metric.\nShows only centroids for ordinal explanation variables.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#numerical-results-and-p-values",
    "href": "slides/12-multivariate-2.html#numerical-results-and-p-values",
    "title": "12-Multivariate methods II",
    "section": "Numerical results and p-values",
    "text": "Numerical results and p-values\n\nef\n\n\n***FACTORS:\n\nCentroids:\n           NMDS1   NMDS2\nHabitatp -0.2714  0.0874\nHabitatr  0.2714 -0.0874\nStreamg  -0.0062 -0.1955\nStreamt   0.0062  0.1955\nFloodn    0.1665  0.1540\nFloodv   -0.1665 -0.1540\n\nGoodness of fit:\n            r2   Pr(&gt;r)    \nHabitat 0.3837 0.000999 ***\nStream  0.1807 0.062937 .  \nFlood   0.2428 0.018981 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nPermutation: free\nNumber of permutations: 1000\n\n\n\n\np-values are based on a permutation test\nuseful, but the ADONIS test is better",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#another-example-data-set-from-the-vegan-package",
    "href": "slides/12-multivariate-2.html#another-example-data-set-from-the-vegan-package",
    "title": "12-Multivariate methods II",
    "section": "Another example data set from the vegan package",
    "text": "Another example data set from the vegan package\n\ndata(varechem); data(varespec)\nmds &lt;- metaMDS(varespec, trace=FALSE)\nef &lt;- envfit(mds, varechem, permu=1000)\nplot(mds, type=\"t\")\nplot(ef, p.max =0.1)\n\n\nData from Väre et al. (1995)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#the-data-set",
    "href": "slides/12-multivariate-2.html#the-data-set",
    "title": "12-Multivariate methods II",
    "section": "The data set",
    "text": "The data set\n\nvarespec: 24 observations of 44 variables (plant species)\n\n\n [1] \"Callvulg\" \"Empenigr\" \"Rhodtome\" \"Vaccmyrt\" \"Vaccviti\" \"Pinusylv\"\n [7] \"Descflex\" \"Betupube\" \"Vacculig\" \"Diphcomp\" \"Dicrsp\"   \"Dicrfusc\"\n[13] \"Dicrpoly\" \"Hylosple\" \"Pleuschr\" \"Polypili\" \"Polyjuni\" \"Polycomm\"\n[19] \"Pohlnuta\" \"Ptilcili\" \"Barbhatc\" \"Cladarbu\" \"Cladrang\" \"Cladstel\"\n[25] \"Cladunci\" \"Cladcocc\" \"Cladcorn\" \"Cladgrac\" \"Cladfimb\" \"Cladcris\"\n[31] \"Cladchlo\" \"Cladbotr\" \"Cladamau\" \"Cladsp\"   \"Cetreric\" \"Cetrisla\"\n[37] \"Flavniva\" \"Nepharct\" \"Stersp\"   \"Peltapht\" \"Icmaeric\" \"Cladcerv\"\n[43] \"Claddefo\" \"Cladphyl\"\n\n\n\nvarechem: 24 observations of 16 variables\n\n\n [1] \"N\"        \"P\"        \"K\"        \"Ca\"       \"Mg\"       \"S\"       \n [7] \"Al\"       \"Fe\"       \"Mn\"       \"Zn\"       \"Mo\"       \"Baresoil\"\n[13] \"Humdepth\" \"pH\"      \n\n\n\nData from Väre et al. (1995) about influence of reindeer grazin gon understorey vegetation in Pinus sylvestris forests in eastern Fennoscandia.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#surface-fitting",
    "href": "slides/12-multivariate-2.html#surface-fitting",
    "title": "12-Multivariate methods II",
    "section": "Surface fitting",
    "text": "Surface fitting\n\n\nef &lt;- envfit(mds ~ Al + Ca, varechem, permu = 1000)\nplot(mds, display = \"sites\", type = \"text\")\nplot(ef)\nwith(varechem, ordisurf(mds, Al, add = TRUE))\nwith(varechem, ordisurf(mds, Ca, add = TRUE, col = \"green4\"))",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#pros-and-cons-of-the-methods-discussed-so-far",
    "href": "slides/12-multivariate-2.html#pros-and-cons-of-the-methods-discussed-so-far",
    "title": "12-Multivariate methods II",
    "section": "Pros and cons of the methods discussed so far",
    "text": "Pros and cons of the methods discussed so far\n\nPCA (and also CCA)\n(+) easy to understand, quick and reproducible\n(+) no non-linear distortion\n(–) but: horseshoe effect possible\n(–) information is often still in a “higher dimension”\n(–) Euclidean distance poorly suited for species lists\nNMDS\n(+) any distance measure can be used\n(+) better mapping on low dimensions\n(–) bias\n(–) numerical effort, iterative method, local minima\n(–) one-matrix method (no separate matrices for species and environmental factors)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#problems-of-ca-and-pca",
    "href": "slides/12-multivariate-2.html#problems-of-ca-and-pca",
    "title": "12-Multivariate methods II",
    "section": "Problems of CA (and PCA)",
    "text": "Problems of CA (and PCA)\n\n\narc (CA) or horseshoe effect (PCA)\n\n\n\n\n\n\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n0\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkaround\n\ndetrended correspondence analysis (DCA) used in the past, not anymore recommended (except you know what you do)\nbetter: NMDS or a “constrained” (2-matrix) method, e.g. CCA, RDA, dbRDA)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#cca-canonical-correspondence-analysis",
    "href": "slides/12-multivariate-2.html#cca-canonical-correspondence-analysis",
    "title": "12-Multivariate methods II",
    "section": "CCA: Canonical Correspondence Analysis",
    "text": "CCA: Canonical Correspondence Analysis\nExample Gauernitzbach-data\n\nord &lt;- cca(bio ~ ., data = env) # ~ . is an abbreviation for all variables in env\nord\n\n\nCall: cca(formula = bio ~ Habitat + Stream + Flood, data = env)\n\n              Inertia Proportion Rank\nTotal          0.8599     1.0000     \nConstrained    0.5210     0.6059    3\nUnconstrained  0.3389     0.3941   12\n\nInertia is scaled Chi-square\n\nEigenvalues for constrained axes:\n  CCA1   CCA2   CCA3 \n0.3821 0.1090 0.0299 \n\nEigenvalues for unconstrained axes:\n    CA1     CA2     CA3     CA4     CA5     CA6     CA7     CA8     CA9    CA10 \n0.13511 0.09923 0.03695 0.02930 0.01771 0.01124 0.00504 0.00213 0.00157 0.00050 \n   CA11    CA12 \n0.00011 0.00000 \n\n\n\ninertia measures error and information (similar to variance)\n\nallows separation of variability into information and error\nin case of CCA it is \\(\\chi^2\\) distance, in case of RDA it is variance\n\nin the example\n\n61% is explained by the constrained axes Habitat, Stream and Flood\n39% is not explained by the provided environmental variables",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#triplot",
    "href": "slides/12-multivariate-2.html#triplot",
    "title": "12-Multivariate methods II",
    "section": "Triplot",
    "text": "Triplot\n\nplot(ord)\n\n\nImportant\n\nThe plot shows only the part of variation that is explained by the constraints.\nIt the number of constraints is high compared to the number of observations, the ordination shows again the full variation, i.e. becomes unconstrained.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#interpretation-of-the-cca",
    "href": "slides/12-multivariate-2.html#interpretation-of-the-cca",
    "title": "12-Multivariate methods II",
    "section": "Interpretation of the CCA",
    "text": "Interpretation of the CCA\n\n\ntriplot with observations, species and environmental factors, note different scaling!\n\ndistance from the origin: \\(\\chi^2\\)\nspecies in the middle: either “average species” or poorly explained species\nspecies at the very edge: attention, often rare species\n\northogonal angle of species on connecting line origin - centroid of environmental factor",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#statistical-significance-anova-like-permutation-test",
    "href": "slides/12-multivariate-2.html#statistical-significance-anova-like-permutation-test",
    "title": "12-Multivariate methods II",
    "section": "Statistical significance: ANOVA like permutation test",
    "text": "Statistical significance: ANOVA like permutation test\n\n\nanova(ord, by = \"terms\")\n\nPermutation test for cca under reduced model\nTerms added sequentially (first to last)\nPermutation: free\nNumber of permutations: 999\n\nModel: cca(formula = bio ~ Habitat + Stream + Flood, data = env)\n         Df ChiSquare       F Pr(&gt;F)    \nHabitat   1   0.31255 11.0671  0.001 ***\nStream    1   0.10766  3.8123  0.078 .  \nFlood     1   0.10081  3.5696  0.071 .  \nResidual 12   0.33889                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nanova(ord, by=\"axis\") tests significance of the CCA axes and anova(ord, by=\"margin\") the marginal effects of the terms.\ncan also be called via permutest",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#adonis-test",
    "href": "slides/12-multivariate-2.html#adonis-test",
    "title": "12-Multivariate methods II",
    "section": "ADONIS test",
    "text": "ADONIS test\n\nadonis2(bio ~ Habitat * Stream * Flood, data = env, method = \"bray\")\n\nPermutation test for adonis under reduced model\nPermutation: free\nNumber of permutations: 999\n\nadonis2(formula = bio ~ Habitat * Stream * Flood, data = env, method = \"bray\")\n         Df SumOfSqs      R2      F Pr(&gt;F)    \nModel     7   3.2227 0.87112 7.7251  0.001 ***\nResidual  8   0.4768 0.12888                  \nTotal    15   3.6994 1.00000                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nAnalysis of variance using distance matrices, uses a permutation test with pseudo-F ratios.\nNot directly related to CCA, RDA etc.\nCan use all dissimilarity measures from the vegdistfunction.\nMore powerful that the permutest-ANOVA, as it can handle interaction effects.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#rda-and-dbrda",
    "href": "slides/12-multivariate-2.html#rda-and-dbrda",
    "title": "12-Multivariate methods II",
    "section": "RDA and dbRDA",
    "text": "RDA and dbRDA\nRDA: redundancy analysis\n\nis the two-matrix extension of the PCA\nuses Euclidean distance for the dependent variables\nvery useful, if the dependent matrix (“bio”) contains physical and chemical variables, e.g. temperature, nutrients, or aggregated biological data like total biomass or chlorophyll and not abundances of different species\n\ndbRDA distance-based RDA / constrained PCoA (Principal Coordinates Analysis)\n\nextends RDA to use arbitrary distance measures like Bray-Curtis for the dependent matrix (bio)\nsometimes more difficult to apply than CCA and RDA because of negative eigenvalues\nvery useful for taxa lists, more flexible than CCA\nworks in principle also with Euclidean distance, but is less efficient",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#example-dbrda-with-the-gauernitz-data",
    "href": "slides/12-multivariate-2.html#example-dbrda-with-the-gauernitz-data",
    "title": "12-Multivariate methods II",
    "section": "Example dbRDA with the Gauernitz data",
    "text": "Example dbRDA with the Gauernitz data\n\nord &lt;- dbrda(bio ~ ., data = env, method=\"bray\")\nord\nplot(ord)\nanova(ord, by=\"terms\")\n\n\n~ . is an abbreviation for all variables in env\nalso possible dbrda(bio ~ Stream + Flood + Habitat, data=env)\nsimilar interpretation like CCA\nRDA with Euclidean distance can, for example, be applied to the UBA-Lakes dada set",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#partial-analyses-prda-pcca-p-dbrda",
    "href": "slides/12-multivariate-2.html#partial-analyses-prda-pcca-p-dbrda",
    "title": "12-Multivariate methods II",
    "section": "Partial Analyses: pRDA, pCCA, p-dbRDA",
    "text": "Partial Analyses: pRDA, pCCA, p-dbRDA\n\nsplit constraints into covariates and\ncan be used to remove the effect of covariates (e.g. conditioning, background or random variables)\n\nExample\nWe know that pools and riffles are different and that the two streams differ somewhat, so we handle this as covariates\n\nord &lt;- cca(bio ~ Flood + Condition(Habitat, Stream), data = env)\nord\n\n\nCall: cca(formula = bio ~ Flood + Condition(Habitat, Stream), data = env)\n\n              Inertia Proportion Rank\nTotal          0.8599     1.0000     \nConditional    0.3125     0.3635    1\nConstrained    0.1429     0.1662    1\nUnconstrained  0.4045     0.4704   12\n\nInertia is scaled Chi-square\n\nEigenvalues for constrained axes:\n  CCA1 \n0.1429 \n\nEigenvalues for unconstrained axes:\n    CA1     CA2     CA3     CA4     CA5     CA6     CA7     CA8     CA9    CA10 \n0.14749 0.10947 0.07021 0.03126 0.01823 0.01502 0.00512 0.00476 0.00207 0.00064 \n   CA11    CA12 \n0.00014 0.00007 \n\n\nSo the inertia is splitted in three components, Conditional (the covariates), Constrained (flood) and Unconstrained.\nThe plot shows then the effect of the flood more clearly.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#which-ordination-method-to-start-with",
    "href": "slides/12-multivariate-2.html#which-ordination-method-to-start-with",
    "title": "12-Multivariate methods II",
    "section": "Which ordination method to start with?",
    "text": "Which ordination method to start with?\n\nMultivariate statistics is a very broad field. Experience shows that it can become quite complex and challenging, but also that it is relatively easy to start with it.\n\nMy personal recommendation\n\nStart with PCA if working with physical, chemical and hydromorphological data. It often also works well with aggregated biomass data.\nUse RDA if you have additional explanation variables (two-matrix method)\nStart with NMDS if working with abundance data of species (taxa lists)\nUse NMDS with envfit to explore influence of explanation variables on the ordination.\nuse CCA, dbRDA or PCoA to get more quantitative results, compared to NMDS.",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#overview",
    "href": "slides/12-multivariate-2.html#overview",
    "title": "12-Multivariate methods II",
    "section": "Overview",
    "text": "Overview\n\n\nCluster analysis aims to group data sets in clusters\nHierarchical clustering\n\nbuild a dendrogram (a tree of grouping)\nagglomerative methods\ndivisive methods\n\nDifferent agglomeration methods\n\ndefine how distance is measured between clusters\n\nNonhierarchical clustering\n\nsplit into a given number of groups\nusually no dendrogram\niterative methods\ne.g. k-means, k-centroids",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#the-uba-lake-data-set-again",
    "href": "slides/12-multivariate-2.html#the-uba-lake-data-set-again",
    "title": "12-Multivariate methods II",
    "section": "The UBA lake data set again",
    "text": "The UBA lake data set again\n\n\n\n\n\n\n\nz_mean\nz_max\nt_ret\nvolume\narea\np_tot\nn_no3\nchl\n\n\n\n\nAmmer\n37.60\n81.1\n2.70\n1.75000\n46.600\n7.3\n1.09\n2.80\n\n\nArend\n28.60\n48.7\n50.00\n0.14700\n5.140\n375.0\n0.05\n22.30\n\n\nBoden\n85.00\n254.0\n4.20\n48.52150\n571.500\n6.9\n0.84\n2.10\n\n\nChiem\n25.60\n73.4\n1.26\n2.04800\n79.900\n9.2\n0.55\n3.80\n\n\nDober\n5.40\n18.8\n2.30\n0.01690\n3.120\n63.9\n0.64\n27.30\n\n\nMuegg\n4.85\n7.5\n0.20\n0.03500\n7.200\n189.9\n0.17\n32.90\n\n\nPloen\n12.40\n58.0\n3.10\n0.37200\n29.970\n62.3\n0.22\n8.80\n\n\nKumme\n8.10\n23.3\n1.50\n0.26300\n32.500\n65.3\n0.78\n16.60\n\n\nMueritz\n6.50\n28.1\n6.00\n0.68000\n105.300\n19.7\n0.11\n6.30\n\n\nMuerB\n9.80\n30.3\n6.00\n0.03800\n3.910\n34.2\n0.11\n6.70\n\n\nPlaue\n6.80\n25.5\n3.00\n0.30000\n38.400\n26.0\n0.09\n6.80\n\n\nSacro\n18.01\n36.0\n15.00\n0.01930\n1.072\n79.8\n0.04\n8.60\n\n\nSchar\n9.00\n29.5\n16.00\n0.10823\n12.090\n35.3\n0.12\n10.40\n\n\nSchwA\n9.40\n52.4\n10.00\n0.33100\n35.200\n100.0\n0.23\n11.70\n\n\nSchwI\n13.50\n44.6\n5.30\n0.35600\n26.400\n246.5\n0.19\n5.86\n\n\nStarn\n53.20\n127.8\n21.00\n2.99900\n56.400\n5.9\n0.32\n1.84\n\n\nStech\n22.80\n68.0\n32.00\n0.09700\n4.250\n15.8\n0.04\n2.60\n\n\nStein\n1.35\n2.9\n2.30\n0.04200\n29.100\n53.3\n0.12\n29.00",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#cluster-analysis-1",
    "href": "slides/12-multivariate-2.html#cluster-analysis-1",
    "title": "12-Multivariate methods II",
    "section": "Cluster analysis",
    "text": "Cluster analysis\n\npar(mfrow=c(1,2))\nhc &lt;- hclust(dist(scale(lakedata2)), method=\"complete\") # the default\nplot(hc)\n\nhc2 &lt;- hclust(dist(scale(lakedata2)), method=\"ward.D2\")\nplot(hc2)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#identification-of-clusters-in-the-tree",
    "href": "slides/12-multivariate-2.html#identification-of-clusters-in-the-tree",
    "title": "12-Multivariate methods II",
    "section": "Identification of clusters in the tree",
    "text": "Identification of clusters in the tree\n\nplot(hc, hang = -1)    # -1: extend vertical lines to the bottom\nrect.hclust(hc, 5)\n\ngrp &lt;- cutree(hc, 5)\n# grp                  # can be used to show the groups",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#color-nmds-according-to-clusters",
    "href": "slides/12-multivariate-2.html#color-nmds-according-to-clusters",
    "title": "12-Multivariate methods II",
    "section": "Color NMDS according to clusters",
    "text": "Color NMDS according to clusters\n\nmd &lt;- metaMDS(lakedata2, scale = TRUE, distance = \"euclid\", trace=FALSE)\n\n\nplot(md, type = \"n\")\ntext(md$points, row.names(lakedata2), col = grp)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#non-hierarchical-clustering",
    "href": "slides/12-multivariate-2.html#non-hierarchical-clustering",
    "title": "12-Multivariate methods II",
    "section": "Non-hierarchical clustering",
    "text": "Non-hierarchical clustering\n\n\ne.g. k-means clustering\nan iterative method\navoids the problem that cluster assignment depends on the order of clustering and the agglomeration method\n\nDisadvantages, depending on the question\n\nnumber of clusters needs to be specified beforehand (e.g. from hierarchical clustering)\nno dendrogramm\n\n\ncl &lt;- kmeans(scale(log(lakedata)), centers = 5)\ncl$cluster\n\n  Ammer   Arend   Boden   Chiem   Dober   Muegg   Ploen   Kumme Mueritz   MuerB \n      5       1       5       5       3       2       4       3       4       1 \n  Plaue   Sacro   Schar   SchwA   SchwI   Starn   Stech   Stein \n      4       1       4       4       4       5       1       2",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#scatterplot-of-data-and-clusters",
    "href": "slides/12-multivariate-2.html#scatterplot-of-data-and-clusters",
    "title": "12-Multivariate methods II",
    "section": "Scatterplot of data and clusters",
    "text": "Scatterplot of data and clusters\n\nplot(log(lakedata), col = cl$cluster, pch=16)",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#further-reading",
    "href": "slides/12-multivariate-2.html#further-reading",
    "title": "12-Multivariate methods II",
    "section": "Further reading",
    "text": "Further reading\n\n\nOksanen (2015) Multivariate analysis of ecological communities in R: Vegan tutorial.\nBorcard, D., Gillet, F., & Legendre, P. (2018) Numerical ecology with R. Springer International Publishing. https://doi.org/10.1007/978-3-319-71404-2",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/12-multivariate-2.html#references",
    "href": "slides/12-multivariate-2.html#references",
    "title": "12-Multivariate methods II",
    "section": "References",
    "text": "References\n\n\n\n\n\nBorcard, D., Gillet, F., & Legendre, P. (2018). Numerical ecology with R. Springer International Publishing. https://doi.org/10.1007/978-3-319-71404-2\n\n\nOksanen, J. (2015). Multivariate analysis of ecological communities in R: Vegan tutorial.\n\n\nUmweltbundesamt. (2021). Kenndaten ausgewählter Seen Deutschlands. https://www.umweltbundesamt.de/daten/wasser/zustand-der-seen#okologischer-zustand-der-seen\n\n\nVäre, H., Ohtonen, R., & Oksanen, J. (1995). Effects of reindeer grazing on understorey vegetation in dry Pinus sylvestris forests. Journal of Vegetation Science, 6(4), 523–530. https://doi.org/10.2307/3236351\n\n\nWinkelmann, C., Hellmann, C., Worischka, S., Petzoldt, T., & Benndorf, J. (2011). Fish predation affects the structure of a benthic community. Freshwater Biology, 56(6), 1030–1046. https://doi.org/10.1111/j.1365-2427.2010.02543.x",
    "crumbs": [
      "Basic Statistics",
      "12-Multivariate methods II"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#table-of-contents",
    "href": "slides/x2-r-graphics.html#table-of-contents",
    "title": "x2-Graphics with R",
    "section": "Table of Contents",
    "text": "Table of Contents\n\n\nThe easy way\nCustomizing graphics\nMultiple grapics on one page\nSaving and exporting graphics\nLattice\ngrid and gridbase\nggplot2",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#the-easy-way",
    "href": "slides/x2-r-graphics.html#the-easy-way",
    "title": "x2-Graphics with R",
    "section": "The Easy Way",
    "text": "The Easy Way\n\n\nplot(iris)\n\n\n\nR contains many graphics functions with convenient defaults.\niris is a built-in data set in R (see next slide)\nplot is a so-called generic function that automatically decides how to plot.",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#the-iris-data-set",
    "href": "slides/x2-r-graphics.html#the-iris-data-set",
    "title": "x2-Graphics with R",
    "section": "The iris data set",
    "text": "The iris data set\n\nThe famous (Fisher’s or Anderson’s) iris data set contains measurements (in centimeter) of the variables sepal length, sepal width, petal length and petal width of 50 flowers from each of 3 species of iris, Iris setosa, I. versicolor, and I. virginica.\n\nsee ?iris in R’s online help.\nor: https://en.wikipedia.org/wiki/Iris_flower_data_set",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#plotting-colums-of-a-data-frame",
    "href": "slides/x2-r-graphics.html#plotting-colums-of-a-data-frame",
    "title": "x2-Graphics with R",
    "section": "Plotting colums of a data frame",
    "text": "Plotting colums of a data frame\n\n\nplot(iris$Sepal.Length, iris$Petal.Length)\n\n\n\n\n\n\n\n\n\n\nplot(iris$Sepal.Length, iris$Petal.Length,\n     col=iris$Species)\n\n\n\n\n\n\n\n\nA column of a data.frame is accessed with $.",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#the-use-of-with-saves-dollars",
    "href": "slides/x2-r-graphics.html#the-use-of-with-saves-dollars",
    "title": "x2-Graphics with R",
    "section": "The use of with() saves dollars",
    "text": "The use of with() saves dollars\n\n\n\nplot(iris$Sepal.Length, iris$Petal.Length, \n     col=iris$Species)\n\n\n\n\n\n\n\n\n\n\nwith(iris, plot(Sepal.Length, Petal.Length, \n                col=Species))",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#colors-and-plotting-symbols-in-r",
    "href": "slides/x2-r-graphics.html#colors-and-plotting-symbols-in-r",
    "title": "x2-Graphics with R",
    "section": "Colors and plotting symbols in R",
    "text": "Colors and plotting symbols in R\nR allows to change style and color of plotting symbols:\n\ncol: color, can be one of 8 default colors or a user-defined color\npch: plotting character, can be one of 25 symbols or a quoted letter\ncex: character extension: size of a plotting character\n\n\nplot(1:25, col=1:25, pch=1:15, cex=2)",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#special-plotting-symbols",
    "href": "slides/x2-r-graphics.html#special-plotting-symbols",
    "title": "x2-Graphics with R",
    "section": "Special plotting symbols",
    "text": "Special plotting symbols\n\nsymbols 21..25 have an optional background color\nlwd: border width of the symbol\n\n\nplot(21:25, col=\"darkred\", pch=21:25, cex=2, bg=\"green\", lwd=2)",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#r-as-function-plotter",
    "href": "slides/x2-r-graphics.html#r-as-function-plotter",
    "title": "x2-Graphics with R",
    "section": "R as function plotter",
    "text": "R as function plotter\n\nx &lt;- seq(0, 20, length.out=100)\ny1 &lt;- sin(x)\ny2 &lt;- cos(x)\nplot(x, y1, type=\"l\", col=\"red\")\nlines(x, y2, col=\"blue\")\n\n\n\ntype: “p”: points, “l”: lines, “b”: both, points and lines, “c”: empty points joined by lines, “o”: overplotted points and lines, “s” and “S”: stair steps, “h” histogram-like vertical lines, “n”: no points or lines.",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#line-styles",
    "href": "slides/x2-r-graphics.html#line-styles",
    "title": "x2-Graphics with R",
    "section": "Line styles",
    "text": "Line styles\n\nx &lt;- seq(0, 20, length.out=100)\nplot(x, sin(x), type=\"l\", col=\"red\", lwd=3, lty=\"dotted\")\nlines(x, cos(x), col=\"blue\", lwd=2, lty=\"dashed\")\n\n\n\nlty: line type (“blank”, “solid”, “dashed”, “dotted”, “dotdash”, “longdash”, “twodash”) or a number from 1…7, or a string with up to 8 numbers for drawing and skipping (e.g. “4224”).\nlwd: line width (a number, defaults to 1)",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#coordinate-axes-and-annotations",
    "href": "slides/x2-r-graphics.html#coordinate-axes-and-annotations",
    "title": "x2-Graphics with R",
    "section": "Coordinate axes and annotations",
    "text": "Coordinate axes and annotations\n\nplot(iris$Sepal.Length, iris$Petal.Length, xlim=c(0, 8), ylim=c(2,8),\n     col=iris$Species, pch=16,\n     xlab=\"Sepal Length (cm)\", ylab=\"Petal Length (cm)\", main=\"Iris Data\",\n     las = 1)\n\n\n\ncol=iris$Species: works because Species is a factor\nlas=1: numbers on y-axis upright (try: 0, 1, 2 or 3)\nlog: may be used to transform axes (e.g. log=“x”, log=“y”, log=“xy”)",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#adding-a-legend",
    "href": "slides/x2-r-graphics.html#adding-a-legend",
    "title": "x2-Graphics with R",
    "section": "Adding a legend",
    "text": "Adding a legend\n\nmycolors &lt;- c(\"blue\", \"red\", \"cyan\")\nplot(iris$Sepal.Length, iris$Petal.Length, xlim=c(0, 8), ylim=c(2,8),\n     col=mycolors[iris$Species], pch = 16,\n     xlab=\"Sepal Length (cm)\", ylab=\"Petal Length (cm)\", main=\"Iris Data\",\n     las = 1)\nlegend(\"topleft\", legend=c(\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"),\n  col=mycolors, pch=16)\n\n\n\nsee ?legend for more options (e.g. line styles, position of the legend)",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#global-parameters-font-size-margins",
    "href": "slides/x2-r-graphics.html#global-parameters-font-size-margins",
    "title": "x2-Graphics with R",
    "section": "Global parameters, font size, margins, …",
    "text": "Global parameters, font size, margins, …\n\n\nMany figure options can be specified globally with par()\npar(lwd=2) all lines have double width\npar(mfrow=c(2,2)) subdivides the graphics area in 2 x 2 fields\npar(las=1) numbers at y axis upright\npar(mar=c(5, 5, 0.5, 0.5)) changes figure margins (bottom, left, top, right)\npar(cex=2) increase font size\n\\(\\rightarrow\\) sometimes it is better to leave font size as is and change size of the figure instead\n\n\n\nRead the ?par help page!",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#example",
    "href": "slides/x2-r-graphics.html#example",
    "title": "x2-Graphics with R",
    "section": "Example",
    "text": "Example\n\n\n#\nplot(iris$Sepal.Length, iris$Petal.Length, \n     col=iris$Species)\n#\n\n\n\n\n\n\n\n\n\n\n\n\nopar &lt;- par(cex=2, mar=c(4,4,1,1), las=1)\nplot(iris$Sepal.Length, iris$Petal.Length, \n     col=iris$Species)\npar(opar)\n\n\n\n\n\n\n\n\n\n\n\nchange font size (cex), margins (mar) and axis label orientation (las)\nopar stores previuos parameter and allows resetting",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#saving-and-exporting-figures",
    "href": "slides/x2-r-graphics.html#saving-and-exporting-figures",
    "title": "x2-Graphics with R",
    "section": "Saving and exporting figures",
    "text": "Saving and exporting figures\n\nEasiest way ist to use the RStudio’s Export –&gt; Save as Image (or copy to clipboard)\nImportant: Select correct image format and image size!\n\n\n\n\n\n\n\n\n\n\nFormat\nType\nUsage\nNotes\n\n\n\n\nPNG\nbitmap\ngeneral purpose\nfixed size, use at least 300 pixels per inch\n\n\nJPEG\nbitmap\nphotographs\nnot good for R images\n\n\nTIFF\nbitmap\nPNG is easier\noutdated, required by some journals\n\n\nBMP\nbitmap\nnot recommended\noutdated, needs huge memory\n\n\nMetafile\nvector\nWindows standard format\neasy to use, quality varies\n\n\nSVG\nvector\ncan be edited\nallows editing with Inkscape\n\n\nEPS\nvector\nPDF is easier\nrequired by some journals\n\n\nPDF\nvector\nbest quality\nperfect for LaTex, RMarkdown and Quarto, MS Office requires conversion",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#vector-vs.-bitmap-graphics",
    "href": "slides/x2-r-graphics.html#vector-vs.-bitmap-graphics",
    "title": "x2-Graphics with R",
    "section": "Vector vs. Bitmap Graphics",
    "text": "Vector vs. Bitmap Graphics\nBitmap formats\n\njpg, png, tiff\nfixed resolution, cannot be magnified without loss\nretouching possible, but not editing\nwell suited for pictures or plots with huge number of data (color maps)\ncannot be converted to vector without complications and quality loss\n\nVector formats\n\nsvg, pdf, [wmf, emf]\ncan be up- and downscaled and edited\nwell suited drawings and diagrams (except if huge amount of data)\ncan always be converted to bitmap",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#writing-figures-directly-to-pdf",
    "href": "slides/x2-r-graphics.html#writing-figures-directly-to-pdf",
    "title": "x2-Graphics with R",
    "section": "Writing figures directly to PDF",
    "text": "Writing figures directly to PDF\n\n\n\npdf(\"myfile.pdf\", width=8, height=6)\npar(las=1)\nplot(iris$Sepal.Length, iris$Petal.Length, col=iris$Species)\ndev.off()\n\n\n\nwidth and height in inch (1 inch = 2.54cm)\nprofessional quality, size can be changed without quality loss\nconversion to PNG can be done later with free programs\n\n\\(\\rightarrow\\) Inkscape, SumatraPDF, ImageMagick",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#writing-figures-directly-to-png",
    "href": "slides/x2-r-graphics.html#writing-figures-directly-to-png",
    "title": "x2-Graphics with R",
    "section": "Writing figures directly to PNG",
    "text": "Writing figures directly to PNG\n\n\npng(\"myfile.png\", width=1600, height=1200, res=300)  # good for Word\n#png(\"myfile.png\", width=800, height=600, res=150)   # good for Powerpoint\npar(las=1)\npar(mar=c(5, 5, 1, 1))\nplot(iris$Sepal.Length, iris$Petal.Length)\ndev.off()\n\n\nwidth and height given in pixels\nHint: play with res to change nominal resolution and font size\nuse at least 300 dpi (dots per inch, i.e. number of pixels = 300/2.54 * width in cm)\nprofessionals use 600 or even 1200 pixels per inch, but then .docx and .pptx files will dramatically increase\n1600 x 1200px is good for 13.3 x 10 cm size in the printed document",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#font-size-of-ggplot-figures",
    "href": "slides/x2-r-graphics.html#font-size-of-ggplot-figures",
    "title": "x2-Graphics with R",
    "section": "Font size of ggplot figures",
    "text": "Font size of ggplot figures\n\nAppearance and font sizes of ggplot figures can be controlled with themes.\nIt makes sense to create a theme separately and then add it with “+”.\n\n\n\nlibrary(ggplot2)\ndata(iris) \n\n# define a theme with  user-specified font sizes\nfigure_theme &lt;- theme(\n  axis.text    = element_text(size = 12),\n  axis.title   = element_text(size = 12, face = \"bold\"),\n  legend.title = element_text(size = 12, face = \"bold\"),\n  legend.text  = element_text(size = 12))\n\n# ggplots can be stored in a variable\np &lt;- iris |&gt; \n  ggplot(aes(Petal.Length, Petal.Width, colour = Species)) +  \n  geom_point() + figure_theme\n\nPrint to a file:\n\npng(\"iris.png\", width=1600, height=1000, res=300)\nprint(p)\ndev.off()\n\n\nPrint to the screen:\n\n\n\n\n\n\n\n\n\nMore about themes can be found in the books of Chang (2024) and Wickham et al. (in press).",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#example-solar-radiation-data-in-dresden",
    "href": "slides/x2-r-graphics.html#example-solar-radiation-data-in-dresden",
    "title": "x2-Graphics with R",
    "section": "Example: Solar Radiation Data in Dresden",
    "text": "Example: Solar Radiation Data in Dresden\n\nradiation &lt;- read.csv(\"../data/radiation.csv\")\nradiation$Date &lt;- as.Date(radiation$date)\n\n\nplot(radiation$Date, radiation$rad)\n\n\nNote: The data set contains derived data from the German Weather Service (http://www.dwd.de), station Dresden. Missing data were interpolated.",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#date-and-time-classes-in-r",
    "href": "slides/x2-r-graphics.html#date-and-time-classes-in-r",
    "title": "x2-Graphics with R",
    "section": "Date and time classes in R",
    "text": "Date and time classes in R\n\n\nMost important classes\n\nas.Date (dates only)\nas.POSIXct (date and time)\n\nformat and strptime\nextract day, month, year, Julian day\ntime series objects tseriesand zoo",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#format-and-strptime",
    "href": "slides/x2-r-graphics.html#format-and-strptime",
    "title": "x2-Graphics with R",
    "section": "format and strptime",
    "text": "format and strptime\n\nformat(x, format = \"\", tz = \"\", usetz = FALSE, ...)\n\n\n\n\n%Y\nyear with century\n\n\n%m\nmonth as decimal number\n\n\n%d\nday of the month\n\n\n%H\nhours as decimal number (00-23)\n\n\n%M\nminute as decimal number (00-59)\n\n\n%S\nsecond as decimal number (00-59)\n\n\n%j\nday of year (001-366)\n\n\n%u\nweekday, Monday is 1\n\n\n\n\nas.Date(\"11.03.2015\", format=\"%d.%m.%Y\")\n\n[1] \"2015-03-11\"",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#date-conversion-for-the-solar-radiation-data-set",
    "href": "slides/x2-r-graphics.html#date-conversion-for-the-solar-radiation-data-set",
    "title": "x2-Graphics with R",
    "section": "Date conversion for the solar radiation data set",
    "text": "Date conversion for the solar radiation data set\n\nradiation$year &lt;- format(radiation$Date, \"%Y\")\nradiation$month &lt;- format(radiation$Date, \"%m\")\nradiation$doy &lt;- format(radiation$Date, \"%j\")\nradiation$weekday &lt;- format(radiation$Date, \"%u\")\n\nhead(radiation)\n\n        date rad interpolated       Date year month doy weekday\n1 1981-01-01 197            0 1981-01-01 1981    01 001       4\n2 1981-01-02  89            0 1981-01-02 1981    01 002       5\n3 1981-01-03  49            0 1981-01-03 1981    01 003       6\n4 1981-01-04 111            0 1981-01-04 1981    01 004       7\n5 1981-01-05 161            0 1981-01-05 1981    01 005       1\n6 1981-01-06  55            0 1981-01-06 1981    01 006       2\n\n\n\nThe lubridate package has date and time functions that are easier to use.",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#summarize-data-with-aggregate",
    "href": "slides/x2-r-graphics.html#summarize-data-with-aggregate",
    "title": "x2-Graphics with R",
    "section": "Summarize data with aggregate",
    "text": "Summarize data with aggregate\nSyntax\n\naggregate(x, by, FUN, ..., simplify = TRUE)\n\nExample\n\nyearmax &lt;- aggregate(\n  list(rad = radiation$rad),\n  list(year = radiation$year),\n  max)\n\nmonmean &lt;- aggregate(\n  list(radiation = radiation$rad),\n  list(year = radiation$year, month = radiation$month),\n  mean)\n\n\naggregate is essentially a wrapper to apply",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#plot-aggregated-radiation-data",
    "href": "slides/x2-r-graphics.html#plot-aggregated-radiation-data",
    "title": "x2-Graphics with R",
    "section": "Plot aggregated radiation data",
    "text": "Plot aggregated radiation data\n\npar(mfrow=c(1,2), las=1)\nboxplot(rad ~ year, data = radiation)\nboxplot(rad ~ month, data = radiation)\n\n\nMost functions that support a formula argument (containing ~) allow to specify the data frame with a data argument.",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#different-plotting-packages-with-different-philosophies",
    "href": "slides/x2-r-graphics.html#different-plotting-packages-with-different-philosophies",
    "title": "x2-Graphics with R",
    "section": "Different plotting packages with different philosophies",
    "text": "Different plotting packages with different philosophies\n\n\nbase graphics\npackage lattice\npackage ggplot2\nManipulation of plots\n\nset size and fonts; save plots to disk\nuse pdf, svg or png – not jpg - except for photographs\n\nRelated software\n\nedit/convert svg (and pdf) with Inkscape\nconvert images with ImageMagick",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#base-graphics-1",
    "href": "slides/x2-r-graphics.html#base-graphics-1",
    "title": "x2-Graphics with R",
    "section": "Base Graphics",
    "text": "Base Graphics\n\nx &lt;- rnorm(100)\npar(mfrow=c(2,2))\nplot(x)\nhist(x)\nqqnorm(x)\nboxplot(x)",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#grid-and-gridbase",
    "href": "slides/x2-r-graphics.html#grid-and-gridbase",
    "title": "x2-Graphics with R",
    "section": "grid and gridBase",
    "text": "grid and gridBase\n\ncomplete freedom to organise plotting area\ninterface relatively raw\nbasis of other plotting packages",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#lattice-graphics",
    "href": "slides/x2-r-graphics.html#lattice-graphics",
    "title": "x2-Graphics with R",
    "section": "Lattice Graphics",
    "text": "Lattice Graphics\n\nImplements “trellis graphics” (i.e. gridded graphics) in R\nSarkar, D. (2008). Lattice: multivariate data visualization with R. Springer Science & Business Media.\n\n\nrequire(lattice)\ndata(iris)\nxyplot(Sepal.Length ~ Sepal.Width|Species, data=iris, layout=c(3,1))",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#ggplot2",
    "href": "slides/x2-r-graphics.html#ggplot2",
    "title": "x2-Graphics with R",
    "section": "ggplot2",
    "text": "ggplot2\n\n\nImplements the “Grammar of Graphics”\n\nLeland Wilkinson (2005) The Grammar of Graphics. 2nd edn. Springer\nHadley Wickham (2009, 2016) ggplot2: Elegant Graphics for Data Analysis. Springer.\n\nvery popular, part of the tidyverse family of packages\nhttps://ggplot2.tidyverse.org/",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#ggplot-example",
    "href": "slides/x2-r-graphics.html#ggplot-example",
    "title": "x2-Graphics with R",
    "section": "ggplot-Example",
    "text": "ggplot-Example\n\nlibrary(ggplot2)\ndata(iris)\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = factor(Species))) + \n  geom_point() + \n  stat_smooth(method = \"lm\")",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#pipelines-and-faceting-in-ggplot2",
    "href": "slides/x2-r-graphics.html#pipelines-and-faceting-in-ggplot2",
    "title": "x2-Graphics with R",
    "section": "Pipelines and faceting in ggplot2",
    "text": "Pipelines and faceting in ggplot2\n\nlibrary(\"dplyr\")\nlibrary(\"lubridate\")\nlibrary(\"ggplot2\")\nread.csv(\"../data/radiation.csv\") |&gt;\n  mutate(year=year(date), doy=yday(date)) |&gt;\n  ggplot(aes(doy, rad)) + geom_line() + facet_wrap(. ~ year)",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x2-r-graphics.html#further-reading",
    "href": "slides/x2-r-graphics.html#further-reading",
    "title": "x2-Graphics with R",
    "section": "Further Reading",
    "text": "Further Reading\n\nMore presentations\n\nR Basics\nFunctions everywhere\n\nBooks\n\nChang, W. (2024) R Graphics Cookbook. O’Reilly.\nWickham, H. Navarro, D. and Pedersen, T.L. (in press) ggplot2: Elegant Graphics for Data Analysis\n\nManuals\nMore details in the official R manuals, especially in An Introduction to R\nVideos\nMany videos can be found on Youtube, at the Posit webpage and somewhere else.\nThis tutorial was made with Quarto\nContact\nAuthor: tpetzoldt +++ Homepage +++ Github page",
    "crumbs": [
      "R Topics",
      "x2-Graphics with R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#prerequisites",
    "href": "slides/x4-pipes-intro.html#prerequisites",
    "title": "x4-Pipelines in R",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nThe examples in slis slide require the following R packages: packages, that must be installed and loaded.\n\nInstallation\n\ninstall.packages(c(\"dplyr\", \"tiryr\", \"lubridate\", \"readxl\", \"ggplot2\"))\n\n\nLoading\n\nlibrary(\"dplyr\")\nlibrary(\"tidyr\")\nlibrary(\"lubridate\")\nlibrary(\"readxl\")\nlibrary(\"ggplot2\")\n\n\nThe examples were tested with R 4.3.1 and RStudio 2023.06.1",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#an-introductory-example",
    "href": "slides/x4-pipes-intro.html#an-introductory-example",
    "title": "x4-Pipelines in R",
    "section": "An introductory example",
    "text": "An introductory example\n\nIn a statistics course, two samples of Maple (Acer platanoides) leaves were collected by two groups of students:\ngroup HSE: had the freedom to collect leaves individually from trees close to the institute\ngroup HYB: got a random sample from the supervisor\n\nHypothesis: sampling bias may affect statistical parameters, especially mean and variance.\n\nDownload the file leaves.csv, save it to a working directory and then read it with read.csv:\n\nleaves &lt;- read.csv(\"leaves.csv\") \n\n\nHave a look at the data:\n\nhead(leaves)\n\n  group no length width stalk\n1   HSE  1     83    87    74\n2   HSE  2    130   153   105\n3   HSE  3    140   148   135\n4   HSE  4    102   110    94\n5   HSE  5    190   151    89\n6   HSE  6    225   139    91",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#a-boxplot",
    "href": "slides/x4-pipes-intro.html#a-boxplot",
    "title": "x4-Pipelines in R",
    "section": "A boxplot",
    "text": "A boxplot\n\n\n\nboxplot(width ~ group, data=leaves)",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics",
    "href": "slides/x4-pipes-intro.html#summary-statistics",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics",
    "text": "Summary statistics\n\n\nsummary(leaves)\n\n    group                 no             length           width      \n Length:126         Min.   :  1.00   Min.   : 37.00   Min.   : 44.0  \n Class :character   1st Qu.: 32.25   1st Qu.: 72.00   1st Qu.: 96.0  \n Mode  :character   Median : 63.50   Median : 90.00   Median :118.5  \n                    Mean   : 63.50   Mean   : 95.83   Mean   :117.4  \n                    3rd Qu.: 94.75   3rd Qu.:102.00   3rd Qu.:138.5  \n                    Max.   :126.00   Max.   :250.00   Max.   :199.0  \n     stalk       \n Min.   : 29.00  \n 1st Qu.: 62.00  \n Median : 82.00  \n Mean   : 81.89  \n 3rd Qu.:101.00  \n Max.   :175.00",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-per-group",
    "href": "slides/x4-pipes-intro.html#summary-statistics-per-group",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics per group",
    "text": "Summary statistics per group\n\n\ndifferent ways to calculate summary statistics\nclassical method with aggregate\n\nA few examples\n\naggregate(cbind(length, width, stalk) ~ group, mean, data=leaves)\n\n  group    length    width     stalk\n1   HSE 137.82857 139.0286 102.02857\n2   HYB  79.67033 109.1319  74.14286\n\naggregate(cbind(length, width, stalk) ~ group, sd, data=leaves)\n\n  group   length    width    stalk\n1   HSE 47.99356 26.07285 26.47917\n2   HYB 19.93771 30.90423 25.24087\n\naggregate(cbind(length, width, stalk) ~ group, min, data=leaves)\n\n  group length width stalk\n1   HSE     83    87    52\n2   HYB     37    44    29\n\n\n\naggregate is very powerful, but the modern “tidyverse” approach is easier to understand. This is explained in the following.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-with-the-dplyr-package",
    "href": "slides/x4-pipes-intro.html#summary-statistics-with-the-dplyr-package",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics with the dplyr-package",
    "text": "Summary statistics with the dplyr-package\nPackage dplyr contains two handy functions:\n\ngroup_by\nsummarize\n\nThe functions can be combined in different ways:\nA) Two separate code lines\n\nleaves_grouped &lt;- group_by(leaves, group)\nsummarize(leaves_grouped, mean = mean(width), sd = sd(width), min = min(width), max = max(width))\n\n\\(\\ominus\\) needs a temporary variable: leaves_grouped\n B) One line, group_by enclosed in parentheses\n\nsummarize(group_by(leaves, group), mean=mean(width), sd=sd(width), min=min(width), max=max(width))\n\n\\(\\oplus\\) no temporary variables necessary  \\(\\ominus\\) nested parentheses",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#more-streamlined-pipelines",
    "href": "slides/x4-pipes-intro.html#more-streamlined-pipelines",
    "title": "x4-Pipelines in R",
    "section": "More streamlined: pipelines",
    "text": "More streamlined: pipelines\n\nAim to make code easier to understand:\n\navoids nested parentheses\navoids temporary variables\n\nRecent versions of R (since 4.1) have built-in support for pipelines:\n\nNative pipeline operator |&gt;\n\nA predecessor was the so-called “magrittr” pipeline operator %&gt;%\n\nMost examples from these slides will work with both pipeline operators.\nThere are a few differences regarding use of so-called placeholders.\nI recommended to prefer native pipes |&gt;\n\n\n\nThe `%&gt;%-pipeline was introduced by the user-contributed magrittr package (Bache & Wickham, 2022) and became very popular. It is automatically loaded by the dplyr package (Wickham, François, et al., 2023). In his new book, Hadley Wickham recommends native pipes (Wickham, Çetinkaya-Rundel, et al., 2023).",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#application-of",
    "href": "slides/x4-pipes-intro.html#application-of",
    "title": "x4-Pipelines in R",
    "section": "Application of |>",
    "text": "Application of |&gt;\n\nThe output from the first function is piped to the next\n\ngroup_by(leaves, group) |&gt;\nsummarize(mean = mean(width), sd = sd(width), \n          min = min(width), max = max(width))\n\n\nOr, even more streamlined: start pipeline with the data frame\n\nleaves |&gt;\n  group_by(group) |&gt;\n  summarize(mean = mean(width), sd = sd(width), \n            min = min(width), max = max(width))\n\n# A tibble: 2 × 5\n  group  mean    sd   min   max\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 HSE    139.  26.1    87   195\n2 HYB    109.  30.9    44   199",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#how-it-works",
    "href": "slides/x4-pipes-intro.html#how-it-works",
    "title": "x4-Pipelines in R",
    "section": "How it works",
    "text": "How it works\nThe pipe operator |&gt; inserts the output from one function into the first argument of the next function.\n\n\nClassical functional style\n\ngroup_by(leaves, group)\n\n\n\n\n\n\n\n\nPipeline style\n\nleaves |&gt; group_by(group)",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables",
    "href": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics for all variables",
    "text": "Summary statistics for all variables\n\nThe leaves dataset contains different variables length, width and stalk length.\nWe can now, in principle, extend summarize:\n\nAdd more code rows\n\nleaves |&gt;\n  group_by(group) |&gt;\n  summarize(mean_l=mean(width),  sd_l=sd(width),  min_l=min(width),  max_l=max(width),\n            mean_w=mean(length), sd_w=sd(length), min_w=min(length), max_w=max(length),\n            mean_s=mean(stalk),  sd_s=sd(stalk),  min_s=min(stalk),  max_s=max(stalk)\n  )\n\n\nIs copy and paste a good idea?\nNo, at least not in excess.\n\nCopy and paste can lead to errors.\n… and there are more compact and elegant ways.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#tidy-your-data-and-use-long-data-formats",
    "href": "slides/x4-pipes-intro.html#tidy-your-data-and-use-long-data-formats",
    "title": "x4-Pipelines in R",
    "section": "Tidy your data and use “long” data formats!",
    "text": "Tidy your data and use “long” data formats!\n\n\nLong data formats are more database like and more flexible.\n\nIf you are used to working with LibeOffiice or Excel, you will probably prefer “wide” tables that fit well on the computer screen. However, this is not such a good idea for data bases and scripted data science.\nModern data analysis packages like dplyr and ggplot2 mandatorily require the long format.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#long-data-format-tidy-format",
    "href": "slides/x4-pipes-intro.html#long-data-format-tidy-format",
    "title": "x4-Pipelines in R",
    "section": "Long data format (= tidy format)",
    "text": "Long data format (= tidy format)\n\nPut data from all 3 variables in one column: length, width, stalk \\(\\rightarrow\\) value\nIdentifier column for the variables: name\n\n\nWide format\n\n\n\n\n\ngroup\nno\nlength\nwidth\nstalk\n\n\n\n\nHSE\n1\n83\n87\n74\n\n\nHSE\n2\n130\n153\n105\n\n\nHSE\n3\n140\n148\n135\n\n\nHSE\n4\n102\n110\n94\n\n\nHSE\n5\n190\n151\n89\n\n\nHSE\n6\n225\n139\n91\n\n\nHSE\n7\n195\n165\n76\n\n\nHSE\n8\n216\n135\n113\n\n\nHSE\n9\n250\n195\n119\n\n\nHSE\n10\n152\n168\n158\n\n\n\n\n\n\nLong format\n\n\n\n\n\ngroup\nno\nname\nvalue\n\n\n\n\nHSE\n1\nlength\n83\n\n\nHSE\n1\nwidth\n87\n\n\nHSE\n1\nstalk\n74\n\n\nHSE\n2\nlength\n130\n\n\nHSE\n2\nwidth\n153\n\n\nHSE\n2\nstalk\n105\n\n\nHSE\n3\nlength\n140\n\n\nHSE\n3\nwidth\n148\n\n\nHSE\n3\nstalk\n135\n\n\nHSE\n4\nlength\n102\n\n\n\n\n\n… … …",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#long-data-format-with-pivot_longer",
    "href": "slides/x4-pipes-intro.html#long-data-format-with-pivot_longer",
    "title": "x4-Pipelines in R",
    "section": "Long data format with pivot_longer",
    "text": "Long data format with pivot_longer\n\n\n\nleaves |&gt; \n  pivot_longer(c(\"length\", \"width\", \"stalk\"))\n\n# A tibble: 378 × 4\n   group    no name   value\n   &lt;chr&gt; &lt;int&gt; &lt;chr&gt;  &lt;int&gt;\n 1 HSE       1 length    83\n 2 HSE       1 width     87\n 3 HSE       1 stalk     74\n 4 HSE       2 length   130\n 5 HSE       2 width    153\n 6 HSE       2 stalk    105\n 7 HSE       3 length   140\n 8 HSE       3 width    148\n 9 HSE       3 stalk    135\n10 HSE       4 length   102\n# ℹ 368 more rows",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables-groupwise",
    "href": "slides/x4-pipes-intro.html#summary-statistics-for-all-variables-groupwise",
    "title": "x4-Pipelines in R",
    "section": "Summary statistics for all variables groupwise",
    "text": "Summary statistics for all variables groupwise\n\n\n\nleaves |&gt; \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |&gt;\n  group_by(group, name) |&gt;\n  summarize(mean = mean(value), \n            sd   = sd(value), \n            min  = min(value),\n            max  = max(value))\n\n# A tibble: 6 × 6\n# Groups:   group [2]\n  group name    mean    sd   min   max\n  &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;\n1 HSE   length 138.   48.0    83   250\n2 HSE   stalk  102.   26.5    52   175\n3 HSE   width  139.   26.1    87   195\n4 HYB   length  79.7  19.9    37   124\n5 HYB   stalk   74.1  25.2    29   144\n6 HYB   width  109.   30.9    44   199",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#pipes-and-the-assignment-operator--",
    "href": "slides/x4-pipes-intro.html#pipes-and-the-assignment-operator--",
    "title": "x4-Pipelines in R",
    "section": "Pipes and the assignment operator <-",
    "text": "Pipes and the assignment operator &lt;-\n\n\nIn the examples before, the pipe-output was directly printed to the screen\nIf we need the result in a subsequent operation, we assign it to a variable as usual with &lt;-\n\n\n\ntotals &lt;- \n  leaves |&gt; \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |&gt;\n  group_by(group, name) |&gt;\n  summarize(mean=mean(value), sd=sd(value), min=min(value), max=max(value))\n\n\nDon’t get confused!\n\nthe pipe starts with leaves in the second code line\nthe direction of the pipeline is from left \\(\\rightarrow\\) right\nthen the of the complete pipeline is assigned to totals\n\nIt follows the convention, that the result of an equation is assigned from right to the left.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#reverse-assignment",
    "href": "slides/x4-pipes-intro.html#reverse-assignment",
    "title": "x4-Pipelines in R",
    "section": "Reverse assignment?",
    "text": "Reverse assignment?\n\n“More logical”, but less common would be a consequent left to the right notation with -&gt;\n\n\nleaves |&gt; \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |&gt;\n  group_by(group, name) |&gt;\n  summarize(mean=mean(value), sd=sd(value), min=min(value), max=max(value)) -&gt;\n  totals\n\n\nAmelia McNamara used this style in her keynote talk at the 2020 use!R conference about Speaking R on youtube.\nBut, Headley Wickham discouraged this style, because the -&gt; breaks with mathematical convention and is difficult to spot in the code.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#indentation",
    "href": "slides/x4-pipes-intro.html#indentation",
    "title": "x4-Pipelines in R",
    "section": "Indentation",
    "text": "Indentation\n\n\ntotals &lt;- \n  leaves |&gt; \n  pivot_longer(c(\"length\", \"width\", \"stalk\")) |&gt;\n  group_by(group, name) |&gt;\n  summarize(mean=mean(value), sd=sd(value), min=min(value), max=max(value))\n\n\nThe pipeline above shows essentially one single line of code.\n\nTo improve readability, code lines should not be longer than 80 characters.\nRemember: Line breaks can be at any position, as long as a code line is not complete.\nCommon style: make a newline after &lt;- and |&gt; and use 2 characters for indentation.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#the-clementine-orange-data-set",
    "href": "slides/x4-pipes-intro.html#the-clementine-orange-data-set",
    "title": "x4-Pipelines in R",
    "section": "The Clementine orange data set",
    "text": "The Clementine orange data set",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#clementine-orange-data-set",
    "href": "slides/x4-pipes-intro.html#clementine-orange-data-set",
    "title": "x4-Pipelines in R",
    "section": "Clementine orange data set",
    "text": "Clementine orange data set\n\n\nSamples of clementine oranges, measured, weighed and consumed in a statistic course.\nExcel file with two tables:\n\nlong table with the fruits\nshorter table brands with meta data\n\nData can be downloaded from here.\n\n\nRead data directly from Excel file\n\nbrands  &lt;- read_excel(\"clementines_2019.xlsx\", \"Brands\")\nfruits  &lt;- read_excel(\"clementines_2019.xlsx\", \"Fruits\")",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#clementine-orange-data-set-1",
    "href": "slides/x4-pipes-intro.html#clementine-orange-data-set-1",
    "title": "x4-Pipelines in R",
    "section": "Clementine orange data set",
    "text": "Clementine orange data set\n\n\nTable “fruits”\n\n\n\n\n\nyear\nbrand\nweight\nwidth\nheight\n\n\n\n\n2019\nEB\n107\n61.0\n54\n\n\n2019\nEB\n100\n65.0\n55\n\n\n2019\nEB\n89\n58.0\n49\n\n\n2019\nEB\n99\n62.0\n52\n\n\n2019\nEB\n99\n64.0\n58\n\n\n2019\nEB\n96\n63.0\n59\n\n\n2019\nEB\n100\n54.0\n54\n\n\n2019\nEB\n89\n58.5\n47\n\n\n2019\nEB\n86\n66.0\n48\n\n\n2019\nEB\n92\n43.0\n33\n\n\n2019\nEB\n102\n48.0\n33\n\n\n2019\nEB\n92\n43.0\n32\n\n\n\n\n\n\nTable “brands”\n\n\n\n\n\nyear\nbrand\ntype\nkilogram\nprice\n\n\n\n\n2019\nEB\nBasic\n1.50\n2.99\n\n\n2019\nEP\nGold\n0.75\n1.49\n\n\n2019\nLB\nBasic\n1.00\n1.49\n\n\n2019\nLO\nBio\n0.50\n1.49\n\n\n2019\nNX\nBox\n2.30\n2.99\n\n\n2019\nNP\nPremium\n1.00\n2.29\n\n\n2019\nNB\nBasic\n1.00\n1.49\n\n\n2019\nNC\nBasic\n1.00\n1.49",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#database-join",
    "href": "slides/x4-pipes-intro.html#database-join",
    "title": "x4-Pipelines in R",
    "section": "Database join",
    "text": "Database join\n\njoin two tables to bring the information together\nIn case of left_join, the (larger) main table is at the left.\nThe tables have two key fields in common: year and brand.\nThe key fields can be automatically detected or explicitly specified or renamed \\(\\rightarrow\\) help page of left_join\n\n\n\nfruits2 &lt;- left_join(fruits, brands)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nno\nyear\nbrand\nweight\nwidth\nheight\ntaste\nshop\ntype\nquality\nkilogram\nprice\n\n\n\n\n1\n2019\nEB\n107\n61\n54\n6\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n2\n2019\nEB\n100\n65\n55\n7\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n3\n2019\nEB\n89\n58\n49\n10\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n4\n2019\nEB\n99\n62\n52\n7\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n5\n2019\nEB\n99\n64\n58\n8\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n6\n2019\nEB\n96\n63\n59\n8\nEdeka\nBasic\nBasic\n1.5\n2.99\n\n\n7\n2019\nEB\n100\n54\n54\n7\nEdeka\nBasic\nBasic\n1.5\n2.99",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#selection-of-columns-and-rows-select-and-filter",
    "href": "slides/x4-pipes-intro.html#selection-of-columns-and-rows-select-and-filter",
    "title": "x4-Pipelines in R",
    "section": "Selection of columns and rows: select and filter",
    "text": "Selection of columns and rows: select and filter\n\n\nselect: select columns\nfilter: filters rows\n\n\n\n\nfruits2 |&gt;\n  select(brand, shop, type, weight, width) |&gt;\n  filter(shop %in% c(\"Edeka\", \"Lidl\"))\n\n# A tibble: 44 × 5\n   brand shop  type  weight width\n   &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1 EB    Edeka Basic    107  61  \n 2 EB    Edeka Basic    100  65  \n 3 EB    Edeka Basic     89  58  \n 4 EB    Edeka Basic     99  62  \n 5 EB    Edeka Basic     99  64  \n 6 EB    Edeka Basic     96  63  \n 7 EB    Edeka Basic    100  54  \n 8 EB    Edeka Basic     89  58.5\n 9 EB    Edeka Basic     86  66  \n10 EB    Edeka Basic     92  43  \n# ℹ 34 more rows",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#create-or-transform-columns-mutate",
    "href": "slides/x4-pipes-intro.html#create-or-transform-columns-mutate",
    "title": "x4-Pipelines in R",
    "section": "Create or transform columns: mutate",
    "text": "Create or transform columns: mutate\n\nExample\nTransform a variable, e.g. weight by \\(x^{1/3}\\) into a theoretical mean diameter\n\nfruits2 &lt;- \n  fruits2 |&gt;\n  mutate(L_mean = weight^(1/3))\n\n\nShow results\nClassical boxplot\n\n\nboxplot(L_mean ~ brand, data=fruits2)",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#plot-in-tidyverse-style-with-pipes-and-ggplot",
    "href": "slides/x4-pipes-intro.html#plot-in-tidyverse-style-with-pipes-and-ggplot",
    "title": "x4-Pipelines in R",
    "section": "Plot in “tidyverse”-style with pipes and ggplot",
    "text": "Plot in “tidyverse”-style with pipes and ggplot\n\nfruits2 |&gt;\n  mutate(L_mean = weight^(1/3)) |&gt;\n  ggplot(aes(brand, L_mean)) + geom_boxplot()",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#another-mutate-example",
    "href": "slides/x4-pipes-intro.html#another-mutate-example",
    "title": "x4-Pipelines in R",
    "section": "Another mutate example",
    "text": "Another mutate example\nLet’s compare the measured weight of our fruits with a “theoretical volume” calculated from length and height using the formula of an ellipsoid. This is of course an approximation:\n\\[\nV = 4/3 \\pi \\cdot \\rm (length/2)^2 \\cdot height/2\n\\]\n\nfruits &lt;-\n  fruits |&gt;\n  mutate(V = 0.001 * 4/3 * pi * (width/2)^2 * height/2, index = weight / V)\n\n\n\n\nlibrary(ggplot2)\nfruits |&gt;\n  ggplot(aes(weight, index)) + \n  geom_point()\n\nThe “+” operator in ggplot looks like a pipeline, but works differently.\nIt adds elements to a plot.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#color-coded-points",
    "href": "slides/x4-pipes-intro.html#color-coded-points",
    "title": "x4-Pipelines in R",
    "section": "Color coded points",
    "text": "Color coded points\n\nlibrary(ggplot2)\nfruits |&gt; ggplot(aes(weight, index, color=brand)) + geom_point()",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#categorial-split-faceting-and-regression-line",
    "href": "slides/x4-pipes-intro.html#categorial-split-faceting-and-regression-line",
    "title": "x4-Pipelines in R",
    "section": "Categorial split (faceting) and regression line",
    "text": "Categorial split (faceting) and regression line\n\nfruits |&gt; \n  ggplot(aes(weight, V)) + \n  geom_point() + \n  geom_smooth(method=lm, se=FALSE) + \n  facet_wrap( ~ brand)",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#modify-font-size",
    "href": "slides/x4-pipes-intro.html#modify-font-size",
    "title": "x4-Pipelines in R",
    "section": "Modify font size",
    "text": "Modify font size\n\nfruits |&gt; \n  ggplot(aes(weight, V)) + \n  geom_point() + \n  geom_smooth(method=lm, se=FALSE) + \n  facet_wrap( ~ brand) +\n  theme(text = element_text(size=24))\n\n\n\\(\\rightarrow\\) themes allow to configure “almost everything” …",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#discharge-of-the-elbe-river",
    "href": "slides/x4-pipes-intro.html#discharge-of-the-elbe-river",
    "title": "x4-Pipelines in R",
    "section": "Discharge of the Elbe River",
    "text": "Discharge of the Elbe River\nElbe River in Dresden 2006-04-01",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#read-data-to-r",
    "href": "slides/x4-pipes-intro.html#read-data-to-r",
    "title": "x4-Pipelines in R",
    "section": "Read data to R",
    "text": "Read data to R\n\nThe example file elbe.csv contains daily discharge of the Elbe River in \\(\\mathrm{m^3 s^{-1}}\\) from gauging station Dresden, river km 55.6. The data are from the Federal Waterways and Shipping Administration (WSV) and where provided by the Federal Institute for Hydrology (BfG).\n\nWe can skip downloading and read the file directly from its internet location:\n\n\nelbe &lt;- read.csv(\"https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/elbe.csv\")\n\n\nThe third column “validated” indicate whether the values were finally approved by WSV and BfG. Data of the 19th century are particularly uncertain. Please consult the file elbe_info.txt for details.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#date-and-time-conversion",
    "href": "slides/x4-pipes-intro.html#date-and-time-conversion",
    "title": "x4-Pipelines in R",
    "section": "Date and time conversion",
    "text": "Date and time conversion\n\nNow, let’s extend the elbe data frame by adding information about the day, month, year and day of year. Here function mutate adds additional columns.\nNote also that the day of year function in the date and time package lubridate is named yday.\nDetails about date and time conversion can be found in the lubridate cheatsheet.\n\n\nlibrary(lubridate) # a tidyverse package for dates\nelbe &lt;- mutate(elbe,\n               date  = as.POSIXct(date),\n               day   = day(date), \n               month = month(date), \n               year  = year(date), \n               doy   = yday(date))",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#inspect-data-structure",
    "href": "slides/x4-pipes-intro.html#inspect-data-structure",
    "title": "x4-Pipelines in R",
    "section": "Inspect data structure",
    "text": "Inspect data structure\nIf we work with RStudio, may have a look at the “Global Environment” pane and inspect the data structure of the elbe data frame.\n\n\n\n\n\ndate\ndischarge\nvalidated\nday\nmonth\nyear\ndoy\n\n\n\n\n1989-01-01\n765\nTRUE\n1\n1\n1989\n1\n\n\n1989-01-02\n713\nTRUE\n2\n1\n1989\n2\n\n\n1989-01-03\n684\nTRUE\n3\n1\n1989\n3\n\n\n1989-01-04\n612\nTRUE\n4\n1\n1989\n4\n\n\n1989-01-05\n565\nTRUE\n5\n1\n1989\n5\n\n\n1989-01-06\n519\nTRUE\n6\n1\n1989\n6\n\n\n1989-01-07\n522\nTRUE\n7\n1\n1989\n7\n\n\n1989-01-08\n524\nTRUE\n8\n1\n1989\n8\n\n\n1989-01-09\n544\nTRUE\n9\n1\n1989\n9\n\n\n1989-01-10\n539\nTRUE\n10\n1\n1989\n10\n\n\n1989-01-11\n606\nTRUE\n11\n1\n1989\n11\n\n\n1989-01-12\n606\nTRUE\n12\n1\n1989\n12",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#annual-summary-statistics",
    "href": "slides/x4-pipes-intro.html#annual-summary-statistics",
    "title": "x4-Pipelines in R",
    "section": "Annual summary statistics",
    "text": "Annual summary statistics\n\nSummarize data\n\n## calculate annual mean, minimum, maximum\ntotals &lt;- elbe |&gt;\n  group_by(year) |&gt;\n  summarize(mean = mean(discharge), \n            min = min(discharge), \n            max = max(discharge))\n\n\nShow table of summary statistics\n\nhead(totals)\n\n# A tibble: 6 × 4\n   year  mean   min   max\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1989  268.   122   765\n2  1990  217.    89   885\n3  1991  189.    97   634\n4  1992  267.    89  1090\n5  1993  256.    92  1610\n6  1994  317.    92  1030\n\n\n Exercise: Compute monthly discharge mean values and monthly sums.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#more-about-pivot-tables",
    "href": "slides/x4-pipes-intro.html#more-about-pivot-tables",
    "title": "x4-Pipelines in R",
    "section": "More about pivot tables",
    "text": "More about pivot tables\n\n\nIn a section before, we already used pivot_longer to reorganize data. Now we do the opposite and convert a data base table (long data format) into a cross-table (wide data format) and vice versa.\nR provides several function pairs for this, so you may see functions like melt and cast or gather and spread.\nRecently the two functions pivot_wider and pivot_longer were recommended for this purpose.\n\nThe first argument is a data base table, the other arguments define the structure of the desired crosstable.\nid_cols is the name of a column in a long table that will become the rows\nnames_from indicates where the names of the columns are taken from\nvalues_from is the column with the values for the cross table.\n\n\n\\(\\rightarrow\\) If more than one value exists for a row x column combination, an optional aggregation function values_fn can be given.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#crosstable-with-one-column-per-year",
    "href": "slides/x4-pipes-intro.html#crosstable-with-one-column-per-year",
    "title": "x4-Pipelines in R",
    "section": "Crosstable with one column per year",
    "text": "Crosstable with one column per year\n\n\nelbe_wide &lt;-  elbe |&gt;\n  pivot_wider(id_cols = doy, \n              names_from = year, \n              values_from = discharge, \n              values_fn = mean)\nelbe_wide\n\n\nExercise\n\nCreate a suitable crosstable elbe_wide.\nThen create a crosstable for monthly maximum discharge over all years.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "href": "slides/x4-pipes-intro.html#back-conversion-of-a-crosstable-into-a-data-base-table",
    "title": "x4-Pipelines in R",
    "section": "Back-conversion of a crosstable into a data base table",
    "text": "Back-conversion of a crosstable into a data base table\n\nThe inverse case is also possible, e.g. the conversion of a cross table into a data base table. It can be done with the function pivot_longer. The column of the id.vars variable(s) will become identifier(s) downwards.\n\n\npivot_longer(elbe_wide, names_to=\"year\", cols=as.character(1989:2019))\n\n\nExercise\nTry it yourself.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "href": "slides/x4-pipes-intro.html#minimum-maximum-plot-with-summarize-and-ggplot2",
    "title": "x4-Pipelines in R",
    "section": "Minimum-Maximum plot with summarize and ggplot2",
    "text": "Minimum-Maximum plot with summarize and ggplot2\n\nelbe |&gt; \n  mutate(doy = yday(date)) |&gt;\n  group_by(doy) |&gt;\n  summarize(max = max(discharge), \n            mean = mean(discharge), \n            min = min(discharge)) |&gt;\n  pivot_longer(cols = c(\"min\", \"mean\", \"max\"), \n               names_to = \"statistic\", \n               values_to = \"discharge\") |&gt;\n  ggplot(aes(doy, discharge, color = statistic)) + geom_line()\n\n\nExercise\n\nRead the code and try to understand it. Then add a dry and a wet year.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#cumulative-sums",
    "href": "slides/x4-pipes-intro.html#cumulative-sums",
    "title": "x4-Pipelines in R",
    "section": "Cumulative sums",
    "text": "Cumulative sums\nAnnual cumulative sum plots are a hydrological standard tool used by reservoir managers. We can use the R function cumsum, that by successive cumulation converts a sequence of:\n\\(x_1, x_2, x_3, x_4, \\dots\\)\ninto\n\\((x_1), (x_1+x_2), (x_1+x_2+x_3), (x_1+x_2+x_3+x_4), \\dots\\)\n\nExample\n\nx &lt;- c(1, 3, 2, 6, 4, 2, 3)\ncumsum(x)\n\n[1]  1  4  6 12 16 18 21",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#cumulative-sums-of-the-elbe-river",
    "href": "slides/x4-pipes-intro.html#cumulative-sums-of-the-elbe-river",
    "title": "x4-Pipelines in R",
    "section": "Cumulative sums of the Elbe River",
    "text": "Cumulative sums of the Elbe River\nCummulative sums allow to detect dry and wet years, or periods within years.\nIf we just use cumsum, we get a cumulative sum for all years:\n\nelbe |&gt; \n  mutate(doy = yday(date), year = year(date)) |&gt;\n  filter(year %in% 2000:2010) |&gt;\n  group_by(year = factor(year)) |&gt;\n  mutate(cum_discharge = cumsum(discharge) * 60*60*24) |&gt;\n  ggplot(aes(doy, cum_discharge, color = year)) + geom_line()\n\n\nThe multiplication with \\(60 \\cdot 60 \\cdot 24\\) converts \\(\\rm m^3 s^{-1}\\) in \\(\\rm m^3 d^{-1}\\).",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#exercises",
    "href": "slides/x4-pipes-intro.html#exercises",
    "title": "x4-Pipelines in R",
    "section": "Exercises",
    "text": "Exercises\n\n\nRepeat the same for other time periods (years).\nWhich year was the wettest, which one the driest year in total? Find a year with dry spring and wet summer.\nIdentify some (e.g. 3 or 5) large floods in the historical time series and plot it together.\nModify the commands so that the hydrological year is shown. Note that the German hydrological year goes from 1st November to 31st October of the following year. Other countries have different regulations.",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#further-reading",
    "href": "slides/x4-pipes-intro.html#further-reading",
    "title": "x4-Pipelines in R",
    "section": "Further reading",
    "text": "Further reading\n\nOnline material\n\n“Welcome to tidyverse: https://dplyr.tidyverse.org/.\n“ggplot Elegant Graphics for Data Analysis: https://ggplot2-book.org/\n“R for Data Science”: https://r4ds.had.co.nz/\nHadley Wickham’s homepage: https://hadley.nz/\n\n\nPrinted books\n\n“R for Data Science” (Wickham, Çetinkaya-Rundel, et al., 2023)\n“ggplot Elegant Graphics for Data Analysis” (Wickham, 2016)",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "slides/x4-pipes-intro.html#references",
    "href": "slides/x4-pipes-intro.html#references",
    "title": "x4-Pipelines in R",
    "section": "References",
    "text": "References\n\n\n\n\nBache, S. M., & Wickham, H. (2022). Magrittr: A forward-pipe operator for r. https://CRAN.R-project.org/package=magrittr\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science (2nd ed.). O’Reiley. https://r4ds.hadley.nz/\n\n\nWickham, H., François, R., Henry, L., Müller, K., & Vaughan, D. (2023). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr",
    "crumbs": [
      "R Topics",
      "x4-Pipelines in R"
    ]
  },
  {
    "objectID": "00-index.html",
    "href": "00-index.html",
    "title": "Elements of Data Analysis and Statistics",
    "section": "",
    "text": "This website contains a collection of material for introductory statistics courses with R. The aim is to provide insight in fundamental principles and an overview to enable students to select and understand specialized books and online material to dig in deeper in the diverse and fascinating field of statistical computing.\nThe corresponding lab exercises are found at https://tpetzoldt.github.io/element-labs/."
  },
  {
    "objectID": "00-index.html#status",
    "href": "00-index.html#status",
    "title": "Elements of Data Analysis and Statistics",
    "section": "Status",
    "text": "Status\nThe selection of topics reflects what I think is useful as door-opener into various fields, useful in ecology and aquatic sciences. It is an ongoing project based on former PDF slides and other course material, constructive comments and suggestions for improvement are welcome."
  },
  {
    "objectID": "00-index.html#further-reading",
    "href": "00-index.html#further-reading",
    "title": "Elements of Data Analysis and Statistics",
    "section": "Further reading",
    "text": "Further reading\n\nDalgaard, P, 2008: Introductory Statistics with R. Springer Verlag, New York, 2nd edition. https://link.springer.com/book/10.1007/b97671\nKleiber, C and Zeileis, A, 2008: Applied Econometrics with R. Springer Verlag, New York. https://link.springer.com/book/10.1007/978-0-387-77318-6\nVerzani, J, 2014: Using R for introductory statistics. CRC press. https://www.routledge.com/Using-R-for-Introductory-Statistics/Verzani/p/book/9781466590731\nWickham, H., Çetinkaya-Rundel, M and Grolemund, G, 2023: R for Data Science. https://r4ds.hadley.nz/"
  },
  {
    "objectID": "00-index.html#related-pages",
    "href": "00-index.html#related-pages",
    "title": "Elements of Data Analysis and Statistics",
    "section": "Related Pages",
    "text": "Related Pages\n\nGerman translation of the slides\nCollection of lab exercises\nDatasets\nSource code of the slides"
  },
  {
    "objectID": "00-index.html#author",
    "href": "00-index.html#author",
    "title": "Elements of Data Analysis and Statistics",
    "section": "Author",
    "text": "Author\nhomepage +++ github\n2025-09-16"
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html",
    "href": "tutorials/s2-multivar-3d.html",
    "title": "Ordination and Clustering",
    "section": "",
    "text": "The following demo demonstrates the reduction of a three dimensional data set into two dimensions by principal component analysis (PCA) and nonmetric multidimensional scaling.\nThe demo uses artificial data and a 3D plotting package, so the code may look somewhat technical. But don’t worry. This is for demonstration of the main principles. Practical demonstrations with real data will follow.",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#introduction",
    "href": "tutorials/s2-multivar-3d.html#introduction",
    "title": "Ordination and Clustering",
    "section": "",
    "text": "The following demo demonstrates the reduction of a three dimensional data set into two dimensions by principal component analysis (PCA) and nonmetric multidimensional scaling.\nThe demo uses artificial data and a 3D plotting package, so the code may look somewhat technical. But don’t worry. This is for demonstration of the main principles. Practical demonstrations with real data will follow.",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#packages-and-data-set",
    "href": "tutorials/s2-multivar-3d.html#packages-and-data-set",
    "title": "Ordination and Clustering",
    "section": "0.2 Packages and data set",
    "text": "0.2 Packages and data set\nFirst we load the required packages and a test data set multivar.csv. It consists of 3 clusters of correlated multivariate normally distributed data points that were generated with the rmvnorm function. Each data subset has a separate mean value and a common variance-covariance matrix \\(\\sigma\\) that is created at random. A separate script is used to generate the data.\n\nlibrary(\"rgl\")\nlibrary(\"vegan\")\nlibrary(\"vegan3d\")\n\nA &lt;- read.csv(\"../data/multivar.csv\")",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#plotting",
    "href": "tutorials/s2-multivar-3d.html#plotting",
    "title": "Ordination and Clustering",
    "section": "0.3 Plotting",
    "text": "0.3 Plotting\nThe data set has three columns, so we can, in principle, plot all column variables versus each other, or use a 3D plot.\nNote: The figure below can be rotated and zoomed with the mouse. Here we employ R’s 3D graphics interface package rgl and the multivariate 3D visualisation package vegan3d.\n\nnsamp    &lt;- nrow(A) / 3  # number of points in each of the 3 samples\nmycolors &lt;- rep(c(\"#e41a1c\", \"#377eb8\", \"#4daf4a\"), each = nsamp)\n\npar(mfrow=c(1,3))\nplot(y ~ x, data=A, col=mycolors)\nplot(y ~ z, data=A, col=mycolors)\nplot(z ~ x, data=A, col=mycolors)\n\n\n\n\n\n\n\n\nWe see that the colors overlap more or less in the 3 figures, so let’s try to improve this in the 3D view. The goal is to rotate the axes such, so that the colored dots form well separated clusters.\n\nordirgl(A, type=\"p\", ax.col = \"black\", col=mycolors, box=FALSE)\nview3d(theta = 5, phi = 15, fov=30, zoom=0.7)\naxes3d(labels=FALSE)\n\n\n\n\n\nWe see that the coordinate axes can be rotated in the direction of maximum variance of the data, so that overlap between data points is minimized. We can try to do this by hand, or let the computer do this. This is then calles a “principal components analysis” (PCA).",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#principal-components",
    "href": "tutorials/s2-multivar-3d.html#principal-components",
    "title": "Ordination and Clustering",
    "section": "0.4 Principal components",
    "text": "0.4 Principal components\nR contains several functions for principal components analysis, for example princomp and prcomp in the stats package or function rda in the vegan package. The result is, that the coordinate system is rotated in the direction om maximum variance, using Eigen value calculations. The resulting “synthetic” new dimensions are then the principal components.\nEach principal component represents then a specific fraction of the total variance of the data in ascending order. This can be plotted as bar chart or printed with summary.\n\npc &lt;- prcomp(A)\nplot(pc)\nbox()\n\n\n\n\n\n\n\nsummary(pc)\n\nImportance of components:\n                          PC1    PC2    PC3\nStandard deviation     3.2753 1.6495 1.1778\nProportion of Variance 0.7231 0.1834 0.0935\nCumulative Proportion  0.7231 0.9065 1.0000\n\n\nThe objects in the rotated coordinate axes can then be visualized with function biplot in 2 dimensions (shown later with real data) or with ordirgl in 3D. Function view3d rotates the plot in the direction of PC1 and PC2, but you can use the mouse to see that there is still a 3rd dimension.\n\npc &lt;- prcomp(A)\nordirgl(pc, type=\"p\", display=\"sites\", \n        ax.col = \"black\", col=mycolors)\nview3d(theta = 0, phi = 0, fov=0, zoom=0.7)\naxes3d()",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#nonmetric-multi-dimensional-scaling-nmds",
    "href": "tutorials/s2-multivar-3d.html#nonmetric-multi-dimensional-scaling-nmds",
    "title": "Ordination and Clustering",
    "section": "0.5 Nonmetric multi-dimensional scaling (NMDS)",
    "text": "0.5 Nonmetric multi-dimensional scaling (NMDS)\nThe PCA is a very useful technique as it reduces dimensions without bias, but as we have seen, large proportions of information may still be found at higher dimensions.\nThis is, where the nonmetric dimensional scaling comes into play. Here, we can request a number of dimensions \\(k\\), typically \\(k=2\\) or \\(k=3\\) and then the computer tries hard to squeeze the information as much as possible into these dimensions. The aim is, that the distances between in all dimensions are represented in two or three dimensions only. Though it is done in an iterative way, it is clear, that this is not perfectly possible and will introduce considerable distortion, called stress.\n\nmds &lt;- metaMDS(A, distance=\"euclid\", scale=TRUE, autotransform = FALSE, k=2)\n\n'comm' has negative data: 'autotransform', 'noshare' and 'wascores' set to FALSE\n\n\nRun 0 stress 0.07782409 \nRun 1 stress 0.0778241 \n... Procrustes: rmse 6.175108e-06  max resid 4.004689e-05 \n... Similar to previous best\nRun 2 stress 0.0972286 \nRun 3 stress 0.09722253 \nRun 4 stress 0.07783593 \n... Procrustes: rmse 0.0003804096  max resid 0.003854329 \n... Similar to previous best\nRun 5 stress 0.09722861 \nRun 6 stress 0.09722861 \nRun 7 stress 0.07783594 \n... Procrustes: rmse 0.0003847878  max resid 0.003892716 \n... Similar to previous best\nRun 8 stress 0.07782409 \n... Procrustes: rmse 1.698278e-05  max resid 0.0001817883 \n... Similar to previous best\nRun 9 stress 0.07782409 \n... New best solution\n... Procrustes: rmse 9.14501e-06  max resid 7.467697e-05 \n... Similar to previous best\nRun 10 stress 0.07782409 \n... New best solution\n... Procrustes: rmse 7.841621e-06  max resid 4.794574e-05 \n... Similar to previous best\nRun 11 stress 0.07782411 \n... Procrustes: rmse 1.956095e-05  max resid 0.00019481 \n... Similar to previous best\nRun 12 stress 0.07782411 \n... Procrustes: rmse 1.899172e-05  max resid 0.0001924791 \n... Similar to previous best\nRun 13 stress 0.07783593 \n... Procrustes: rmse 0.0003814829  max resid 0.003868235 \n... Similar to previous best\nRun 14 stress 0.07782409 \n... New best solution\n... Procrustes: rmse 5.666184e-06  max resid 4.704731e-05 \n... Similar to previous best\nRun 15 stress 0.0778241 \n... Procrustes: rmse 1.33636e-05  max resid 0.0001237828 \n... Similar to previous best\nRun 16 stress 0.0778241 \n... Procrustes: rmse 1.225242e-05  max resid 0.0001308233 \n... Similar to previous best\nRun 17 stress 0.07782409 \n... New best solution\n... Procrustes: rmse 4.292724e-06  max resid 3.463446e-05 \n... Similar to previous best\nRun 18 stress 0.09722252 \nRun 19 stress 0.4151559 \nRun 20 stress 0.09722253 \n*** Best solution repeated 1 times\n\n\nThe resulting stress and the representation of distances in two (or three) dimensions can then be shown in a so-called stressplot. Here, one should not overestimate the hich \\(r^2\\) values, that are always big, even in bad cases. The shape of the step-line does not matter much either. However, the value of stress is most important and the data points should be close to the red line.\n\nstressplot(mds)\n\n\n\n\n\n\n\n\nThe stress itself shuld be small. As a rule of thumb, a stress value of \\(&gt;0.2\\) means that the NMDS was not successful, stress \\(&lt; 0.1\\) is considered as sufficient, \\(&lt;0.05\\) as good, \\(&lt;0.025\\) as very good and \\(\\approx 0\\) as perfect.\n\nmds\n\n\nCall:\nmetaMDS(comm = A, distance = \"euclid\", k = 2, autotransform = FALSE,      scale = TRUE) \n\nglobal Multidimensional Scaling using monoMDS\n\nData:     A \nDistance: euclidean \n\nDimensions: 2 \nStress:     0.07782409 \nStress type 1, weak ties\nBest solution was repeated 1 time in 20 tries\nThe best solution was from try 17 (random start)\nScaling: centring, PC rotation \nSpecies: scores missing\n\n\n\nxyz &lt;- as.data.frame(scores(mds, display=\"sites\"))\nxyz$. &lt;- 0\nordirgl(xyz, type=\"p\", ax.col = \"black\", col=mycolors)\nview3d(theta = 0, phi = 0, fov=0, zoom=0.7)\naxes3d(labels=FALSE, expand=1.5)\n\n\n\n\n\nHere, stress is \\(&lt;0.1\\). This is o.k., so that we now can have a look at the 2D representation. The figure is again technically 3D, so use the mouse to see that the data are now at a plane.",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#cluster-analysis",
    "href": "tutorials/s2-multivar-3d.html#cluster-analysis",
    "title": "Ordination and Clustering",
    "section": "0.6 Cluster analysis",
    "text": "0.6 Cluster analysis\nThe methods discussd so far try to map high dimensional structures to lower dimensions as good as possible, but there is still variation left that is not shown, either because it is in a higher dimension as in PCA or because the mapping is “under stress” (NMDS). This means that points shown closely together may not as similar as they appear in the projection plane.\nCluster analysis takes another route. It shows the distance, not the location. Many different cluster analysis methods exist, here we show just an example of hierarchical clustering with “ward.D2” as agglomeration scheme. More about this will be discussed later.\n\nhc &lt;- hclust(dist(A), method=\"ward.D2\")\nplot(hc)\n\n\n\n\n\n\n\n\nThe result can also be combined with PCA or NMDS, here again a 3D visualization.\n\nhc &lt;- hclust(dist(A), method=\"ward.D2\")\nordirgltree(ord=mds, cluster=hc, col=mycolors)\naxes3d(expand=1.5)\n\n\n\n\n\nThe plot can again be rotated and zoomed in. The x- and y axes show the NMDS coordinates and the y axis the euclidean distance. Two-dimensional plots are of course also possible, practical examples will be given later.",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "tutorials/s2-multivar-3d.html#distance-and-dissimilarity-measures",
    "href": "tutorials/s2-multivar-3d.html#distance-and-dissimilarity-measures",
    "title": "Ordination and Clustering",
    "section": "1.1 Distance and dissimilarity measures",
    "text": "1.1 Distance and dissimilarity measures\n\n1.1.1 Quantitative form\nwith \\(x_{ij}, x_{ik}\\) abundance of species \\(i\\) at sites (\\(j, k\\)).\nEuclidean distance:\n\\[\nd_{jk} = \\sqrt{\\sum (x_{ij}-x_{ik})^2}\n\\]\nManhattan distance: \\[\nd_{jk} = \\sum |x_{ij}-x_{ik}|\n\\]\nGower distance: \\[\nd_{jk} = \\frac{1}{M} \\sum\\frac{|x_{ij}-x_{ik}|}{\\max(x_i)-\\min(x_i)}\n\\]\nBray-Curtis dissimilarity: \\[\nd_{jk} = \\frac{\\sum{|x_{ij}-x_{ik}|}}{\\sum{(x_{ij}+x_{ik})}}\n\\]\n\n\n1.1.2 Binary form\nThe binary form is applicable to binary and factor variables, where:\n\n\\(A, B\\) = numbers of species on compared sites\n\\(J\\) = (joint) is the number of species that occur on both compared sites\n\\(M\\) = number of columns (excluding missing values)\n\nEuclidean: \\(\\sqrt{A+B-2J}\\)\nManhattan: \\(A+B-2J\\)\nGower: \\(\\frac{A+B-2J}{M}\\)\nBray-Curtis: \\(\\frac{A+B-2J}{A+B}\\)\nJaccard: \\(\\frac{2b}{1+b}\\) with \\(b\\) = Bray-Curtis dissimilarity\n\n\n1.1.3 Applications\nAdditional distance measures and application suggestions are found in the vegdist help page.",
    "crumbs": [
      "Tutorials",
      "Ordination and Clustering"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#checking-distributions-for-descriptive-purposes",
    "href": "slides/04-distributions.html#checking-distributions-for-descriptive-purposes",
    "title": "04-Distributions",
    "section": "Checking distributions for descriptive purposes",
    "text": "Checking distributions for descriptive purposes\nIn some disciplines, such as hydrology, it is occasionally necessary to know which distribution type best describes a dataset. This is especially critical for Extreme Value Analysis (e.g., the 100-year flood), as the Central Limit Theorem (CLT) does not apply here.\nProcedure\n\nVisual Inspection (Focus on the Tails)\n\nHistogram (first impression)\nQ-Q Plots (detailed goodness-of-fit)\n\nFormal Tests (exclusion principle)\n\nTests: Kolmogorov-Smirnov, Cramér-von Mises, Anderson-Darling\nReject the distribution type if the p-value is significant (p&lt;0.05).\n\nModel Selection (Choosing the best model)\n\nChoose the model that provides the best fit while also demonstrating Parsimony\nCriteria like AIC/BIC will be covered later\nConsider the physical plausibility of the parameters.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#box-cox-transformation",
    "href": "slides/04-distributions.html#box-cox-transformation",
    "title": "04-Distributions",
    "section": "Box-Cox transformation",
    "text": "Box-Cox transformation\nEstimate optimal transformation from the class of powers and logarithms\n\\[\ny' = y^\\lambda\n\\]\n\n\\(\\lambda=0\\): use logarithmic transformation\nboxcox requires a “model formula” or the outcome of a linear model (lm)\n\n\nlibrary(MASS)\ndat &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/prk_nit.csv\")\nNit90 &lt;- dat$biovol[dat$group == \"nit90\"]\nboxcox(Nit90 ~ 1)",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#box-cox-transformation-ii",
    "href": "slides/04-distributions.html#box-cox-transformation-ii",
    "title": "04-Distributions",
    "section": "Box-Cox transformation II",
    "text": "Box-Cox transformation II\nThe dotted vertical lines and the horizontal 95,%-line show the confidence limits for possible transformations. Here we see that either a logarithmic transformation (\\(\\lambda=0\\)) or a power of approximately 0.5 are suitable.\nIt is also possible to obtain the numerical value directly:\n\nbc &lt;- boxcox(Nit90 ~ 1, plotit = FALSE)\nstr(bc)\n\nList of 2\n $ x: num [1:41] -2 -1.9 -1.8 -1.7 -1.6 -1.5 -1.4 -1.3 -1.2 -1.1 ...\n $ y: num [1:41] -237 -228 -220 -212 -204 ...\n\nbc$x[bc$y == max(bc$y)]\n\n[1] 0.2\n\n\nNote: these are approximate numbers so that it makes not much sense to use more than one decimal.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  },
  {
    "objectID": "slides/04-distributions.html#box-cox-transformation-iii",
    "href": "slides/04-distributions.html#box-cox-transformation-iii",
    "title": "04-Distributions",
    "section": "Box-Cox transformation III",
    "text": "Box-Cox transformation III\nIt is also possible to test for joint distribution of all groups at once by using explanatory variables (here group) on the right hand side of the model formula:\n\ndat &lt;- read.csv(\"https://tpetzoldt.github.io/datasets/data/prk_nit.csv\")\nboxcox(biovol ~ group, data=dat)\n\n\n\nFigure 1: Log-Likelihood profile of a Box-Cox-transformation for the pooled data sets Nit85 and Nit90.",
    "crumbs": [
      "Basic Statistics",
      "04-Distributions"
    ]
  }
]