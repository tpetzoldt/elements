---
subtitle: "Applied Statistics -- A Practical Course"
title: "05-Classical Tests"
date:   "`r Sys.Date()`"
--- 


```{r, echo=FALSE, include=FALSE}
library("exactRankTests")
```


# Hypotheses, errors and the p-value


<!--
ToDo: insert formulae and algorithms from the slides
-->


## Statistical test

<br>

A statistical hypothesis test is a method of statistical inference.

* Commonly, two samples are compared, or a sample is compared
  against properties from an idealized model.
* A hypothesis $H_a$ for the statistical relationship between the two data
  sets, is compared to an idealized null hypothesis H0 that proposes no
  relationship between two data sets.
* The comparison is considered statistically significant if the
  relationship between the data sets would be an unlikely realization of
  the null hypothesis according to a threshold probability – the
  significance level.


adapted from:
[https://en.wikipedia.org/wiki/Statistical hypothesis testing](https://en.wikipedia.org/wiki/Statistical hypothesis testing)

## Effect size and significance

<br>

In case of relative mean differences, the relative effect size is:

:::{.bigfont}
$$
  \delta = \frac{\bar{\mu}_1-\bar{\mu}_2}{\sigma}=\frac{\Delta}{\sigma}
$$
:::

with:

* mean values of two populations $\mu_1, \mu_2$
* effect size $\Delta$
* relative effect size $\delta$ (also called Cohen's d)
* **significance** means that an observed effect is unlikely 
  the result of pure random variation.


## Null hypothesis and alternative hypothesis

<br>

[**$H_0$**]{.blue} null hypothesis: two populations are not different
with respect to a certain property.

* Assumption: observed effect occured purely at random, true effect is zero.

[**$H_a$**]{.darkred} alternative hypothesis (experimental hypothesis): existence of a certain effect.

* An alternative hypothesis is never completely true or "proven". 
* Acceptance of $H_A$ means only than $H_0$ is unlikely.

**"Not significant" means either no effect or sample size too small!**

<br>

[Note: Different meaning of **significance** ($H_0$ unlikely) and **relevance**
(effect large enough to play a role in practice).]{.blue}

<!------------------------------------------------------------------------------
## Notes

Statistical tests can only test for differences between samples, not
for equality.  This means that $H_0$ is formulated to be rejected and that it
is impossible to test if two populations are equal -- for principal reasons. If a
test cannot reject "the null", it means only that an observed effect
can also be explained as a result of random variability or error and
that a potential effect was too small to be "significantly"
detected, given the available sample size.

**Important:** Not significant does not necessarily mean "no effect",
it means "no effect or sample size too small"!

Whether an effect is significant or not is determined by comparing the
**p-value** of a test with a pre-defined critical value, the
significance level $\alpha$ (or probability of error).  Here, the
p-value is an estimate for the probability that an observed effect occured at random, 
under the assumption that the null hypothesis is to be true.

------------------------------------------------------------------------------->

## The p-value

<br>

The interpretation of the p-value was often confused in the past, 
even in statistics textbooks, so it is good to refer to a clear definition:


:::{.bigfont}
> The p-value is defined as the probability of obtaining a result equal to or
'more extreme' than what was actually observed, when the null hypothesis is true. 
:::

<br>

* [https://en.wikipedia.org/wiki/P-value](https://en.wikipedia.org/wiki/P-value):

* @hubbard_alphabet_2004 Alphabet Soup: Blurring the Distinctions Between p’s and a’s in
Psychological Research, Theory Psychology 14(3),
295-327. DOI: [10.1177/0959354304043638](https://doi.org/10.1177/0959354304043638)


## Alpha and beta errors


| Reality          | Decision of the test | correct? | probability      |
|:----------------:|:--------------------:|:--------:|:----------------:|
| $H_0$ = true     |significant           | no       | $\alpha$-error   |
| $H_0$ = false    |not significant       | no       | $\beta$-error    |
| $H_0$ = true     |not significant       | yes      | $1-\alpha$       |
| $H_0$ = false    |significant           | yes      | $1-\beta$ (power)|
|                  |                      |          |                  |


1.$H_0$ falsely rejected (error of the first kind or $\alpha$-error)

  * we claim an effect, that does not exist, e.g. a drug with no effect
  
2.$H_0$ falsely retained (error of the second kind or $\beta$-error)

* typical case in small studies, where effect was not enough to detect existing effects

**Use in practice**

* common convention in environmental sciences: $\alpha=0.05$, must be set beforehand
* $\beta=f(\alpha, \text{effectsize}, \text{sample size}, \text{kind of test})$, should be $\le 0.2$

## Significance and relevance

<br>

Significance is not the only important. 
It is often more important to focus on effect size and relevance.

When evaluating significant effects, it is always necessary to look also at the 
effect size. While significance means that the null hypothesis $H_0$ is unlikely 
in a statistical sense, relevance means that the effect size is large enough to 
play a role in practice. This means that whether an effect can be relevant or not
depends on its effect size and the field of application.

Let's for example consider a vaccination. If a vaccine
had a significant effect in a clinical test, but protected only 10 out of 1000 
people, one would not consider this effect as relevant and not produce this vaccine.

On the other hand, even small effects can be relevant. So if a toxic substance 
would have an effect on 1 out of 1000 people to produce cancer, we would consider this
as relevant. To detect this as a significant effect would need an epidemiological
study with a large number of people. But as it is highly relevant, it is worth the effort.


## Take home messages

<br>

* A p-value measures the probability that a purely random effect
  would be equally or more extreme than an observed effect.
* "Significant" means that an observed result is unlikely to have occurred by chance alone.
* **"Not significant" means either "no effect" or "sample size too small".**
* Don't focus on p-values alone. Never forget to report also sample size, **effect 
  size** and relevance of your results.
* With large data sets, the p-value loses importance. It is then easy to find 
  significant effects, but it is often very small and not relevant in practice.
* The p-value is an important tool in classical statistics, but its abuse
  can lead to mis-interpretation.


# Differences between mean values


## One sample t-Test

<br>

* tests if a sample is from a population with given mean value $\mu$ 
* based on checking if the population mean $\mu$ is in the confidence interval of $\bar{x}$

1. Let's assume a sample of size with $n=10, \bar{x}=5.5, s=1$ and $\mu=5$. 
2. Estimate the 95% confidence interval of $\bar{x}$:


$$
CI = \bar{x} \pm t_{1-\alpha/2, n-1} \cdot s_{\bar{x}}
$$
with 
$$
s_{\bar{x}} = \frac{s}{\sqrt{n}} \qquad \text{(standard error)}
$$ 



Different ways of calculation shown at the next slides


## Remember: standard deviation and standard error

```{r sd-and-se, echo=FALSE, fig.height=2.5}
par(mar=c(4.1, 5.1, 1.1, 1.1), mfrow=c(1,2))
x<-seq(0,10, length=100)
plot(x, dnorm(x, mean=5.5, sd=1), col="blue", type="l", lwd=2, ylab="Density", las=1)
abline(v=5, col="red", lwd=2)


plot(x, dnorm(x, mean=5.5, sd=1/sqrt(5)), col="blue", type="l", lwd=2, ylab="Density", las=1)
abline(v=5, col="red", lwd=2)
```

<small>
Visualization of a one-sample t-test. Left: original distribution of the data measured by standard deviation, right: distribution of mean values, measured by its standard error.
</small>

$$
s_{\bar{x}} = \frac{s}{\sqrt{n}} \qquad \text{(standard error)}
$$ 

* standard error < standard deviation
* measures precision of the mean value
* CLT!

The test is based on the distribution of the means, not distribution of original data.


## Method 1: Is $\mu$ in the confidence interval?

<br>

1. Sample: $n=10, \bar{x}=5.5, s=1$ and $\mu=5$

2. Let $\alpha = 0.05$, we get a two-sided 95% confidence interval with:

$$\bar{x} \pm t_{0,975, n-1} \cdot \frac{s}{\sqrt{n}}$$

:::{.bigfont}
```{r, echo=TRUE}
5.5 + c(-1, 1) * qt(0.975, 10-1) * 1/sqrt(10)
```
:::

<br>

3. Check if  $\mu=5.0$ is in this interval? 

4. Yes, it **is** inside $\Rightarrow$ [difference not significant]{.blue}.


## Method 2: Comparison with a tabulated t-value

1. Rearrange the equation of the confidence interval, to calculate an observed $t_{obs}$

$$
t_{obs} = |\bar{x}-\mu | \cdot \frac{1}{s_{\bar{x}}} = \frac{|\bar{x}-\mu |}{s} \cdot \sqrt{n} = \frac{|5.5 -5.0|}{1.0} \cdot \sqrt{10}
$$

We can calculate this in **R**:

```{r, echo=TRUE}
t <- abs(5.5 - 5.0) / 1.0 * sqrt(10)
t
```

2. Compare $t_{obs}$ with a tabulated value

* "Old style": find critical t-value in a table for given $\alpha$ and degrees of freedom ($n-1$)
* For $\alpha=0.05$ and two-sided, this is: $t_{1-\alpha/2, n-1} = `r round(qt(0.975, 9), 2)`$. 

Comparison: $`r round(t, 2)` < `r round(qt(0.975, 9), 2)`$ $\Rightarrow$ no significant difference between $\bar{x}$ and $\mu$.


## Method 3: Calculation of the p-value from $t_{obs}$

<br>

* use computerized probability function (`pt`) instead of table lookup
* $t = t_{obs}$ and the degrees of freedom ($n-1$):

<br>


```{r, echo=TRUE}
2 * (1 - pt(t, df = 10 - 1)) # 2 * (1 - p) is re-arranged from 1-alpha/2
```


This p-value = `r 2 * (1 - pt(t, df=10-1))` is greater than $0.05$ so we consider 
the difference as not significant.

<br>

**FAQ: less than or greater than?**

<br>

|                |                                 |                          |             |
|----------------|---------------------------------|--------------------------|-------------|
| p-value        | $\text{p-value} < \alpha$       | null hypothesis unlikely | significant |
| test statistic | $t_{obs} > t_{1-\alpha/2, n-1}$ | effect exceeds confint.  | significant |
|                |                                 |                          |             |


## Method 4: Built-in t-test function in R

<br>

The same can be done much easier with the computer in **R**.

Let's assume we have a sample with $\bar{x}=5, s=1$:


```{r, echo=TRUE}
## define sample
x <- c(5.5, 3.5, 5.4, 5.3, 6, 7.2, 5.4, 6.3, 4.5, 5.9)

## perform one-sample t-test
t.test(x, mu=5)
```


The test returns the observed t-value, the 95% confidence interval and the p-value.

An important difference is, that this method needs the original data, while the other 
methods need only mean, standard deviation and sample size.


## Two sample t-test


<br>

The two-sample t-test compares two independent samples:


```{r, echo=TRUE}
x1 <- c(5.3, 6.0, 7.1, 6.4, 5.7, 4.9, 5.0, 4.6, 5.7, 4.0, 4.5, 6.5)
x2 <- c(5.8, 7.1, 5.8, 7.0, 6.7, 7.7, 9.2, 6.0, 7.2, 7.8, 7.8, 5.7)
t.test(x1, x2)
```

<br>

* $\rightarrow$ both samples differ significantly ($p < 0.05$)
* Note: **R** has not performed the "ordinary" t-test but the
Welch test (= heteroscedastic t-test)
* where variances of both samples don't need to be identical.

## Hypothesis and formula of the two-sample t-test

<br>

::: {.column width="49%"}

$H_0$ $\mu_1 = \mu_2$

$H_a$ the two means are different

**test criterion**

$$
 t_{obs} =\frac{|\bar{x}_1-\bar{x}_2|}{s_{tot}} \cdot \sqrt{\frac{n_1 n_2}{n_1+n_2}}
$$
:::

::: {.column width="49%"}

```{r twosample-t-test}
par(cex=1.4)
x<-seq(0,10, length=100)
plot(x, dnorm(x, mean=5, sd=1), col="red", type="l", lwd=2, ylab="Density", las=1)
lines(x, dnorm(x, mean=7, sd=1), col="blue", lwd=2)
```


:::

**pooled standard deviation**

$$
s_{tot} = \sqrt{{({n}_1 - 1)\cdot s_1^2 + ({n}_2 - 1)\cdot s_2^2
\over ({n}_1 + {n}_2 - 2)}}
$$

**assumptions:** independence, equal variances, approximate normal distribution

## The Welch test

<br>

**Known as t-test for samples with unequal variance, works also for equal variance!**

<br>

Test criterion:

$$
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{s^2_{\bar{x}_1} + s^2_{\bar{x}_2}}}
$$


Standard error of each sample:

$$
s_{\bar{x}_i} = \frac{s_i}{\sqrt{n_i}}
$$
Corrected degrees of freedom:

$$
\text{df} = \frac{\frac{s^2_1}{n_1} + \frac{s^2_2}{n_2}}{\frac{s^4_1}{n^2_1(n_1-1)} + \frac{s^4_2}{n^2_2(n_2-1)}}
$$

# Welch test in R

:::{.bigfont}
```{r f-test, echo=TRUE}
t.test(x1, x2)
```
:::

... is just the default method of the `t.test`-function.


## Equality of variance: F-test

::: {.column width="69%"}

$H_0$: $\sigma_1^2 = \sigma_2^2$

$H_a$: variances unequal

**Test criterion:**

$$F = \frac{s_1^2}{s_2^2} $$


- larger of the two variances in the enumerator $(s^2_1 >  s^2_2)$
- separate degrees of freedom ($n-1$)
:::

::: {.column width="29%"}
```{r var-homogeneity, fig.height=4, fig.width=4}
par(cex=1.4, mar=c(4,4,1,1))
x <- seq(0,10, length=100)
plot(x, dnorm(x, mean=5, sd=1), col="red", type="l", lwd=2, ylab="Density", las=1)
lines(x, dnorm(x, mean=5, sd=2), col="blue", lwd=2)
```
:::

**Example:**

- [$s_1=1$]{.red}, [$s_2 =2$]{.blue}, $n_1=5, n_2=10, F=\frac{2^2}{1^2}=4$
- deg. of freedom: $9 \atop 4$

[$\Rightarrow$ $F_{9, 4, \alpha=0.975} = `r round(qf(0.975, 9, 4),2)` > 4 \quad\rightarrow$ not significant]{.darkred}




## Homogeneity of variances with > 2 samples

<br>

::: {.column width="59%"}

```{r, echo=FALSE}
set.seed(123)
```

```{r}
x1 <- rnorm(10, 5,  1)
x2 <- rnorm(10, 5,  2)
x3 <- rnorm(10, 10, 1)
```

**Bartlett's test:**

```{r, echo=TRUE}
bartlett.test(list(x1, x2, x3))
```

<br>
**Fligner-Killeen test** (recommended):

```{r, echo=TRUE}
fligner.test(list(x1, x2, x3))
```
:::


::: {.column width="39%"}
```{r three-samples, fig.width=3, fig.height=4}
par(mar=c(3,3,1,1))
boxplot(x1, x2, x3, las=1, names=c("x1", "x2", "x3"), col="wheat")
```
:::

* tests are often used to check assumptions of the ANOVA

<!------------------------------------------------------------------------------
## Role of the F-test for the classical t-test procedure

The classical procedure would be as follows:

1. Perform a check for the identity of both variances with `var.test` beforehand:

```{r, echo=TRUE}
var.test(x1, x2)  # F-Test
```


2. if $p < 0.05$ $\Rightarrow$ variances are significantly different $\rightarrow$ Welch test (`var.equal=FALSE`).

3. if $p> 0.05$, $\Rightarrow$ assume equal variances $\rigtarrow$ "ordinary" t-test (`var.equal=TRUE`)

```{r, echo=TRUE}
t.test(x1, x2, var.equal = TRUE) # classical t-test
```
------------------------------------------------------------------------------->

## Recommendation for two sample t-tests

<br>

**Traditional procedure:**

1. Test for equal variances using the F-test: `var.test(x, y)`
2. If variances are equal: `t.test(x, y, var.equal=TRUE)`
3. otherwise, use `t.test(x, y)` (= Welch test)
4. Check if both samples follow a normal distribution.

<br>

**More modern recommendation:**

1. Don't use pre-tests!
2. Use always the Welch test: `t.test(x, y)`
3. Check approximate normal distribution by using box-plots. Not necessary if $n$ is large.

see @zimmerman_note_2004 or [Wikipedia](https://en.wikipedia.org/wiki/Welch%27s_t-test).



## Paired t-Test

* sometimes also called "t-test of dependent samples" 
  - the term "dependent" can be misleading, better "pairwise"
  - values within samples must still be independent
* examples: left arm / right arm; before / after  

* is essentially a one-sample t-test of pairwise differences against $\mu=0$  
* advantage: eliminates "covariate"

::: {.column width="49%"}

```{r, echo=TRUE}
x1 <- c(2, 3, 4, 5, 6)
x2 <- c(3, 4, 7, 6, 8)
t.test(x1, x2, var.equal=TRUE)
```
p=0.20, not significant
:::

::: {.column width="49%"}
```{r, echo=TRUE}
x1 <- c(2, 3, 4, 5, 6)
x2 <- c(3, 4, 7, 6, 8)
t.test(x1, x2, paired=TRUE)
```
p=0.016, significant
:::

It can be seen that the paired t-test has a greater discriminatory
power in this case.


## Mann-Whitney and Wilcoxon-test

<br>

* Non-parametric tests:
    * No assumptions about shape and parameters of distribution, but
    * distributions should be similar, otherwise test may be misleading.
* Mann-Whitney U-test and Wilcoxon-test are very similar.
* In a strict sense, "Wilcoxon" is the version for paired data

<br> 
**Basic principle:** Count of so-called "inversions" of ranks, where samples overlap

* Sample A: 1, 3, 4, 5, 7
* Sample B: 6, 8, 9, 10, 11
* Both samples ordered together: 1, 3, 4, 5, [6]{.red}, 7, [8]{.red}, [9]{.red}, [10]{.red}, [11]{.red}
* Inversions: $\rightarrow$ $U = 1$


## Mann-Whitney test procedure in practice

<br>

1. Assign ranks $R_A$ and $R_B$ to both samples $A$, and $B$ with sample size $m$ and $n$.
2. Calculate number of inversions $U$:
  
\begin{align*}
     U_A &= m \cdot n + \frac{m (m + 1)}{2} - \sum_{i=1}^m R_A \\
     U_B &= m \cdot n + \frac{n (n + 1)}{2} - \sum_{i=1}^n R_B \\
     U   &= \min(U_A, U_B)
\end{align*}

* Critical values of $U$ can be found in common statistics text books.
* Not necessary in **R**, p-value directly printed.
* **Note:** Use special version `wilcox.exact` with correction if sample has ties.


## Mann-Whitney - Wilcoxon-test in R

<br>


```{r, echo=TRUE}
A <- c(1, 3, 4, 5, 7)
B <- c(6, 8, 9, 10, 11)

wilcox.test(A, B) # use optional argument `paired = TRUE` for paired data.
```

<br>


**Mann-Whitney - Wilcoxon-test with tie correction**

* applied if the rank differences contain doubled values

```{r, echo=TRUE}
A <- c(1, 3, 4, 5, 7)
B <- c(6, 8, 9, 10, 11)
```


```{r, echo=TRUE}
library("exactRankTests")
wilcox.exact(A, B, paired=TRUE)
```


## Permutation methods

* Basic principle: Estimation of a test statistic $\xi_{obs}$ from sample,
* Resampling: Simulate many $\xi_{i, sim}$ from randomly permuted data set ($n = 999$ or more)
* Where does $\xi_{est}$ appear within the ordered series of simulated values $\xi_{i, sim}$?

```{r permutation-test, fig.align='center'}
  par(mar=c(4.1, 5.1, 1.1, 1.1))
  set.seed(123)
  x <- rgamma(100, 2, 1)
  plot(seq(0,1, length=100), sort(x), xlab="p", ylab=expression(xi[i]),
  pch=16, cex=0.5, las=1, col="blue")
  xx <- 4.5
  qxx <- approx(sort(x), seq(0,1, length=100), xx)$y
  arrows(0, xx, qxx, xx, col="red")
  arrows(qxx, xx, qxx, 0, col="red")
  points(qxx, xx, pch=16, col="red")
```

Let $\xi_{obs}$ be $`r xx`$ in our example, then $\Rightarrow$ $p= `r round(qxx,2)`$.

# Test for distributions


## Testing for distributions

**Nominal variables**

*  $\chi^2$-test
*  Fisher's exact test

**Ordinal variables**

*  Cramér-von-Mises-Test
* $\rightarrow$ more powerful than $\chi^2$ or KS-test

**Metric scales**

*  Kolmogorov-Smirnov-Test (KS-test)
*  Shapiro-Wilks-Test (for normal distribution)

## Contingency tables for nominal variables


* used for nominal (i.e. categorical or qualitative) data
* examples: eye and hair color, medical treatment and the number of cured/not cured
* important: use **absolute mesurements** (true numbers!), not percentages
    or other calculated data (e.g. not something like biomass per area)

**Data example**

* *Daphnia* (water flee)-clones and their preferred depth in a lake
* food algae in the deep water zone, poor of oxygen
* only adapted clones with high haemoglobin can dive into deep water


| Clone | Upper layer | Deep layer |
|-----:|:------------:|:----------:|
A      |          50  |          87| 
B      |          37  |          78|
C      |          72  |          45|


## Calculation of the $\chi^2$-test 


1. Observed frequencies $O_{ij}$ and sums of rows and columns:


|            | Clone A | Clone B | Clone C | Sum $s_i$|
|------------|---------|---------|---------|----------|
|Epilimnion  |   50    |    37   |   72    | 159      |
|Hypolimnion |   87    |    78   |   45    | 210      |
|Sum {$s_j$} |  137    |   115   |  117    | $n=369$  |
|            |         |         |         |          |

2. Expected frequencies $E_{ij}$ under the $H_0$: ${\mathbf E = s_i \otimes s_j} / n$

|            | Clone A | Clone B   | Clone C |
|------------|---------|-----------|---------|
|Epilimnion  |   59.0  |    49.6   |   50.4  |
|Hypolimnion |   78.0  |    65.4   |   66.6  |
|            |         |           |         |

3. Calculate $\hat{\chi}^2 = \sum_{i, j} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$
  with $(n_{row} - 1) \cdot (n_{col} - 1)$ degrees of freedom.

* Compare with critical $\chi^2$ from table.
* Note: The results are only reliable if all observed frequencies are $\geq 5$.
* For smaller samples, use Fisher's exact test


## The $\chi^2$-test in **R**

<br>

Organize data in a matrix with 3 rows (for the clones) and 2 columns (for the depths):

:::{.bigfont}
```{r, echo=TRUE}
x <- matrix(c(50, 37, 72, 87, 78, 45), ncol=2)
x
```
:::


:::{.bigfont}
```{r, echo=TRUE}
chisq.test(x)
```
:::

## Fisher's exact test

<br>

:::{.bigfont}
```{r, echo=TRUE}
x <- matrix(c(50, 37, 72, 87, 78, 45), ncol=2)
x
```

```{r, echo=TRUE}
fisher.test(x)
```
:::

<br>

$\rightarrow$ significant correlation between the clones and their location.

* dependency between clones and vertical distribution in the lake


## Favorite numbers of HSE students

```{r favorite-numbers}
x <- c(1,1,6,2,2,5,8,6,3)
barplot(x, names.arg=1:9, las=1, ylim=c(0,10))
text((1:9)*1.2 - 0.5, x+0.5, x, cex=1.2)
abline(h=mean(x), lty="dashed", lwd=2,col="red")
```

*  Numbers from 1..9, $n=34$
* $H_0$: equal probability of all numbers $1/9$ (discrete uniform distribution)
* $H_A$: some numbers are favored $\rightarrow$ departure from discrete uniform

## Chisquare test

<br>

:::{.bigfont}
```{r, echo=TRUE}
obsfreq <- c(1, 1, 6, 2, 2, 5, 8, 6, 3)
chisq.test(obsfreq)
chisq.test(obsfreq, simulate.p.value=TRUE, B=1000)
```
:::

<br>

*  **one-sample** $\chi^2$-test. It tests for equality of frequency in all classes.
*  The simulation-based version of the test (with 1000 replicates) is slightly more precise than
the standard $\chi^2$-test, but both ar not significant.

## Cramér-von-Mises-Test


```{r von-mises, fig=TRUE,echo=FALSE,width=6,height=4}
library(dgof)
par(mar=c(4,5,0.5,0.5)+.1)
obsfreq <- c(1, 1, 6, 2, 2, 5, 8, 6, 3)
x <- rep(1:length(obsfreq), obsfreq)
cdf <- stepfun(1:9, cumsum(c(0, rep(1/9, 9))))
n <- length(x)
plot(cdf, las=1, main="", col="red")
points(sort(x), (1:n)/n, col="blue", pch=16, cex=0.8)
lines(c(0, sort(x)), c(0, (1:n)/n), type="s", col="blue", lty="dashed")
#lines(c(0, sort(x)), c(0, (2*(1:n)+1)/(2*n)), type="s", col="blue", lty="dashed")
legend(0.5, 0.9, col=c("red", "blue"), lty=c("solid", "dashed"), legend=c("theoretical", "observed"), box.lty=0)
```


$$
T = n \omega^2 = \frac{1}{12n} + \sum_{i=1}^n \left[ \frac{2i-1}{2n}-F(x_i) \right]^2
$$

## Cramér-von-Mises-Test in **R**

```{r, echo=TRUE}
library(dgof)
obsfreq <- c(1, 1, 6, 2, 2, 5, 8, 6, 3)

## CvM-test needs individual values, not class frequencies
x <- rep(1:length(obsfreq), obsfreq)
x
```

<br>
```{r, echo=TRUE}
## create a cumulative function with equal probability of all cases
cdf <- stepfun(1:9, cumsum(c(0, rep(1/9, 9))))
cdf <- ecdf(1:9)

## perform the test
cvm.test(x, cdf)
```

*  The Cramér-von-Mises-test works with the original, unbinned values
*  Use of cumulative function [respects order of classes]{.blue} $\rightarrow$ more powerful, than $\chi^2$-test.

## Test distribution type of metric variables

```{r normality-graphical,fig.align='center'}
set.seed(123)
par(mfrow=c(1, 3))
x <- rnorm(100, mean=50, sd=10)
hist(x, probability=TRUE)
lines(10:90, dnorm(10:90, mean(x), sd(x)), col="red", lwd=2)
qqnorm(x); qqline(x, col="red")
boxplot(x, col="wheat")
```

<br>

* histogram, boxplot, quantile-quantile plot
* Shapiro Wilk W-test
* Box-Cox method
* see section **Distributions**


# Dependency and correlation

## Correlation

<br>

**Frequencies of nominal variables**

* $\chi^2$-test
* Fisher’s exact test

⇒ dependence between plant society and soil type

[(see before)]{.gray}

**Ordinal variables**

* Spearman-Correlation

$\rightarrow$ rank numbers

**Metric scales**

* Pearson-correlation
* Spearman-correlation


## Variance and Covariance

<br>

::: {.column width="59%"}
**Variance**

* measures variation of a single variable


$$
  s^2_x = \frac{\text{sum of squares}}{\text{degrees of freedom}}=\frac{\sum_{i=1}^n (x_i-\bar{x})^2}{n-1}
$$

**Covariance**

* measures how two variables change together

$$
  q_{x,y} = \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{n-1}
$$
  
Correlation: scaled to $(-1, +1)$

$$
  r_{x,y} = \frac{q_{x,y}}{s_x \cdot s_y}
$$

:::

::: {.column width="39%"}

```{r var-covar, fig.width=3, fig.height=6}
library(mvtnorm)
set.seed(123)
par(mfrow=c(2, 1), mar=c(4,4,1,1))
boxplot(rnorm(10), names="x")
xy <- rmvnorm(n=100, mean=c(5,5), sigma=matrix(c(1,0.8,0.8,1), ncol=2))
plot(xy, pch=16, xlab="x", ylab="y")
```

:::

## Correlation coefficient after Pearson

<br>

::: {.column width="69%"}

* the usual correlation coefficient that we all know
* tests for [linear dependence]{.blue}

$$
r_p=\frac{\sum{(x_i-\bar{x})  (y_i-\bar{y})}}
       {\sqrt{\sum(x_i-\bar{x})^2\sum(y_i-\bar{y})^2}}
$$

**Or:**
  
$$
r_p=\frac {\sum xy - \sum y \sum y / n}
        {\sqrt{(\sum x^2-(\sum x)^2/n)(\sum y^2-(\sum y)^2/n)}}
$$
<br>

Range of values: $-1 \le r_p \le +1$

|                     |                                            |
|---------------------|--------------------------------------------|  
|$0$                  | no interdependence                         |
|$+1 \,\text{or}\,-1$ | strictly positive resp. negative dependence|
|$0 < |r_p| < 1$      | positive resp. negative dependence         |
|                     |                                            |
:::


::: {.column width="29%"}
```{r pearson1, fig.width=4, fig.height=4}
par(mar=c(4,4,1,1))
plot(xy, pch=16, xlab="x", ylab="y")
```
:::


## Which size of correlation indicates dependency?

::: {.column width="49%"}  

```{r pearson2, fig.width=5,fig.height=5}
library("mvtnorm")
par(mar=c(4.1, 5.1, 3.1, 1.1))
set.seed(1235)
x1 <- rmvnorm(n=50, mean=c(5,5), sigma=matrix(c(1,0.5,0.5,1), ncol=2))
plot(x1, xlab="x", ylab="y", las=1, xlim=c(0,10), ylim=c(0,10),
  main="n=50", pch=16)
```

$r=`r round(cor(x1[,1], x1[,2]), 2)`, \quad p=`r round(cor.test(x1[,1], x1[,2])$p.value, 4)`$
:::
  
::: {.column width="49%"}  
```{r pearson3, fig.width=5,fig.height=5}
par(mar=c(4.1, 5.1, 3.1, 1.1))
set.seed(345)
x2 <- rmvnorm(n=5, mean=c(5,5), sigma=matrix(c(1,0.7,0.7,1), ncol=2))
plot(x2, xlab="x", ylab="y", las=1, xlim=c(2,8), ylim=c(2,8),
main="n=5", pch=16)
```
$r=`r round(cor(x2[,1], x2[,2]), 2)`, \quad p=`r round(cor.test(x2[,1], x2[,2])$p.value, 2)`$

:::

## Significant correlation?

$$
\hat{t}_{\alpha/2;n-2} =\frac{|r_p|\sqrt{n-2}}{\sqrt{1-r^2_p}}
$$


$t=0.829 \cdot \sqrt{1000-2}/\sqrt{1-0.829^2}=46.86, df=998$

<br>
**Quick test: critical values for $r_p$**

|     |     |         |          |
|----:|----:|--------:|---------:|
|$n$  | d.f.|  $t$    |$r_{crit}$|
|3    |1    |12.706   |0.997     |
|5    |3    |3.182    |0.878     |
|10   |8    |2.306    |0.633     |
|20   |18   |2.101    |0.445     |
|50   |48   |2.011    |0.280     |
|100  |98   |1.984    |0.197     |
|1000 |998  |1.962    |0.062     |
|     |     |         |          |

## Rank-correlation according to Spearman

<br>

* measures **monotonous** (and not necessarily linear) dependence
* estimation from rank differences:

$$
r_s=1-\frac{6 \sum d^2_i}{n(n^2-1)}
$$

* or, alternatively: Pearson-correlation of ranked data (necessary in case of ties).
* Test: for $n < 10$ $\rightarrow$ table of critical values

for $10 \leq n$  $\rightarrow$ $t$-distribution

$$
   \hat{t}_{1-\frac{\alpha}{2};n-2}
      =\frac{|r_s|}{\sqrt{1-r^2_S}} \sqrt{n-2}
$$

::: aside
Computer statistics packages use a special algorithm (algorithm AS 89 according to Best and Roberts, 1975).
:::

## Example

<br>

|$x$ | $y$   | $R_x$ | $R_y$ | $d$ | $d^2$|
|----|------:|-------|-------|-----|------|
|1   | 2.7   |     1 | 1     | 0   | 0    |
|2   | 7.4   |     2 | 2     | 0   | 0    |
|3   | 20.1  |     3 | 3     | 0   | 0    |
|4   | 500.0 |     4 | 5     | -1  | 1    |
|5   | 148.4 |     5 | 4     | +1  | 1    |
|    |       |       |       |     | 2    |
|    |       |       |       |     |      |


$$
r_s=1-\frac{6 \cdot 2}{5\cdot (25-1)}=1-\frac{12}{120}=0.9
$$

For comparison: $r_p=0.58$

## Application of Spearman's-$r_s$

<br>

**Advantages**

* distribution free (does not require normal distribution),
* detects any \emph{monotonous} dependence,
* not much affected by outliers.

**Disadvantages:**

* certain information loss due to ranking,
* no information about type of dependency,
* no direct relationship to coefficient of determination.

Conclusion: $r_s$ is nevertheless highly recommended!


## Correlation coefficients in **R**

* Pearson's product-moment correlation coefficient
* Spearman's rank correlation coefficient

```{r, echo=TRUE}
x <- c(1, 2, 3, 5, 7,  9)
y <- c(3, 2, 5, 6, 8, 11)
cor.test(x, y, method="pearson")
```


If linearity or normality of residuals is doubtful, use a rank correlation

```{r, echo=TRUE}
cor.test(x, y, method="spearman")
```


## Problematic cases

```{r cor-violations, fig.width=6,fig.height=4}
par(mfrow=c(2,2))
par(mar=c(4.1, 5.1, 1.1, 1.1))
x <- exp(rmvnorm(n=100, mean=c(5,5), sigma=matrix(c(1,0.8,0.8,1), ncol=2)))
plot(x, xlab="x", ylab="y", las=1, pch=16, cex=0.5)

x <- seq(1, 10, length=100)
y <- exp(0.3*x) + rnorm(x, mean=5, sd=1)
plot(x, y, xlab="x", ylab="y", las=1, pch=16, cex=0.5)

x <- rmvnorm(n=100, mean=c(5,5), sigma=matrix(c(1,0.8,0.8,1), ncol=2))
x[,2] <- exp(x[,2])
plot(x, xlab="x", ylab="y", las=1, pch=16, cex=0.5)

x <- rmvnorm(n=20, mean=c(5,5), sigma=matrix(c(0.3,0.0,0.0,0.3), ncol=2))
x[1,] <- c(8,8)
plot(x, xlab="x", ylab="y", las=1, pch=16, cex=0.5, xlim=c(0,10), ylim=c(0,10))
```


## Outlook: More than two independent variables

<br>

**Multiple correlation**
  
* [Example:]{.blue} Chl-a=$f(x_1, x_2, x_3, \dots)$, where $x_i$ =
      biomass of the $i$th phytoplankton species.
* multiple correlation coefficient
* partial correlation coefficient
* attractive method $\leftrightarrow$ but difficult in practice:
    * "independent" variables may correlate with each other (multi-collinearity)<br>
        $\Rightarrow$ bias of the multiple $r$.
    * non-linearities are even more difficult to handle than in the two-sample case.

**Recommendation:**

* Use multivariate methods (NMDS, PCA, ...) for a first overview,
* apply multiple regression with care and use process knowledge.


# Power Analysis

## Determining the power of statistical tests

<br>

**How many replicates will I need?**

* Depends on:
    * the relative effect size $\frac{\mathrm{effect}}{\mathrm{standard ~ deviation}}$
    
    $$\delta=\frac{(\bar{x}_1-\bar{x}_2)}{s}$$
    
    * the sample size $n$
    * and the pre-defined significance level $\alpha$
    * and the applied method


* The smaller $\alpha$, $n$ and $\delta$, the bigger the type II ($\beta$) error.
* This is the probability to overlook effects despite of their existence.


## Power analysis

<br>

Formula for minimum sample size in the one-sample case:


$$
n = \bigg(\frac{z_\alpha + z_{1-\beta}}{\delta}\bigg)^2
$$

* $z$: the quantiles (`qnorm`) of the standard normal
distribution for $\alpha$ and for $1-\beta$ 
* $\delta=\Delta / s$: relative effect size.

**Example**

Two-tailed test with $\alpha=0.025$ and $\beta=0.2$

$\rightarrow$ $z_\alpha = 1.96$, $z_\beta=0.84$, then:


$$
n= (1.96 \pm 0.84)^2 \cdot 1/\delta^2 \approx 8 /\delta^2
$$


## Power of the t-test


The power of a t-test, or the minimum sample size, can be calculated with:
`power.t.test()`:

```{r, echo=TRUE}
power.t.test(n=5, delta=0.5, sig.level=0.05)
```

$\rightarrow$ power = 0.10

* For $n=5$ an existing effect of $0.5\sigma$ is only detected in 1 out of 10 cases.
* For a power of 80% at $n=5$ we need an effect size of at least $2\sigma$:

```{r, echo=TRUE,eval=FALSE}
power.t.test(n=5, power=0.8, sig.level=0.05)
```

For a weak effect of $0.5\sigma$ we need a sample size of $n\ge64$ in each group:

```{r, echo=TRUE,eval=FALSE}
power.t.test(delta=0.5,power=0.8,sig.level=0.05)
```

[$\Rightarrow$ we ned either a large sample size or a strong effect.]{.darkred}

## Simulated power of a t-test 

```{r, echo=TRUE, eval=FALSE}
# population parameters
n      <- 10
xmean1 <- 50
xmean2 <- 55
xsd1   <- xsd2 <- 10
alpha  <- 0.05

# number of test runs in the simulation
nn <- 1000
a <- b <- 0
for (i in 1:nn) {
  # creating random numbers
  x1 <- rnorm(n, xmean1, xsd1)
  x2 <- rnorm(n, xmean2, xsd2)
  # results of the t-test
  p <- t.test(x1,x2,var.equal = TRUE)$p.value 
  if (p < alpha) {
     a <- a+1
   } else {
     b <- b+1
  }
}
print(paste("a=", a, ", b=", b, ", a/n=", a/nn, ", b/n=", b/nn))
```

## References

