---
subtitle: "Applied Statistics -- A Practical Course"
title: "09-Time Series Basics"
date:   "`r Sys.Date()`"
--- 
```{r packages, echo=FALSE, include=FALSE}
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)
```


## An introductory example

```{r co2-maunaloa-show, eval=FALSE, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)

co2 <- read_csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv", 
                skip = 40, show_col_types = FALSE)

co2 |> 
  mutate(date=as_date(paste(year, month, 15, sep="."))) |>
  ggplot(aes(date, average)) + 
  geom_line() + ylab(expression(CO[2]~"in the atmosphere (ppm)")) + 
  ggtitle("Mauna Loa Observatory, Hawaii, Monthly Averages") +
  geom_smooth()
```

```{r co2-maunaloa, echo=FALSE, fig.align='center'}
#co2 <- read_csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv", skip=40) |>
co2 <- read_csv("../data/co2_mm_mlo.csv", skip=40, show_col_types = FALSE) |>
  mutate(date=as_date(paste(year, month, 15, sep=".")))

co2 |> ggplot(aes(date, average)) + 
  geom_line() + ylab(expression(CO[2]~"in the atmosphere (ppm)")) + 
  ggtitle("Mauna Loa Observatory, Hawaii, Monthly Averages") +
  geom_smooth()
```


Data source:

@keeling_exchanges_2001, @tans_maunaloa_2023, [https://gml.noaa.gov/ccgg/trends/data.html](https://gml.noaa.gov/ccgg/trends/data.html)

<!--
https://gml.noaa.gov/ccgg/trends/data.html
License of the data: CC BY 4.0 (see https://scrippsco2.ucsd.edu/data/atmospheric_co2/primary_mlo_co2_record.html)
-->

## Time series analysis

<br>

* Deals with development of processes in time: $x(t)$,
* Huge number of specific approaches, only a few examples can be
  presented here.


**Aims of time series analysis**

* Does a trend exist? (significance)
* How strong is a trend? (effect size)
* Identification of covariates. (influencing factors)
* Trend, seasonality and random error. (component model)
* Statistical modeling and forecasting.
* Breakpoint analysis, intervention analysis, ..., and more.



## Stationarity

<br>

* Time series methods have certain assumptions.
* One of the most common assumptions is **stationarity**.

<br>

:::{.bigfont}
**Note:** we are often not primarily interested in stationarity itself!

$\rightarrow$ It is often just "the ticket" to go further.
:::
<br>


**Example 1: trend analysis**

We test stationarity to check if a trend test would be appropriate.

**Example 2: linear models**

Estimation of a linear trend or a breakpoint model. We check residuals for stationarity
to get in formation about reliability of the fitted model.

## Stationarity

```{r stat-nonstant, fig.align='center'}
  set.seed(123)
  par(mfrow=c(1, 2), cex=1.4)
  plot(rnorm(100), type="l", main="stationary", xlab="time", ylab="x")
  plot(rnorm(100, sd=seq(0.1,1,length=100)), type="l",
    main="non-stationary", xlab="time", ylab="x")
```

... is a central concept in time series analysis.

* Strictly (or strong) stationary process: distribution of
  $(x_{s+t})$ independent from index $s$.
* Weakly or wide-sense stationary processes: requires only that
  1st and 2nd moments (mean, variance and covariance) do not vary with
  time.

## Two different basic types time series

<br>

\begin{align}
x_t &= \beta_0 + \beta_1 t + u_t \label{ts:trendstat} \\
x_t &= x_{t-1} + c + u_t          \label{ts:diffstat}
\end{align}

<br>

```{r}
set.seed(1237)
```



* **TSP:** trend stationary process, linear regression model

```{r, echo=TRUE}
time <- 1:100
TSP <- 2 + 0.2 * time + rnorm(time)
```


* **DSP:** difference stationary process (random walk)


```{r, echo=TRUE}
DSP <- numeric(length(time))
DSP[1] <- rnorm(1)
for (tt in time[-1])
  DSP[tt] <- DSP[tt-1] + 0.2 + rnorm(1)
```


## Time series classes in R

* `ts` for regularly spaced time series,
* `zoo` irregularly spaced time series.


```{r ts-example1, echo=TRUE, fig.height=5, fig.width=12, fig.align='center'}
par(mfrow=c(1, 2), las=1)
TSP <- ts(TSP)
DSP <- ts(DSP)

plot(TSP, main="trend stationary process")
plot(DSP, main="difference stationary process")
```


## A Simulation Experiment

<br>

* Define two functions to generate time series with specific properties:


```{r, echo=TRUE}
genTSP <- function(time, beta0, beta1)
  as.ts(beta0 + beta1 * time + rnorm(time))

genDSP <- function(time, c) {
  DSP <- numeric(length(time))
  DSP[1] <- rnorm(1)
  for (tt in time[-1]) DSP[tt] <- DSP[tt-1] + c + rnorm(1)
  as.ts(DSP)
}
```


## Run the experiment

* set the trend to zero, so  `count.signif` counts `a` false positive results.

```{r, echo=TRUE}
count.signif <- function(N, time, FUN, ...) {
  a <- 0
  for (i in 1:N) {
    x <- FUN(time, ...)
    m <- summary(lm(x  ~ time(x)))
    f <- m$fstatistic
    p.value <- pf(f[1], f[2], f[3], lower=FALSE)
    # cat("p.value", p.value, "\n")
    if (p.value < 0.05) a <- a + 1
  }
  a
}
```

* test for the number of significant F-values,
* run loop 100 ... 1000  times for both types of time series,

```{r, echo=TRUE, fig.align='center'}
Nruns <- 100 # better 1000 !!!
count.signif(N=Nruns, time=time, FUN=genTSP, beta0=0, beta1=0) / Nruns
count.signif(N=Nruns, time=time, FUN=genDSP, c=0) / Nruns
```

## Autocorrelation function (ACF)

* ... correlation between time series and its time-shifted (= lagged) versions,
* the time shift (lag) is being varied $\rightarrow$ correlogram.
* indirect dependency, e.g. $x_{t}$ on $x_{t-2}$ because $x_{t-1}$ depends on $x_{t-2}$.
* without indirect effects it is called **partial autocorrelation**, PACF.
* Cross-correlation (CCF): **two** variables; lag is positive or negative.
* Example: CCF between solar radiation and water temperature.

```{r acf-example, fig.align='center'}
par(mfrow=c(1,2), cex=1.4)
acf(TSP)
pacf(TSP)
```

## Typical patterns of ACF and PACF

```{r acfpatterns, echo=FALSE, eval=TRUE, fig.width=12, fig.height=6, fig.align='center'}
par(mfrow=c(2,3), cex=1)
acf(TSP)
acf(diff(TSP))
acf(residuals(lm(TSP ~ time(TSP))))
acf(DSP)
acf(diff(DSP))
acf(residuals(lm(DSP ~ time(DSP))))
```

* ACF plots of TSP and DSP (left) very similar,
* Differentiated time series (middle),
* series with substracted trend (right).


## Typical patterns of ACF and PACF (Code)

<br>


```{r acfpatterns, echo=TRUE, eval=FALSE}
```




## Unit root test

<br>

* determine whether a time series is of type "DSP".
* ADF-test (augmented Dickey-Fuller test), contained in R-package **tseries**:
* Remember: A "TSP" can be made stationary by subtracting a trend.

$\rightarrow$ This is done automatically by the test.



```{r, echo=TRUE}
library(tseries)
adf.test(TSP)
adf.test(DSP)
```


## DSP: presence of unit root

<br>

* DSP series: presence of an unit root cannot be rejected,
* $\rightarrow$ non-stationary.
* But, after differencing, there are no objections against stationarity:

```{r, echo=TRUE}
adf.test(diff(DSP))
```


## KPSS test: Kwiatkowski-Phillips-Schmidt-Shin test

<br>

... tests directly for stationarity or trend stationarity:


```{r, echo=TRUE}
kpss.test(TSP)                # non-stationary
kpss.test(TSP, null="Trend")  # stationary after trend removal
```

<br>

* detrending made the series stationary

## KPSS test: Kwiatkowski-Phillips-Schmidt-Shin test II

<br>


```{r, echo=TRUE}
kpss.test(DSP)                # non-stationary
kpss.test(DSP, null="Trend")  # still non-stationary
```

<br>

* detrending does not cure non-stationarity

## Trend Tests

<br>

* **Mann-Kendall trend test**, popular in environmental sciences
* strictly suitable only for trend-stationary time series, but robust in cases of weak autocorrelation
* not for "pure" difference-stationary time series, because residuals are autocorrelated

* $\Rightarrow$ Test stationarity (or autocorrelation) before testing trend !!!

```{r, echo=TRUE}
library("Kendall")
MannKendall(TSP)
```

Significant trend. 
<br>

```{r, echo=TRUE}
MannKendall(DSP)
```


Indicates significant trend, but as process is DSP, interpretation is wrong.

## References