---
title: "Multivariate methods"
author: "Thomas Petzoldt"
date:   "`r Sys.Date()`"
---

```{r setup, include=FALSE}
## this code chunk sets some technical details, it is not shown to the user
library("readxl")
library("vegan")
library("knitr")
library("kableExtra")
library("leaflet")
library("rgl")
library("vegan3d")
library("scatterplot3d")
#knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
knitr::opts_chunk$set(echo = TRUE, eval=TRUE)
setupKnitr(autoprint = TRUE) # for embedded RGL graphics
```

## Data sets and terms of use

<br>

1. The "UBA-lakes" data set originates from the public data repository of the German 
Umweltbundesamt [@UBA2020]. The data set provided can be used freely according
to the [terms and conditions](https://www.umweltbundesamt.de/datenschutz-haftung#c-urheber-und-kennzeichenrecht) 
published at the [UBA web site](https://www.umweltbundesamt.de/), that refer to
§ 12a EGovG with respect of the data, and to the [Creative Commons CC-BY ND International License 4.0 ](https://creativecommons.org/licenses/by-nd/4.0/deed.de) with respect to other objects directly created by UBA.

2. The "bm-lakes" data set is a teaching data set, derived from historical measurements 
of lakes in Brandenburg and Mecklenburg. The data werde taken from the literature 
[@casper_lake_1985, @koschel_primary_1985] and adapted to teaching purposes.

3. The "gauernitz" data set contains simplified teaching versions from research data, of the study from @winkelmann_fish_2011

4. The document itself, the codes and the ebedded images are own work and can be shared according to [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/deed.en).


## An introductory example

![](../img/uba-lakes.png)

## Correlation between all variables?

```{r}
library("readxl") # read Excel files directly
lakes <- as.data.frame(
  read_excel("../data/uba/3_tab_kenndaten-ausgew-seen-d_2021-04-08.xlsx", sheet="Tabelle1", skip=3)
)
names(lakes) <- c("name", "state", "drainage", "population", "altitude", 
                  "z_mean", "z_max", "t_ret", "volume", "area", "shore_length", 
                  "shore_devel", "drain_ratio", "wfd_type")
str(lakes)
```

* `names(lakes)` replaces the original German column names by abbreviated English abbreviations

## Create pairwise scatter plots for all variables?

```{r lakes-all-variables}
plot(lakes)
```

## Create pairwise scatter plots for all variables?

<br>

* not a good idea, 14 variables would produce 182 (or 91) plots

* can lead to "statistical fishing"

* we need methods to extract the main information with a small number of plots



## The multivariate approach

<br>


[Work with the complete original variables directly]{.bigfont}

<br>

**$\rightarrow$ Multivariate statistics**


 * dependent variables [+ explanation variables optional]{.gray}
 * analyze distance and location in multidimensional space
 * find way to vizualize relationship in lower dimensions

<br>

For comparison

* univariate: 1 variable
* bivariate: 1 dependent, 1 independent variable
* multiple: 1 dependent, >1 independent variables

## Two approches of multivariate statistics

```{r echo=FALSE}
benthos <- read.csv("../data/gauernitz.csv") 
species <- benthos[c("Mollusca", "Diptera", "Baetis", "Plecoptera", 
                     "Coleoptera", "Turbellaria", "Heptageniidae",
                     "Ephemeroptera", "Gammarus", "Trichoptera", 
                     "Acari", "Nematoda", "Oligochaeta")]
```


:::{.column width="46%" .add-space}

**Ordination**

```{r approach-pca, echo=FALSE, fig.width=5, fig.height=5}
par(mar=c(4,4,1,1))
biplot(prcomp(scale(species)))
```

* dimension reduction
* relate observations and variables
:::


::: {.column width="46%"}

**Cluster analysis**

```{r approach-cluster, echo=FALSE, fig.width=5, fig.height=5}
par(mar=c(1,4,1,1))
rownames(species) <- benthos$Site
plot(hclust(dist(scale(species))), hang=-1, main="")
```

* distance and similarity
* identification of groups

:::


## Basic concepts

<br>

**Similarity and correlation**

* distance and similarity:<br>
  $\rightarrow$ How different or similar are the observations?
* correlation and covariance:<br>
  $\rightarrow$  Are variables interdependent?
* dimension reduction:<br>
  $\rightarrow$ Try to show essential parts of information on a lower number of dimensions.
* cluster analysis:<br>
  $\rightarrow$  Show which observations are closely together.
* ordination:<br>
  $\rightarrow$  Plot data at lower dimensions, similar observations closely together.



# The UBA-lakes example

## A data set from German lakes

```{r lakes-leaflet, echo=FALSE, fig.height=7, fig.width=10, fig.align='center'}
library("leaflet")

lakepos <- read.csv("../data/uba/kenndaten-seen-gps.csv")

leaflet(options = leafletOptions(zoomControl = TRUE)) |>
  addTiles() |>
  setView(lng = 10.447, lat = 51.16, zoom = 5) |>
  addMarkers(lng = lakepos$lon, lat = lakepos$lat,
             popup = lakepos$name,
             options = markerOptions(title=lakepos$name, closeButton = FALSE))
```


## Example: A simplified subset from the UBA lake data


```{r read-lakes-combined, echo=FALSE}
lakes <- read.csv(file="../data/uba/lakes-combined-data.csv")
valid_columns <- c(
  "name", "shortname",
  #"drainage", 
  #"population", 
  #"altitude", 
  "z_mean",
  "z_max", 
  "t_ret", 
  "volume", 
  "area", 
  #"shore_length", 
  #"shore_devel", 
  #"drain_ratio", 
  "p_tot", 
  "n_no3", 
  "chl",
  "wfd_type"
)

## less columns, so that we get a simplified subset and more complete cases
lakes <- lakes[valid_columns] |> na.omit()
row.names(lakes) <- lakes$shortname

lakes_saved <- lakes # backup copy of the data set for later use

lake_ids <- lakes[c("name", "shortname")]
lakedata <- lakes[, -c(1, 2)]
lakedata$wfd_type <- NULL
```

<small>
```{r, echo=FALSE}
## print table
kable(lakes_saved)
```
mean and maximum depth (m): z_mean, z_max; retention time (years): t_ret;
volume (10^9 m^3); area (km^2), total phosphorus P (µg/L): p_tot; nitrogen-N (mg/L):
n_no3, chlorophyll (µg/L): chl, water framework directive lake type: wfd_type
</small>


::: aside
Data set from UBA = Umweltbundesamt = German Federal Environmental Agency
:::

## Code to read the data

```{r, eval=FALSE}
lakes <- read.csv(file="lakes-combined-data.csv")
valid_columns <- c("name", "shortname", "z_mean", "z_max",  "t_ret", "volume", 
  "area", "p_tot", "n_no3", "chl", "wfd_type")
lakes <- lakes[valid_columns] |> na.omit()
row.names(lakes) <- lakes$shortname

lake_ids <- lakes[c("name", "shortname")]
lakedata <- lakes[, -c(1, 2)]

## remove wfd_type for now
lakedata$wfd_type <- NULL
```



* data available from ......................


## Data transformation and normalization

```{r uba-lakes-boxplot, fig.align='center', fig.width=8, fig.height=3}
par(mfrow = c(1, 4), mar = c(6, 4, 3, 1), las = 2)
boxplot(lakedata, main = "raw data")
boxplot(scale(lakedata), main = "normalized")
boxplot(scale(sqrt(lakedata)), main = "sqrt + normalized")
boxplot(scale(log(lakedata)), main = "log + normalized")
```


* `scale()` performs normalisation (z-transformation)
* aim: make different scales better comparable


# Ordination methods

## Principal Component Analysis: PCA

<br>

* identify cvovariance or correlation structure
* rotate coordinate system, so that it points in the diretions of maximum variance
* $k$ dimensions in original space are transformed into $k$ orthogonal (rectangular) coordinates in principal components space.
* works with any number of dimensions
* visualisation by a 3D example

## Correlation structure of the lakes data set

<br>

::: {.bigfont}
```{r echo=TRUE}
round(cor(lakedata), 2)
```
:::


<br>

* Let's pick 3 variables for a 3D visualization: `z_mix`, `z_max` and `volume`.
* Use log-transformation to make them symetrically distributed.



## `z_mix`, `z_max` and `volume` are highly corelated

```{r, echo=FALSE}
lakes <- read.csv(file="../data/uba/lakes-combined-data.csv")
valid_columns <- c(
  "name", "shortname",
  "z_mean",
  "z_max", 
  "volume"
)

## less columns, so that we get a simplified subset and more complete cases
lakes <- lakes[valid_columns] |> na.omit()

row.names(lakes) <- lakes$shortname
lake_ids <- lakes[c("name", "shortname")]
lakedata <- lakes[, -c(1, 2)]
```


```{r, echo= FALSE, fig.align='center'}
par(mfrow=c(2,2), mar=c(4,4,1,1)+.1)
plot(log(z_max) ~ log(z_mean), data=lakedata, pch=16)
plot(log(z_max) ~ log(volume), data=lakedata, pch=16)
plot(log(volume) ~ log(z_mean), data=lakedata, pch=16)
par(mar=c(3,4,2,4))
with(lakedata, scatterplot3d(log(z_mean), log(z_max), log(volume), pch=16, type="h"))
```

$\rightarrow$ We see that the three variables carry **redundant** information.



## How rotation of axes works


:::{.column width="48%"}

Original coordinates

```{r ordirgl1, echo=FALSE, eval=TRUE, fig.height=4, fig.width=4, fig.align='center'}
A <- with(lakedata, data.frame(
  log_z_mean = log(z_mean),
  log_z_max  = log(z_max),
  log_volume = log(volume)
))

ordirgl(A, type="p", ax.col = "black", col="red", box=FALSE)
view3d(theta = 5, phi = 15, fov=30, zoom=1)
#axes3d(labels=FALSE)
```

:::


:::{.column width="48%"}

PCA rotated coordinates

```{r ordirgl2, echo=FALSE, eval=TRUE, fig.height=4, fig.width=4, fig.align='center'}
pc <- prcomp(A)
ordirgl(pc, type="p", display="sites", 
        ax.col = "black", col="red")#, xlim=c(0,6), ylim=c(0,6), zlim=c(0,6))
view3d(theta = 0, phi = 0, fov=30, zoom=1)
#axes3d()
```

:::

<br>

* This slide contains interactive 3D graphics, that can be rotated with the mouse.
* Rotate the left image, so that the points on both size show similar patterns.
* Rotate the right image to show PC 3

$\rightarrow$ Most of the 3D information of the data can be vizualized in 2D.

Note: log-transformed variables were used in this example.


## PCA is an orthogonal (model II) regression


:::{.column width="32%"}

* PC1, PC2, PC3 are the principal components
* OLS is ordinary least squares regression (linear model lm)

:::

:::{.column width="66%"}

```{r, echo=FALSE, fig.height=6, fig.width=6, fig.align='center'}

col <- c("red", "cyan", "violet")

rotate_pc <- function(x, pc=c(1, 2), ...) {
  p <- prcomp(x)
  l <- p$rotation #loadings(p)
  a <- p$center

  plot(x, las=1, pch=16, ...)
  abline(lm(x[,2] ~ x[,1]), col="blue", lty="dotted", lwd=2)
  
  b1 <- l[2,1]/l[1,1]
  abline(a=a[2] - b1*a[1], b=b1, col=col[pc[1]], lwd=2)
  
  b2 <-  l[2,2]/l[1,2]
  abline(a=a[2] - b2*a[1], b=b2, col=col[pc[2]], lwd=2)
  
  legend("bottomright", c(paste("PC", pc[1]), paste("PC", pc[2]), "OLS"), 
         col=c(col[pc[1]], col[pc[2]], "blue"), lwd=2,
         lty=c("solid","solid","dotted"), bty="o", bg="white")
}

par(mfrow=c(2, 2))
par(mar=c(4.1, 5.1, 2.1, 1.1))

rotate_pc(A[,1:2], 1:2, xlim=c(0,10), ylim=c(0,10))
rotate_pc(A[,c(3,1)], c(3,1), xlim=c(-5,5), ylim=c(0,10))
rotate_pc(A[,c(2,3)], c(2,3), xlim=c(0,10), ylim=c(-5,5))

p <- prcomp(A)
#biplot(p)

plot(p$x[,1], p$x[,2], pch=16, col="blue", xlab="PC 1", ylab= "PC 2")
abline(h=0, col=col[1], lwd=2)
abline(v=0, col=col[2], lwd=2)
```

:::

## Now analyse all numeric variables

<br>

<small>
```{r, echo=FALSE}
lakes <- lakes_saved

lake_ids <- lakes[c("name", "shortname")]
lakedata <- lakes[, -c(1, 2)]
lakedata$wfd_type <- NULL

lakedata2 <- sqrt(lakedata)

kable(lakes)
```
</small>

## PCA with the UBA lake data


```{r, echo=FALSE}
options(width=120)
```


```{r}
pc <- prcomp(scale(lakedata))
```

Eigenvalues (proportion of variance) indicate importance of components.

```{r}
summary(pc)
```


```{r uba-pca-variance, echo=FALSE, fig.align='center', fig.width=10, fig.height=3.5}
par(mar=c(4,4,2,1), las=1)
plot(pc)
n_pc <- length(pc$sdev)
mtext(side=1, at=1.2*(1:n_pc) - 0.6, text=paste("PC", 1:n_pc))
```

## PCA Biplot

```{r pca-biplot1, fig.align='center', fig.width=6, fig.height=6}
biplot(pc)
```

## PCA with sqrt transformed data

<br>

```{r}
lakedata2 <- sqrt(lakedata)
pc <- prcomp(scale(lakedata2))
summary(pc)
```

<br>

* The PCA with the untransformed data looked very asymmetric, we repeat it with
square root transformed data.
* helps to get a better "resolution"
* must be taken into account when interpreting the results
* log transformation is also possible


## Biplot of sqrt transformed data  (1.-3. PC)

```{r uba-pca-biplot, fig.align='center', fig.width=12, fig.height=6}
par(mfrow=c(1, 2), mar=c(5, 4, 4, 2.4), las=1)
biplot(pc)
biplot(pc, choices=c(3, 2))
```


## PCA with the vegan package

```{r}
pc <- rda(lakedata2, scale = TRUE)
summary(pc)
```

<!---
summary(pc3)$cont$importance # trick to suppress long output
--->

## Biplot

```{r uba-rda-biplot, fig.width=5, fig.height=5, fig.align='center'}
biplot(pc)
```


## Biplot in 3D

```{r ordirgl3, echo=FALSE, eval=TRUE, fig.height=6, fig.width=6, fig.align='center'}
ordirgl(pc, col = "yellow")
orgltext(pc, display = "species", col="red")
orgltext(pc, display = "sites", col="blue", pos=4)
view3d(theta = 5, phi = 15, fov=30, zoom=1)
```


# Distance and similarity

## Euclidean distance

<br>

```{r, echo=FALSE, fig.align='center'}
plot(NULL, xlim=c(0,10), ylim=c(0,10), las=1, type="n", xlab="x", ylab="y")

x <- c(2, 8)
y <- c(1, 7)
points(x, y, pch=16, cex=2, type="b", lwd=2)
text(x, y + 0.2, c("A", "B"), cex=2, pos=3)
lines(c(x, x[2]), c(y[1], y[1], y[2]))
text(mean(x), y[1], "a", pos=1, cex=2)
text(x[2], mean(y), "b", pos=4, cex=2)
text(mean(x), mean(y), "c", pos=3, cex=2)
```


* PCA works with Euclidean distance
* rule of Pythagoras

$$
a^2 + b^2 = c^2 \quad \Rightarrow\quad c = \sqrt{a^2 + b^2} = \sqrt{\Delta x^2 + \Delta y^2}
$$

* but: Euclidean distance is not always the best option.

## Distance and dissimilarity

<br>

**Axiomatic definition**

Measure of distance $d$ between multidimensional points $x_i$ and $x_j$:


1. $d(x_i, x_j) \ge 0$, distances are similar or equal to zero
2. $d(x_i, x_j)=d(x_j,x_i)$, the distance  from A to B is the
    same as from B to A,
3. $d(x_i, x_i)=0$, the distance from a given point to itself is zero

A distance measure is termed metric, if:

1. $d=0$ applies in the case of equality  only, and
2. the triangle inequality aapplies.<br>
  [The indirect route is longer than the direct route]{.gray}

If one or both of the additional conditions are violated, we speak about **nonmetric**
measures and use the term **dissimilarity** instead of distance.

## Similarity

<br>

A measure of similarity $s$ can be defined in a similar way:


1. $s(x_i,x_j) \le s_{max}$
2. $s(x_i,x_j)=s(x_j,x_i)$
3. $s(x_i,x_i)=s_{max}$

it is metric, if:

* $s_{max}$ applies only in the case of equality and
* the triangle inequality applies


## Conversion between dissimilarity and similarity


<br>


<br>

:::{.bigfont}

| similarity     |  dissimilarity        |
|----------------|-----------------------|
| $s=1-d/d_{max}$| $d=1-s/s_{max}$       |
| $s=\exp(-d)$   | $d= - \ln(s-s_{min})$ |

:::

<br>

* distance goes from $0$ to $\infty$
* different transformations, as long as the $\Rightarrow$ transformation is monotonic
* in most cases similarity $s$ is limited between $(0, 1)$ or between 0 and 100%.



## Common distance and dissimilarity measures

<br>

* **Euclidean** distance: shortest connection between 2 points in space
* **Manhattan** distance: around the corner, as in Manhattans grid-like streets
* **Chi-square** distance: for comparison of frequencies,
* **Mahalanobis** distance: takes covariance into account
* **Bray-Curtis** dissimilarity: comparison of species lists in ecology
* **Jaccard** index: for binary (presence-absence) data
* **Gower** dissimilarity:  used for mixed-type variables


## Distance and dissimilarity of **metric** variables

with $x_{ij}, x_{ik}$ abundance of species  $i$ at sites ($j, k$).

Euclidean distance:

$$
d_{jk} = \sqrt{\sum (x_{ij}-x_{ik})^2}
$$

Manhattan distance:
$$
d_{jk} = \sum |x_{ij}-x_{ik}|
$$


Gower distance:
$$
d_{jk} = \frac{1}{M} \sum\frac{|x_{ij}-x_{ik}|}{\max(x_i)-\min(x_i)}
$$


Bray-Curtis dissimilarity:
$$
d_{jk} = \frac{\sum{|x_{ij}-x_{ik}|}}{\sum{(x_{ij}+x_{ik})}}
$$

## Distance and dissimilarity of binary variables

<br>

* Euclidean: $\sqrt{A+B-2J}$
* Manhattan: $A+B-2J$
* Gower: $\frac{A+B-2J}{M}$
* Bray-Curtis: $\frac{A+B-2J}{A+B}$
* Jaccard: $\frac{2b}{1+b}$ with $b$ = Bray-Curtis dissimilarity


where:

* $A, B$ = numbers of species on compared sites
* $J$ =  (joint) is the number of species that occur on both compared sites
* $M$ =  number of columns (excluding missing values)

<br>
**Applications**

Additional distance measures and application suggestions in
the  [`vegdist`](https://www.rdocumentation.org/packages/vegan/versions/2.4-2/topics/vegdist) help page.




## Nonmetric Multidimensional Scaling: NMDS

```{r uba-nmds, fig.align='center', fig.width=5, fig.height=5}
md <- metaMDS(lakedata2, scale = TRUE, distance = "euclid", trace=FALSE)
plot(md, type="text")
abline(h=0, col="grey", lty="dotted")
abline(v=0, col="grey", lty="dotted")
```

## A Taxonomic Table

<small>
```{r echo=FALSE}
benthos <- read.csv("../data/gauernitz.csv") 
kable(benthos[-(2:4)])
```
</small>

* Aggregated part of taxa list from two small streams.

## How to aggregate the data?

<hr></hr>

:::{.column width="44%" .add-space}
**Simpson index**

$$
D = \sum_{i=1}^S p_i^2
$$

* $p_i$: relative abundance of species
* in most cases, Simpson index is given as $\tilde{D} = 1 - D$<br>
  [(large values -- high diversity)]{.gray}
* also possible: inverse Simpson index: $D' = 1 / D$<br> 

:::

:::{.column width="48%"}

**Shannon index**

$$
H = -\sum_{i=1}^S p_i \log_b p_i
$$
  
  * in most cases log base $b=e$ (natural log), some prefer $b=2$ (information theory)
  
  
  **Eveness **
  
  $$
  E = \frac{H}{\log(S)}
  $$
  
* $S$: number of species

:::

<hr></hr>

* more indices: species richness, species deficit, Fisher's $\alpha$ ...

## Diversity indices

:::{.column width="20%"}
:::

:::{.column width="60%"}
<small>
```{r echo=FALSE}
library("vegan")
species <- benthos[c("Mollusca", "Diptera", "Baetis", "Plecoptera", 
                     "Coleoptera", "Turbellaria", "Heptageniidae",
                     "Ephemeroptera", "Gammarus", "Trichoptera", 
                     "Acari", "Nematoda", "Oligochaeta")]
div <- data.frame(
  shannon = diversity(species, index="shannon"),
  simpson = diversity(species, index="simpson"),
  invsimpson = diversity(species, index="invsimpson"),
  eveness = diversity(species, index="shannon") / log(ncol(species)),
  fisher_alpha = fisher.alpha(species)
)
div <- lapply(div, round, 2)
kable(cbind(benthos[c("Site", "Habitat", "Stream", "Flood")], div))
```
</small>
:::

* aggregated data but which of the indices tells what?
* $\rightarrow$ information loss compared to the original list



# Cluster Analysis


## Overview

* Cluster analysis aims to group data sets in clusters

* Hierarchical clustering
  - build a dendrogram (a tree of grouping)
  - agglomerative methods
  - divisive methods

* Different agglomeration methods
  - define how distance is measured between clusters

* Nonhierarchical clustering
   - subdivide in a given number of groups
   - usually no dendrogram
   - iterative methods
   - e.g. k-means, k-centroids
   
   


```{r  fig.height=6, wig.width=12, fig.align='center'}
par(mfrow=c(1,2))
hc <- hclust(dist(scale(lakedata2)), method="complete") # the default
plot(hc)

hc2 <- hclust(dist(scale(lakedata2)), method="ward.D2")
plot(hc2)
```

## Identification of clusters in the tree

```{r}
plot(hc, hang = -1)    # -1: extend vertical lines to the bottom
rect.hclust(hc, 5)
grp <- cutree(hc, 5)
# grp                  # can be used to show the groups
```

## Color NMDS according to clusters

```{r}
plot(md, type = "n")
text(md$points, row.names(lakedata2), col = grp)
```


## Non-hierarchical clustering

Instead of hierarchical clustering, we can also use a non-hierarchical method,
e.g. k-means clustering. This is an iterative method, and avoids the problem that 
cluster assignment depends on the order of clustering and the agglomeration method.

Depending on the question, it may be a disadvantage, that the number of clusters 
needs to be specified beforehand (e.g. from hierarchical clustering) and that we
do not get a tree diagramm.


 
## References

