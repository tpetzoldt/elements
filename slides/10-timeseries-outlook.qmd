---
subtitle: "Applied Statistics -- A Practical Course"
title: "10-Time Series Outlook"
date:   "`r Sys.Date()`"
--- 

```{r setup, echo=FALSE}
library(Kendall)
library(dplyr)
library(readr)
library(ggplot2)
library(lubridate)
```

# Time series decomposition

## Time series decomposition

<br>

**Traditional approach**

Decomposition of time series into:

1. trend component,
2. seasonal or periodic component and
3. stochastic component.

Many of the decomposition methods require normal distribution of the residuals.
If this is not the case or, even worse, the variance of a time series changes proportionally with
a trend, a transformation might be required. 

For hydrological or biomass data, which frequently
show positive skewness, a logarithmic transformation is often helpful.

## Smoothing methods


* use regression vs. time (linear or nonlinear regression)
* or use smoothers (moving averages)
* then subtract the trend line

```{r filter1,fig=TRUE, echo=TRUE, fig.align='center'}
library(Kendall)
data(PrecipGL)
tsp(PrecipGL)
plot(PrecipGL)
kernel <- rep(1, 10)  # a rectangular kernel, vary bandwith
lines(stats::filter(PrecipGL, kernel/sum(kernel)), lwd = 2, col = "blue")
```

## Trend corrected series

Now, the trend corrected series can be obtained by subtracting the trend, e.g.:

```{r filter2, echo=TRUE, fig.align='center'}
smooth <- stats::filter(PrecipGL, kernel/sum(kernel))
res <- PrecipGL - smooth
plot(res)
```

Note: use `stats::filter` to avoid potential conflict with `dplyr::filter`.

## Trend corrected series

For a seasonal time series with monthly values @kleiber_applied_2008 recommend
a filter with 13 coefficients.

Example: water level of Rio Negro 18 km upstream from its confluence with
the Amazon River. The data set is contained in the package **boot**
[@canty_boot_2022]:

```{r filter3,fig=TRUE, echo=TRUE, fig.align='center'}
library(boot)
data(manaus)
tsp(manaus)
plot(manaus)
lines(stats::filter(manaus, c(0.5, rep(1, 11), 0.5)/12), lwd=2, col="blue")
```

## Practical smoothing with the LOWESS filter

```{r lowess, echo=TRUE, fig.align='center'}
#| code-fold: true
#| code-summary: "Show the code"
plot(PrecipGL)
lines(lowess(time(PrecipGL), PrecipGL, f = 2/3), lwd = 3, col = "blue")
lines(lowess(time(PrecipGL), PrecipGL, f = 1/3), lwd = 3, col = "forestgreen")
lines(lowess(time(PrecipGL), PrecipGL, f = 0.1), lwd = 3, col = "red")
```

* locally weighted polynomial regression [@cleveland_robust_1979]
* different functions available in **R**: `lowess` and `loess`
* allows to adjust smoothness (smoother span `f`)



## Identification of seasonal components

<br>

**Periodic phenomena are common in nature**

* seasonal course of solar radiation and temperature
* seasonal development of vegetation 
* heartbeat of animals.

**Harmonic analysis**

* every time series can be described as a sum of sine and a cosine
functions with different periods (Fourier series)

$$
 x_t = a_0 + \sum_{p=1}^{N/2-1} \big(a_p \cos(2 \pi p t/N)
                                 +    b_p \sin(2 \pi p t/N)\big)
           +a_{N/2} \cos(\pi t), \qquad t=1 \dots N
$$

**with**

$x_t$: equidistant time series, $t$: time step, $N$: number of data, $a_p, b_p$: Fourier coefficients,<br>
$a_0 = \bar{x}$, $p$: order of the periodic component


## Formulation with a single cosine term

<br>

The equation with sine and cosine terms can be transformed in an equation with cosine terms only,
using the trigonometric addition formulas:

<br>

$$
x_t = a_0 + \sum(R_p \cdot \cos(2 \pi p t / N + \Phi_p)
$$

with:

* amplitude: $R_p = \sqrt{a_p^2 + b_p^2}$
* phase shift: $\Phi_p = \arctan(-b_p/a_p)$

<br>
see:  [https://en.wikipedia.org/wiki/List_of_trigonometric_identities](https://en.wikipedia.org/wiki/List_of_trigonometric_identities)

## Identification of seasonal parameters


1. linear regression with the `lm` function, example for the simple case with a single periodic component<br> `m <- lm(x ~ sin(2 * pi * t / N) + cos(2 * pi * t / N))`
2. nonlinear regression with `nls` [(allows to identify the period as nonlinear parameter)]{.gray}
3. Classical Fourier analysis:

\begin{align}
     a_0 &= \bar{x}\\
 a_{N/2} &= \sum_{t=1}^{N}(-1)^tx_t/N\\
 a_p     &= 2 \frac{\sum_{t=1}^{N} x_t \cos(2 \pi p t/N)}{N}\\
 b_p     &= 2 \frac{\sum_{t=1}^{N} x_t \sin(2 \pi p t/N)}{N}
\end{align} 

4. Fast Fourer transform (FFT) using complex numbers
  [used in signal processing (CD and MP3 players, mobile phones, ...]{.gray}

## Fast Fourier transform

```{r fft-demo, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
# create an arbitrary time series
set.seed(123)
n   <- 360
t   <- 0:(n-1)
x   <- sin(t*2*pi/n) + rnorm(n, sd = 0.2)
plot(t, x, xlim = c(-10, 370))
  
# perform the FFT
p       <- fft(x)

# invert the FFT for the main period only (keep parameters 1..3)
p[4:n]  <- 0 # set higher order parameters to zero
x_sim      <- fft(p, inverse = TRUE)
lines(t, 2 * Re(x_sim)/n - Re(p[1])/n, col = "blue", lwd = 4)


## perform FFT and invert it for all periods
p       <- fft(x)
p[(n/2):n] <- 0 # set only the "redundant frequencies" to zero
x_sim         <- fft(p, inverse = TRUE)
lines(t, 2 * Re(x_sim)/n - Re(p[1])/n, col = "red", lwd = 1)
```



## Automatic time series decomposition

Function `decompose` implements the classical approach with simple symmetric moving average
filters.

```{r decompose,echo=TRUE,fig=TRUE,width=10, height=10, fig.align='center'}
manaus_dec <- decompose(manaus)
plot(manaus_dec)
```

In this data set the seasonal component possesses an additive character,
but a multiplicative component would be possible too (`type="multiplicative"`).


## Automatic time series decomposition II

The function `stl`, seasonal time series decomposition
[@cleveland_stl_1990] uses a LOESS filter:

```{r stl,fig=TRUE, echo=TRUE,width=10, height=8, fig.align='center'}
manaus_stl <- stl(manaus, s.window=13)
plot(manaus_stl)
```

# Outlook: ARMA and ARIMA models

<br>

* based on autoregressive (AR) and moving average (AR) terms
* the "I" in ARIMA stands for differencing
* seasonal ARIMA models (SARIMA) consider seasonal dependency,<br>
  e.g. differencing with a lag of 12 months
* very important techniques for analyzing time series and for forecasting
* for the interested: self-study by reading @kleiber_applied_2008 or @shumway_time_2019.

## Example: SARIMA model for CO2 in the atmosphere

```{r co2-sarima0, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
## read data directly from NOAA
#co2 <- read_csv("https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_mm_mlo.csv", 
#                skip = 40, show_col_types = FALSE)

## local version
co2 <- read_csv("../data/co2_mm_mlo.csv", skip=40, show_col_types = FALSE)

## year 1958 is incomplete, so let's start with 1959
co2 <- filter(co2, year > 1958) 

co2 |> 
  mutate(date=as_date(paste(year, month, 15, sep="."))) |>
  ggplot(aes(date, average)) + 
  geom_line() + ylab(expression(CO[2]~"in the atmosphere (ppm)")) + 
  ggtitle("Mauna Loa Observatory, Hawaii, Monthly Averages") +
  geom_smooth()
```


## Fit SARIMA model to CO2 data
```{r co2-sarima1, echo=TRUE, results='hide'}
#| code-fold: true
#| code-summary: "Show the code"
library(astsa)

## convert to a time series object
co2ts <- ts(co2$average, start=1959, frequency = 12)

## fit a SARIMA model and show the results
m <- sarima(co2ts, p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12)
```
Mathematical details can be found in @shumway_time_2019.


## SARIMA-forecast of CO2 

```{r co2-sarima2, echo=TRUE}
#| code-fold: true
#| code-summary: "Show the code"
m <- sarima.for(co2ts, n.ahead = 12 * 27, 
           p = 1, d = 1, q = 1, P = 0, D = 1, Q = 1, S = 12, 
           plot.all = TRUE)
```
Mathematical details can be found in @shumway_time_2019.

# Identification of structural breaks


## Identification of structural breaks

```{r nile-intro, fig.align='center', fig.width=10, fig.height=3}
par(mar=c(4,5,2,1))
library(strucchange)
data("Nile")
bp.nile <- breakpoints(Nile ~ 1)
fm1 <- lm(Nile ~ breakfactor(bp.nile,  breaks = 1))
plot(Nile, las=1, ylab=expression(10^~m^3), main="Discharge of the river Nile")
lines(ts(fitted(fm1),  start = 1871),  col = 4, lwd=2)
```


**Structural break**

If one or more statistical parameters are not constant over the whole length of a
time series it is called a structural break. For instance a location parameter
(e.g. the mean), a trend or another distribution parameter (such as variance or
covariance) may change.

<br>

**Testing for structural breaks**

The package **strucchange** implements a number of tests
for identification of structural changes or parameter instability of time 
series. Generally, two approaches are available: fluctuation tests and
F-based tests. Fluctuation tests try to detect the structural instability with
cumulative or moving sums (CUSUMs and MOSUMs).

## The Nile data set

The Nile data set contains
measurements of annual discharge of the Nile at Aswan from 1871 to 1970
[(see help page ?Nile for data source):]{.gray}

```{r nile, fig=TRUE, echo=TRUE, fig.align='center'}
library(strucchange)
data("Nile")
plot(Nile)
```


## Test for structural break at known time

* Are there are periods with different flow?
* Use of OLS-CUSUM (ordinary least squares, cumulative sum)
 or MOSUM tests (moving average sum) can be applied to various types of problems).
* `efp` = empirical fluctuation model,`sctest` = structural change test


```{r ocus, fig=TRUE, echo=TRUE, fig.align='center'}
ocus <- efp(Nile ~ 1, type = "OLS-CUSUM")
plot(ocus)
sctest(ocus)
```


## Breakpoint analysis

<br>

**Purpose**

* identify if there are structural breaks with respect to a
  specified linear model
* identify their number and location
* very general approach, allows different linear models, covariates, ...
* we deal only with the most simplest case here

**Method**

* uses a BIC-based model selection technique
* Which model is the best?

**See also**

* @Bai2003, @Zeileis2002, @Zeileis2003.



## Remember: model selection techniques

<br>

* Model Selection is a modern alternative to $p$-value based testing.
* Based on the principle of parsimony:
    * Models with many parameters fit better, but may contain unnecessary factors.
    * We need is an **optimal compromize** between model fit and complexity.
    * Keep it as simple as possible, but not simpler.

* Compare different models for the same phenomenon:
    * [Full model]{.blue}: includes all potential factors.
    * Several [reduced models]{.blue}, derived from the full model.
    * [Null model]{.blue} without explanatory factors.

* Select [minimal adequate model]{.blue}  (= "optimal", "most parsimonious").
* $\Rightarrow$ Compromize between simplicity and goodness of fit.

More, see @JohnsonOmland2004



## Model comparison with AIC and BIC

<br>

Models with many parameters fit better. We want an
**optimal compromize** between fit and model complexity
(number of parameters, $k$).


* Goodness of fit can be measured with likelihood $L$<br>
  [(how likely are the data given a specific model).]{.gray}
* Log likelihood: makes the problem additive.
* Penalty: penalises number of parameters (e.g. $2 k$)
* AIC (Akaike Information Criterion):

$$
 AIC = - 2 \ln(L) + 2 k
$$

* Alternatively: BIC (Bayesian Information Criterion),
 considers sample size ($n$):
 
$$
 BIC = - 2 \ln(L) + k \cdot \ln(n)
$$


The model with smallest AIC (resp. BIC) is considered as "optimal" model.



## Breakpoint analysis

<br>

```{r fig=FALSE, echo=TRUE}
bp.nile <- breakpoints(Nile ~ 1)
summary(bp.nile)
```

**Notes**

* computationally intensive
* consider to adapt model and parameter `h`, see help page

## Likelihood profile: $\rightarrow$ optimal number of breaks?

```{r bp-nile, fig=TRUE, echo=TRUE, fig.align='center'}
bp.nile <- breakpoints(Nile ~ 1)
plot(bp.nile)
```


* RSS (residual sum of squares) shows goodness of fit. The smaller the better.
* Penalty = 2 $\cdot$ number of breakpoints (not shown).
* Minimum BIC indicates optimal model.


## Plot breakpoints and confidence interval

```{r ci-nile, fig=TRUE, echo=TRUE, fig.align='center'}
## fit null hypothesis model and model with 1 breakpoint
fm0 <- lm(Nile ~ 1)
fm1 <- lm(Nile ~ breakfactor(bp.nile,  breaks = 1))
plot(Nile)
lines(ts(fitted(fm0),  start = 1871),  col = 3)
lines(ts(fitted(fm1),  start = 1871),  col = 4)
lines(bp.nile)

## confidence interval
ci.nile <- confint(bp.nile)
#ci.nile  # output numerical results
lines(ci.nile)
```

## Test significance of breakpoints

<br>

**Likelihood ratio test**

```{r fig=FALSE, echo=TRUE}

anova(fm0, fm1)
```

**BIC based comparison**

```{r fig=FALSE, echo=TRUE}
BIC(fm0, fm1)
```

**AIC based comparison**

```{r fig=FALSE, echo=TRUE}
AIC(fm0, fm1)
```


$\rightarrow$ model with structural break in 1898 is 
significantly better than the null model.


## Diagnostics

<br>


```{r nile-diagnostics, echo=TRUE, fig.align='center'}
par(mfrow = c(2, 2))
acf(residuals(fm0))     # model without breaks
acf(residuals(fm1))     # model with 1 breakpoint
qqnorm(residuals(fm0))
qqnorm(residuals(fm1))
```

<!--
#spectrum(residuals(fm0), log = "no",  spans = c(5, 5))
#spectrum(residuals(fm1), log = "no",  spans = c(5, 5))
-->


## References

<br>

