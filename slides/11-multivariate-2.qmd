---
title: "11-Multivariate methods II"
author: "Thomas Petzoldt"
date:   "`r Sys.Date()`"
---

```{r setup, include=FALSE}
## this code chunk sets some technical details, it is not shown to the user
library("readxl")
library("vegan")
library("knitr")
library("kableExtra")
library("leaflet")
library("rgl")
library("scatterplot3d")
library("ggplot2")
library("tidyr")
knitr::opts_chunk$set(echo = TRUE, eval=TRUE)
setupKnitr(autoprint = FALSE) # for embedded RGL graphics
```

## Data sets and terms of use

<br>

1. The "UBA-lakes" data set originates from the public data repository of the German 
Umweltbundesamt [@UBA2020]. The data set provided can be used freely according
to the [terms and conditions](https://www.umweltbundesamt.de/datenschutz-haftung#c-urheber-und-kennzeichenrecht) 
published at the [UBA web site](https://www.umweltbundesamt.de/), that refer to
ยง 12a EGovG with respect of the data, and to the [Creative Commons CC-BY ND International License 4.0 ](https://creativecommons.org/licenses/by-nd/4.0/deed.de) with respect to other objects directly created by UBA.


2. The "gauernitz" data set contains simplified teaching versions from research data, of the study from @winkelmann_fish_2011

3. The document itself, the codes and the ebedded images are own work and can be shared according to [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/deed.en).



## A Taxonomic Table

<small>
```{r echo=FALSE}
benthos <- read.csv("../data/gauernitz.csv") 
kable(benthos[-(2:4)])
```
</small>

* Aggregated part of taxa list from two small streams.
* Mayfly splitted in most dominant taxa (*Baetis* and *Heptageniidae*) and the 
remaining *Ephemeroptera* for simplicity of the teaching example.

## Bar chart


<!-- use local data -->
```{r gau-barchart, echo=FALSE, fig.height=4, fig.width=12}
benthos <- read.csv("../data/gauernitz.csv")

benthos_long <-
  benthos |>
  pivot_longer(cols=5:17, names_to="Taxon", values_to = "Abundance")

ggplot(benthos_long, aes(x=Site, y=Abundance, fill=Taxon)) + geom_col()
```


```{r, echo=TRUE, eval=FALSE}
library("ggplot2")
library("tidyr")

benthos <- read.csv("https://raw.githubusercontent.com/tpetzoldt/datasets/main/data/gauernitz.csv")

benthos_long <-
  benthos |>
  pivot_longer(cols=5:17, names_to="Taxon", values_to = "Abundance")

ggplot(benthos_long, aes(x=Site, y=Abundance, fill=Taxon)) + geom_col()
```


## How to analyse this kind of data?

<br>

Different approaches

1. Direct interpretation
    - raw data, mean values, ...
    - tables and plots

2. Calculation of biodiversity indices
    - general-purpose indices (Richness, Simpson, Shannon, Eveness)
    - domain-specific indices (in stream ecology: sapropbic index, Perlodes, EPT)


3. Multivariate statistics
    - ordination methods (CCA, NMDS, dbRDA)
    - cluster analysis


## Diversity indices

<hr></hr>

:::{.column width="44%" .add-space}
**Simpson index**

$$
D = \sum_{i=1}^S p_i^2
$$

* $p_i$: relative abundance of species
* in most cases, Simpson index is given as $\tilde{D} = 1 - D$<br>
  [(large values -- high diversity)]{.gray}
* also possible: inverse Simpson index: $D' = 1 / D$<br> 

:::

:::{.column width="48%"}

**Shannon index**

$$
H = -\sum_{i=1}^S p_i \log_b p_i
$$
  
  * in most cases log base $b=e$ (natural log), some prefer $b=2$ (information theory)
  
  
  **Eveness **
  
  $$
  E = \frac{H}{\log(S)}
  $$
  
* $S$: number of species

:::

<hr></hr>

* more indices: species richness, species deficit, Fisher's $\alpha$ ...

## Diversity indices

:::{.column width="20%"}
:::

:::{.column width="60%"}
<small>
```{r echo=FALSE}
library("vegan")
species <- benthos[c("Mollusca", "Diptera", "Baetis", "Plecoptera", 
                     "Coleoptera", "Turbellaria", "Heptageniidae",
                     "Ephemeroptera", "Gammarus", "Trichoptera", 
                     "Acari", "Nematoda", "Oligochaeta")]
div <- data.frame(
  shannon = diversity(species, index="shannon"),
  simpson = diversity(species, index="simpson"),
  invsimpson = diversity(species, index="invsimpson"),
  eveness = diversity(species, index="shannon") / log(ncol(species)),
  fisher_alpha = fisher.alpha(species)
)
div <- lapply(div, round, 2)
kable(cbind(benthos[c("Site", "Habitat", "Stream", "Flood")], div))
```
</small>
:::

* aggregated data but which of the indices tells what?
* $\rightarrow$ information loss compared to the original list



## Problem

<br>

**The PCA does not work well for this kind of data**

* Without standardization: most frequent taxa dominate the analysis, rare species under-represented

* With standardization: rare species will given too much influence, result dominates much on sampling error

* square root transformation does not help, log-transformation is not possible because of zeros


**Why?**

* the distance measure, used in PCA is the so-called Euclidean distance
* it works not well for species lists

**Approach**

* use other distance and dissimilarity measures


# Distance and similarity

## Euclidean distance

<br>

```{r triangle-euclidean, echo=FALSE, fig.align='center'}
plot(NULL, xlim=c(0,10), ylim=c(0,10), las=1, type="n", xlab="x", ylab="y")

x <- c(2, 8)
y <- c(1, 7)
points(x, y, pch=16, cex=2, type="b", lwd=2)
text(x, y + 0.2, c("A", "B"), cex=2, pos=3)
lines(c(x, x[2]), c(y[1], y[1], y[2]))
text(mean(x), y[1], "a", pos=1, cex=2)
text(x[2], mean(y), "b", pos=4, cex=2)
text(mean(x), mean(y), "c", pos=3, cex=2)
```


* PCA works with Euclidean distance
* rule of Pythagoras

$$
a^2 + b^2 = c^2 \quad \Rightarrow\quad c = \sqrt{a^2 + b^2} = \sqrt{\Delta x^2 + \Delta y^2}
$$

* but: Euclidean distance is not always the best option.

## Distance and dissimilarity

<br>

**Axiomatic definition**

Measure of distance $d$ between multidimensional points $x_i$ and $x_j$:


1. $d(x_i, x_j) \ge 0$, distances are similar or equal to zero
2. $d(x_i, x_j)=d(x_j,x_i)$, the distance  from A to B is the
    same as from B to A,
3. $d(x_i, x_i)=0$, the distance from a given point to itself is zero

A distance measure is termed metric, if:

1. $d=0$ applies in the case of equality  only, and
2. the triangle inequality aapplies.<br>
  [The indirect route is longer than the direct route]{.gray}

If one or both of the additional conditions are violated, we speak about **nonmetric**
measures and use the term **dissimilarity** instead of distance.

## Similarity

<br>

A measure of similarity $s$ can be defined in a similar way:


1. $s(x_i,x_j) \le s_{max}$
2. $s(x_i,x_j)=s(x_j,x_i)$
3. $s(x_i,x_i)=s_{max}$

it is metric, if:

* $s_{max}$ applies only in the case of equality and
* the triangle inequality applies


## Conversion between dissimilarity and similarity


<br>


<br>

:::{.bigfont}

| similarity     |  dissimilarity        |
|----------------|-----------------------|
| $s=1-d/d_{max}$| $d=1-s/s_{max}$       |
| $s=\exp(-d)$   | $d= - \ln(s-s_{min})$ |

:::

<br>

* distance goes from $0$ to $\infty$
* different transformations, as long as the $\Rightarrow$ transformation is monotonic
* in most cases similarity $s$ is limited between $(0, 1)$ or between 0 and 100%.



## Common distance and dissimilarity measures

<br>

* **Euclidean** distance: shortest connection between 2 points in space
* **Manhattan** distance: around the corner, as in Manhattans grid-like streets
* **Chi-square** distance: for comparison of frequencies,
* **Mahalanobis** distance: takes covariance into account
* **Bray-Curtis** dissimilarity: comparison of species lists in ecology
* **Jaccard** index: for binary (presence-absence) data
* **Gower** dissimilarity:  used for mixed-type variables


## Distance and dissimilarity of **metric** variables

with $x_{ij}, x_{ik}$ abundance of species  $i$ at sites ($j, k$).

Euclidean distance:

$$
d_{jk} = \sqrt{\sum (x_{ij}-x_{ik})^2}
$$

Manhattan distance:
$$
d_{jk} = \sum |x_{ij}-x_{ik}|
$$


Gower distance:
$$
d_{jk} = \frac{1}{M} \sum\frac{|x_{ij}-x_{ik}|}{\max(x_i)-\min(x_i)}
$$


Bray-Curtis dissimilarity:
$$
d_{jk} = \frac{\sum{|x_{ij}-x_{ik}|}}{\sum{(x_{ij}+x_{ik})}}
$$

## Distance and dissimilarity of binary variables

<br>

* Euclidean: $\sqrt{A+B-2J}$
* Manhattan: $A+B-2J$
* Gower: $\frac{A+B-2J}{M}$
* Bray-Curtis: $\frac{A+B-2J}{A+B}$
* Jaccard: $\frac{2b}{1+b}$ with $b$ = Bray-Curtis dissimilarity


where:

* $A, B$ = numbers of species on compared sites
* $J$ =  (joint) is the number of species that occur on both compared sites
* $M$ =  number of columns (excluding missing values)

<br>

**Applications**

Additional distance measures and application suggestions in
the  [`vegdist`](https://www.rdocumentation.org/packages/vegan/versions/2.4-2/topics/vegdist) help page.

## Which distances are supported by different methods?

<br>

:::{.smallfont}

|         |                                    |  Matrices     |  Distance |  R function         |
|---------|------------------------------------|---------------|-----------|---------------------|
| PCA     | Principal Components Analysis      | one matrix    | euclidean | `prcomp`, `rda`     |
| RDA     | Redundancy Analysis                | two matices   | euclidean | `rda`               |
| CA      | Correspondence Analysis            | one matrix    | chi square| `cca`               |
| CCA     | Canonical Correspondence Analysis  | two matrices  | chi square| `cca`               |
| PCO/MDS | Principal Correspondence Analysis  | one matrix    | any       | `cmdscale`, ...     |
| dbRDA   | distance-based Redundancy Analysis | two matrices  | any       | `dbrda`, `capscale` |
| PCoA    | Principal Coordinate Analysis      | two matrices  | any       | `wcmdscale`         |
| NMDS    | Non-metric Multidimensional Scaling| one matrix    | any       | `metaMDS`           |
|...      | ...                                |...            |...        | ...                 |
|         |  Cluster analysis                  | one matrix    | any       | several packages    |
|         |                                    |               |           |                     |

<small><br></small>

* many different methods, not all are shown
* one matrix methods: all variables depend on each other; optional matrix of explanation variables can be projected afterwards
* two matrix methods (= constrained methods): additional matrix of explanation variables, both matrices handled simultanaeously
* an additional third matrix is supported by so-called partial methods, e.g. pRDA, pCCA, pdbRDA

:::

## Principal coordinate analysis (PCO)

:::{.column width="49%"}

<br>

```{r}
library("vegan")
benthos <- read.csv("../data/gauernitz.csv")
row.names(benthos) <- benthos$Site
species <- benthos[-c(1:4)]

d   <- vegdist(species, method="bray")
ord <- cmdscale(d)
```
```{r gauernitz-mds, eval=FALSE}
plot(ord, type="n", xlab="PCO 1", ylab="PCO 2")
text(ord, row.names(species))
```

$\rigtarrow$ input is distance matrix (`d`) instead of data matrix (`species`)
:::

:::{.column width="49%"}
```{r gauernitz-mds, echo=FALSE, fig.width=4, fig.height=4}
```
:::


* works with arbitrary distance measures, e.g. Bray-Curtis dissimilarity
* supported by different packages, e.g. **stats**, **vegan**, **labdsv**, **ecodist**, **ade4** and **ape**
* basic version in package **stats** (base R), other ith packages more specialized versions
* basic version has no biplot, can be added done separately


## From PCA to NMDS

**PCA: Principal Components analysis**

โถ Starting point: raw data, covariance or correlation matrix

(+) Basic method, very easy to understand

(+) Biplot: common representation of objects and variables

(--) Only metric distance can be used, not suitable for taxa lists

**PCO: Principal Coordinates Analysis**

[also referred to as (metric) MDS]{.gray}

โถ Starting point: distance matrix


(+) any distance measure can be used


**NMDS: non-metric multidimensional scaling**


* starts with results from PCO or random configuration
* attempts to bring similarity structure better into 2 (or 3) dimensions


## NMDS
<br>

* is an extension of the PCO
* iterative procedure, minimize goodness of fit (called "stress")
* several variants, mostly algorithm according to Kruskal
* start: random configuration or PCO

(+/-) geometric distortion

(--) results not always identical

(--) computer intensive method, for large dada sets

<br>

**Note:** stress is sometimes given as ratio 0...1, sometimes in 0...100%, different values in other statistics programs, e.g. SPSS


## Example

```{r}
benthos <- read.csv("../data/gauernitz.csv")
row.names(benthos) <- benthos$Site

env <- benthos[c("Habitat", "Stream", "Flood")]

bio <- benthos[c("Mollusca", "Diptera", "Baetis", "Plecoptera", "Coleoptera", 
         "Turbellaria", "Heptageniidae", "Ephemeroptera", "Gammarus", 
         "Trichoptera", "Acari", "Nematoda", "Oligochaeta")]

ord <- metaMDS(bio)
```
## Results of the NMDS

```{r}
ord
```

* `metaMDS` made automatic decisions about transformation, distance and scaling
* it ran a series of NMDS trials and outputs the best

**Recommendation**

* specify transformation, scaling and distance explicitly 
* consider to increase `try`and `trymax` for big and/or difficult data sets

```{r, eval=FALSE}
ord <- metaMDS(bio, distance="bray", autotransform=FALSE, try=40)
```

```{r, echo=FALSE}
ord <- metaMDS(bio, distance="bray", autotransform=FALSE, try=40, trace = FALSE)
```

## NMDS Plot

```{r gau-nmds, fig.width=5, fig.height=5, fig.algn='center'}
plot(ord, type="text")
abline(h=0, lty="dashed", col="gray")
abline(v=0, lty="dashed", col="gray")
mtext(paste("Stress=", round(ord$stress,2)), side=3)
```


## Stress

<br>

Compares similarity of the ordination with original dissimilarity in all dimensions.

<br>

::: {.column width="39%"}

* $\theta(d_{ij})$: observed dissimilaritiy
* $\tilde{d}|{ij}$: ordination dissimilarity

$$
S = \sqrt{\frac{\sum_{i \ne j} (\theta(d_{ij}) - \tilde{d}|{ij})^2}{\sum_{i \ne j}  \tilde{d}_{ij}^2}}
$$

:::

::: {.column width="39%"}

| Quality of ordination | Stress |
|-----------------------|--------|
| poor                  | > 0.2  |
| sufficient            | < 0.1  |
| good                  | <0.05  |
| excellent             | <0.025 |
| perfect               | 0.0    |
|                       |        |

:::


## Stressplot (Shepherd plot)


```{r stressplot1, fig.align='center'}
stressplot(ord)
```

## Stressplot: a good and a poor example

<br>

:::{.column width="47%" .add-space}

**Good**

```{r stressplot2, echo=FALSE, fig.width=4, fig.height=4}
stressplot(ord, pch=16)
mtext(paste("Stress=", round(ord$stress,2)), side=3)
```

:::

:::{.column width="47%"}

**Poor**

```{r stressplot3, echo=FALSE, fig.width=4, fig.height=4}
set.seed(123)
rdat <- matrix(runif(100), nrow=20)
ord2  <- metaMDS(rdat, distance = "euclid", autotransform=FALSE, trace=FALSE)
stressplot(ord2, pch=16)
mtext(paste("Stress=", round(ord2$stress,2)), side=3)
```

:::


* Important: points should be close to the red line
* Pattern of stairs not important (at least not for now)
* The $R^2$ values are always big, ignore or at least don't overinterpret it.



## Environmental fitting

```{r envfit, fig.align='center'}
ord <- metaMDS(bio, trace=FALSE)
ef <- envfit(ord, env, permu=1000)
plot(ord, type="t")
plot(ef, p.max =0.1)
```

* Plots arrows if explanation variables are metric.
* Shows only centroids for ordinal explanation variables.

## Numerical results and p-values

```{r}
ef
```
<br>

* p-values are based on a permutation test
* useful, but the ADONIS test is better

## Another example data set from the package

```{r vare-envfit, fig.align='center'}
data(varechem); data(varespec)
mds <- metaMDS(varespec, trace=FALSE)
ef <- envfit(mds, varechem, permu=1000)
plot(mds, type="t")
plot(ef, p.max =0.1)
```

## Surface fitting

```{r vare-surf, fig.align='center', echo=FALSE}
ef <- envfit(mds ~ Al + Ca, varechem, permu=1000)
plot(mds, display="sites", type="text")
plot(ef)
invisible(with(varechem, ordisurf(mds, Al, add=TRUE)))
invisible(with(varechem, ordisurf(mds, Ca, add=TRUE, col="green4")))
```

```{r vare-surf2, fig.align='center', eval=FALSE}
ef <- envfit(mds ~ Al + Ca, varechem, permu = 1000)
plot(mds, display = "sites", type = "text")
plot(ef)
with(varechem, ordisurf(mds, Al, add = TRUE))
with(varechem, ordisurf(mds, Ca, add = TRUE, col = "green4"))
```

## Advantages and disadvantages of the methods discussed so far

**PCA**

(+) easy to understand, quick and reproducible

(+) no non-linear distortion

(--) but: horseshoe effect possible

(--) information is often still in a "higher dimension"

(--) Euclidean distance poorly suited for species lists

(--) one-matrix method (no separate matrices for species and environmental factors)

**NMDS**

(+) any distance measure can be used

(+) better mapping on low dimensions

(--) bias

(--) numerical effort, iterative method, local minima

(--) one-matrix method (no separate matrices for species and environmental factors)


## Alternative and generalized methods I

**CA**

* similar to PCS, but uses $\chi^2$ distance
* better for taxa lists
* one-matrix method

**CCA**

* extension of the CA ($\chi^2$ distance)
* two-matrix method: species list and environmental factors

**RDA**

* extension of the PCA (Euclidean distance)
* two-matrix method: species list and environmental factors

## Alternative and generalized methods I

**dbRDA** / **capscale**

* Extension of the RDA for arbitrary distance measures
* Two-matrix method: species list and environmental factors

**For comparison: environmental fitting**

* subsequent overlay of the ordination with environmental factors
* applicable for most ordination methods (CA, PCA, NMDS)
โถ similar to two-matrix method, but both steps independent

## Interpretation of CA

* biplot with species and environmental factors, note scaling (ฮฑ)!
    - distance from the origin: $\chi^2$
    - species in the middle: either "average species" or poorly explained species.
    - species at the very edge: Attention, often rare species
- orthogonal angle of species on connecting line origin - centroid of environmental factor

## Problems of CA (and  PCA)

:::{.column width="44%" .add-space}
* arc (CA) or horseshoe effect (PCA)

```{r, echo=FALSE}
tridi <- function(k) {
  x <- matrix(0, nrow=k+2, ncol=k)
  diag(x) <- 1
  x2 <- x
  x2[2:(k+1), ] <- x2[2:(k+1), ] + x[1:k, ]
  x2[3:(k+2), ] <- x2[3:(k+2), ] + x[1:k, ]
  t(x2)
}
x <- tridi(10)
kable(x)
```


:::

:::{.column width=44%}

```{r horseshoe, fig.width=4, fig.height=4, echo=FALSE}
par(mar=c(4,4,1,1))
cc <- cca(x)
plot(cc)
```

:::

**Workaround**

- detrended correspondence analysis (DCA) used in the past, not anymore recommended
- better: NMDS or a "constrained" (2-matrix) method, e.g. CCA, RDA, dbRDA)


## CCA: Canonical Correspondence Analysis

........... draw a sketch here

* two-matrix method:
    - taxa matrix (bio)
    - environmental matrix (env)
* correlation many - many $\mathbf{Y} =f(\mathbf{X})$
* response variable is not only a diversity index, but the complete species community
* allows numerical assessment of "information partition" (inertia)

## Example Gauernitzbach-data

```{r}
ord <- cca(bio ~ ., data=env)
ord
```


* "inertia" measures error and information (similar to variance)
* in case of CCA it is $\chi^2$ distance, in case of RDA it is variance
* in the example
    - 61% is explained by the constrained axes Habitat, Stream and Flood
    - 39% is not explained by the provided environmental variables

## Triplot

<br>

```{r gau-cca, fig.width=6, fig.height=6, fig.align='center'}
plot(ord)
```


## Statistical significance tests

```{r}
anova(ord, by="terms")
```


## ADONIS test

```{r}
adonis2(bio ~Habitat*Stream*Flood, data=env)
```



## Which ordination method to start with?

<br>

Multivariate statistics is a very broad field. 
Experience shows that it can become quite complex and challenging, but also that it is relatively easy to start with it.

<br>

**My personal recommendation**

1. Start with **PCA** if working with physical, chemical and hydromorphological data. It often also works well with aggregated biomass data.
2. Use RDA if you have additional explanation variables (two-matrix method)
3. Start with NMDS if working with abundance data of species (taxa lists)
4. Use NMDS with `envfit` to explore influence of explanation variables on the ordination.
5. use CCA, dbRDA or PCoA to get more quantitative results, compared to NMDS.





# Cluster Analysis


## Overview

* Cluster analysis aims to group data sets in clusters

* Hierarchical clustering
  - build a dendrogram (a tree of grouping)
  - agglomerative methods
  - divisive methods

* Different agglomeration methods
  - define how distance is measured between clusters

* Nonhierarchical clustering
   - subdivide in a given number of groups
   - usually no dendrogram
   - iterative methods
   - e.g. k-means, k-centroids
   
   
## The UBA lake data set again

```{r echo = FALSE}
lakes <- read.csv(file="../data/uba/lakes-combined-data.csv")
valid_columns <- c(
  "name", "shortname",
  #"drainage", 
  #"population", 
  #"altitude", 
  "z_mean",
  "z_max", 
  "t_ret", 
  "volume", 
  "area", 
  #"shore_length", 
  #"shore_devel", 
  #"drain_ratio", 
  "p_tot", 
  "n_no3", 
  "chl",
  "wfd_type"
)

## less columns, so that we get a simplified subset and more complete cases
lakes <- lakes[valid_columns] |> na.omit()
row.names(lakes) <- lakes$shortname

lakes_saved <- lakes # backup copy of the data set for later use

lake_ids <- lakes[c("name", "shortname")]
lakedata <- lakes[, -c(1, 2)]
lakedata$wfd_type <- NULL
lakedata2 <- sqrt(lakedata)
```

<small>
```{r echo=FALSE}
kable(lakedata)
```

</small>

## Cluster analysis

```{r lakes-cluster2 , fig.height=6, wig.width=12, fig.align='center'}
par(mfrow=c(1,2))
hc <- hclust(dist(scale(lakedata2)), method="complete") # the default
plot(hc)

hc2 <- hclust(dist(scale(lakedata2)), method="ward.D2")
plot(hc2)
```

## Identification of clusters in the tree

```{r lake-clusters2}
plot(hc, hang = -1)    # -1: extend vertical lines to the bottom
rect.hclust(hc, 5)
grp <- cutree(hc, 5)
# grp                  # can be used to show the groups
```

## Color NMDS according to clusters

```{r uba-nmds, fig.align='center', fig.width=5, fig.height=5}
md <- metaMDS(lakedata2, scale = TRUE, distance = "euclid", trace=FALSE)
```

```{r lake-nmds-colored}
plot(md, type = "n")
text(md$points, row.names(lakedata2), col = grp)
```


## Non-hierarchical clustering

Instead of hierarchical clustering, we can also use a non-hierarchical method,
e.g. k-means clustering. This is an iterative method, and avoids the problem that 
cluster assignment depends on the order of clustering and the agglomeration method.

Depending on the question, it may be a disadvantage, that the number of clusters 
needs to be specified beforehand (e.g. from hierarchical clustering) and that we
do not get a tree diagramm.


 
## References

